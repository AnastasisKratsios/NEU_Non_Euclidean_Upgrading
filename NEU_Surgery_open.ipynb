{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU (Reconfigurations Map and Related Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Algorithm (NEU-OLS)\n",
    "\n",
    "1. Perform Basic Algorithm (in this case OLS)\n",
    "2. Map predictions to their graph; ie $x\\mapsto (x,\\hat{f}_{OLS}(x))$ where $\\hat{f}_{OLS}$ is the least-squares regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning & ML\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras as K\n",
    "from keras import backend as Kback\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import utils as np_utils\n",
    "from scipy import linalg as scila\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Alerts\n",
    "import os as beepsnd\n",
    "\n",
    "# General Outputs\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for NEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d67a88031f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reshape Data Into Compatible Shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Perform OLS Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlinear_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Reshape Data Into Compatible Shape\n",
    "data_x = np.array(data_x).reshape(-1,d)\n",
    "data_y = np.array(data_y)\n",
    "# Perform OLS Regression\n",
    "linear_model = LinearRegression()\n",
    "reg = linear_model.fit(data_x, data_y)\n",
    "model_pred_y = linear_model.predict(data_x)\n",
    "# Map to Graph\n",
    "data_NEU = np.concatenate((data_x,model_pred_y.reshape(-1,D)),1)\n",
    "NEU_targets  = data_y.reshape(-1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------#\n",
    "# Helper Functions T/F Functions\n",
    "#----------------------------#\n",
    "def bump_True(x):\n",
    "    x_out = 1- tf.math.pow(tf.math.abs(x),2)\n",
    "    x_out = tf.math.exp(-tf.math.divide(1,x_out))\n",
    "    return x_out\n",
    "\n",
    "def bump_False(x):\n",
    "    x_out = 0\n",
    "    return x_out\n",
    "\n",
    "# Approximation to max(0,x) function by smooth function\n",
    "# https://math.stackexchange.com/questions/517482/approximating-a-maximum-function-by-a-differentiable-function\n",
    "def abs_helper(x):\n",
    "    return tf.math.sqrt(tf.math.pow(x,2)+(10**(-5)))\n",
    "\n",
    "def max_helper(x):\n",
    "    return tf.math.multiply(abs_helper(x) + x,.5)\n",
    "\n",
    "def soft_indicator(x):\n",
    "    return max_helper(tf.math.sign(x))\n",
    "\n",
    "def bump_function(x):\n",
    "    # Gaussian rescaled to [-1,1]^d\n",
    "    bump_out = 1-tf.math.pow(x,2)        \n",
    "    bump_out = tf.math.divide(-1,bump_out)\n",
    "    bump_out = tf.math.exp(bump_out)\n",
    "    # Indicator\n",
    "    indicator = soft_indicator(1-x)\n",
    "    # 0 outside of [-1,1]^d\n",
    "    bump_out = tf.math.multiply(bump_out,indicator)\n",
    "    return bump_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reconfiguration Unit\n",
    "$$\n",
    "x \\mapsto \\exp\\left(\n",
    "\\psi(x-c;\\sigma) X\n",
    "\\right) (x-c) + c\n",
    "$$\n",
    "where:\n",
    "#### Workflow\n",
    "1. Shifts $x \\in \\mathbb{R}^d$ to $x- c$; c trainable.\n",
    "2. Applies the map $\\psi(x;\\sigma)\\triangleq e^{\\frac{\\sigma}{\\sigma-|x|}}I_{\\{|x|<\\sigma\\}}$ component-wise.  \n",
    "3. Applies transformation $x \\mapsto x +b$, $b \\in \\mathbb{R}^d$ trainable.\n",
    "4. Applies the diagonalization map to that output: $ \\left(x_1,\\dots,x_d\\right)\\mapsto\n",
    "                \\begin{pmatrix}\n",
    "                x_1 & & 0\\\\\n",
    "                &\\ddots &\\\\\n",
    "                0 & & x_d\\\\\n",
    "                \\end{pmatrix}.$\n",
    "5. Applies map $X \\mapsto XA$, $A$ is a trainable $d\\times d$ matrix.\n",
    "6. Applies matrix exponential.\n",
    "7. Multiplies output with result of (1).\n",
    "8. Re-centers output to $x +c$ where $c$ is as in (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Build and Training NEU Units (Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit_steps(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Reconfiguration_unit_steps, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                     shape=input_shape[1:],\n",
    "                                     initializer='GlorotUniform',\n",
    "                                     trainable=True)\n",
    "        \n",
    "        \n",
    "# #     Step 3\n",
    "#         self.scale = self.add_weight(name='location',\n",
    "#                                     shape=input_shape[1:],\n",
    "#                                     initializer='GlorotNormal',\n",
    "#                                     trainable=True)\n",
    "    \n",
    "#         self.threshold = self.add_weight(name='threshold',\n",
    "#                             shape=input_shape[1:],\n",
    "#                             initializer='GlorotUniform',\n",
    "#                             trainable=True)\n",
    "        \n",
    "\n",
    "        # STEP 3 - NEW\n",
    "        # INITIALIZE KERNELS\n",
    "        self.kernel_a = self.add_weight(name='kernel_a ',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        #inverse width\n",
    "        self.kernel_b = self.add_weight(name='kernel_b',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='ones',\n",
    "                                    trainable=True)\n",
    "\n",
    "        #center\n",
    "        self.kernel_c = self.add_weight(name='kernel_c',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    " \n",
    "\n",
    "    \n",
    "        ## Steps 4-9\n",
    "        self.tangentbiases = self.add_weight(name='tangentbiases',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "        self.tangentweights = self.add_weight(name='tangentweights',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "        self.location_b = self.add_weight(name='location_b',\n",
    "                                    shape=input_shape[1:],\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "    \n",
    "    \n",
    "    def call(self, input):\n",
    "        ### Steps 1-2\n",
    "        # Relocated\n",
    "        output_steps_1_2 = input + self.location\n",
    "        \n",
    "        ### Step 3\n",
    "        # Old\n",
    "#         thresholdin_3 = tf.math.multiply(tf.math.pow(self.scale,2),output_steps_1_2)\n",
    "#         thresholdin_3 = output_steps_1_2-self.threshold\n",
    "#         thresholdin_3 = bump_function(thresholdin_3)    \n",
    "        # NEW\n",
    "        exp_arg = - self.kernel_b * tf.math.square(output_steps_1_2 - self.kernel_c)\n",
    "        output_step_3 = self.kernel_a * tf.math.exp(exp_arg)\n",
    "\n",
    "        \n",
    "#         output_step_3 = tf.math.multiply(output_step_3,thresholdin_3)\n",
    "        \n",
    "        ### Step 4-9\n",
    "        x_out = tf.math.multiply(tf.math.abs(self.tangentweights),output_step_3) + self.tangentbiases\n",
    "        \n",
    "        # 7. Apply Matrix Exponential\n",
    "        x_out = tf.linalg.diag(x_out)\n",
    "        #------------------------------#\n",
    "        # Using an approximation of the matrix exponential (tf.linalg.expm(x_out)) is prefeable \n",
    "        # **Since:** TF uses Sylvester's method to for (fast) computation but this requires additional assumptions on the matrix which are typically not satified...\n",
    "        # **Instead:** Use a truncated power series representation (standard definition of expm) of order 4\n",
    "        x_out = tf.linalg.diag(tf.ones(d+D)) + x_out + tf.linalg.matmul(x_out,x_out)/2 + tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out))/6 +tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out)))/24 #+tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out))))/120\n",
    "        #x_out = tf.linalg.expm(x_out)\n",
    "        \n",
    "        # 8. Muliply by output of (1)\n",
    "        x_out = tf.linalg.matvec(x_out,input)\n",
    "        \n",
    "        # 9. Recenter Transformed Data\n",
    "        x_out = x_out + self.location_b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Projection Layer (For Regression)\n",
    "Maps $\\mathbb{X}\\left((x,f(x))\\mid \\theta \\right) \\in \\mathbb{R}^{d\\times D}$ to an element of $\\mathbb{R}^D$ by post-composing with the second canonical projection\n",
    "$$\n",
    "(x_1,x_2)\\mapsto x_2\n",
    ",\n",
    "$$\n",
    "where $x_1 \\in \\mathbb{R}^d$ and $x_2 \\in \\mathbb{R}^D$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions: Compiling and Training NEU-OLS\n",
    "\n",
    "#### First Unit\n",
    "These are helper functions for training the reconfiguration map.\n",
    "\n",
    "Build and greedily-initialize the first reconfiguration unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    # Apply Reconfiguration Unit\n",
    "    reconfigure  = Reconfiguration_unit_steps()\n",
    "    current_layer = reconfigure(input_layer)\n",
    "    # Output\n",
    "    output_layer = projection_layer(current_layer)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit_steps()\n",
    "    current_layer_new = new_reconfiguration_unit(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(current_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit_steps()\n",
    "    current_layer_new = new_reconfiguration_unit(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(current_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "\n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_NEU' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-06f8a649d372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Base Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_NEU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNEU_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Greedy Initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNEU_OLS_Greedy_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_NEU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNEU_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_NEU' is not defined"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "model = get_base_model(data_NEU,NEU_targets,Pre_Epochs)\n",
    "\n",
    "# Greedy Initialization\n",
    "NEU_OLS_Greedy_init = get_base_model(data_NEU,NEU_targets,Pre_Epochs)\n",
    "for i in range(N_Reconfigurations):\n",
    "    NEU_OLS_Greedy_init = add_reconfiguration_unit_greedily(NEU_OLS_Greedy_init,data_NEU,NEU_targets,Pre_Epochs)\n",
    "    # Update User on Status of Initialization\n",
    "    print((i/N_Reconfigurations))\n",
    "    \n",
    "# Train Full Model Using Initializatoins\n",
    "NEU_OLS = build_reconfiguration(NEU_OLS_Greedy_init,data_NEU,NEU_targets,Full_Epochs)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions (for comparison: TEMP)\n",
    "NEU_OLS_single_unit_prediction = model.predict(data_NEU)\n",
    "NEU_OLS_greedy_initializations = NEU_OLS_Greedy_init.predict(data_NEU)\n",
    "NEU_OLS_prediction = NEU_OLS(data_NEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-acb107acf083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import matplotlib.pyplot as plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Adjust Figure Details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plot Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# Adjust Figure Details\n",
    "plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Plot Models\n",
    "plt.plot(data_x,true_y,color='k',label='true',linestyle='--')\n",
    "plt.plot(data_x,NEU_OLS_single_unit_prediction,color='b',label='NEU-OLS')\n",
    "plt.plot(data_x,NEU_OLS_greedy_initializations,color='g',label='NEU-OLS')\n",
    "plt.plot(data_x,NEU_OLS_prediction,color='Aqua',label='NEU-OLS')\n",
    "plt.plot(data_x,model_pred_y,color='r',label='NEU-OLS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE!!!\n",
    "https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
