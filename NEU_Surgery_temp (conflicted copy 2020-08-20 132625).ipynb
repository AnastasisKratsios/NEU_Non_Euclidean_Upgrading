{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on current version:\n",
    "recentered_input vs norm_at_center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU (Reconfigurations Map and Related Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Algorithm (NEU-OLS)\n",
    "\n",
    "1. Perform Basic Algorithm (in this case OLS)\n",
    "2. Map predictions to their graph; ie $x\\mapsto (x,\\hat{f}_{OLS}(x))$ where $\\hat{f}_{OLS}$ is the least-squares regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning & ML\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras as K\n",
    "import keras.backend as Kb\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import utils as np_utils\n",
    "from scipy import linalg as scila\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Alerts\n",
    "import os as beepsnd\n",
    "\n",
    "# Others\n",
    "import math\n",
    "\n",
    "# General Outputs\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_Reconfigurations = 10**2\n",
    "d = 1 # Dimension of X\n",
    "D = 1 # Dimension of Y\n",
    "\n",
    "\n",
    "# Data Meta-Parameters\n",
    "noise_level = 0.1\n",
    "uncertainty_level= 0.5\n",
    "\n",
    "# Training meta-parameters\n",
    "Pre_Epochs = 50\n",
    "Full_Epochs = 100\n",
    "\n",
    "# # Height Per Reconfiguration\n",
    "# Height_factor_Per_reconfig = d+D\n",
    "\n",
    "# Number of Datapoints\n",
    "N_data = 10**3\n",
    "# Unknown Function\n",
    "def unknown_f(x):\n",
    "    return np.sin(x) #+ (x % 2)\n",
    "\n",
    "# Generate Data\n",
    "%run Data_Generator.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for NEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Data Into Compatible Shape\n",
    "data_x = np.array(data_x).reshape(-1,d)\n",
    "data_y = np.array(data_y)\n",
    "# Perform OLS Regression\n",
    "linear_model = LinearRegression()\n",
    "reg = linear_model.fit(data_x, data_y)\n",
    "model_pred_y = linear_model.predict(data_x)\n",
    "# Map to Graph\n",
    "data_NEU = np.concatenate((data_x,model_pred_y.reshape(-1,D)),1)\n",
    "NEU_targets  = data_y.reshape(-1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bump Function\n",
    "# def bump_function(self, x):\n",
    "#         return tf.math.exp(-self.sigma / (self.sigma - tf.math.pow(x, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reconfiguration Unit\n",
    "$$\n",
    "x \\mapsto \\exp\\left(\n",
    "\\psi(a\\|x\\|+b)\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) (x-c) + c\n",
    "$$\n",
    "where:\n",
    "#### Workflow\n",
    "1. Shifts $x \\in \\mathbb{R}^d$ to $x- c$; c trainable.\n",
    "2. Applies the map $\\psi(x;\\sigma)\\triangleq e^{\\frac{\\sigma}{\\sigma-|x|}}I_{\\{|x|<\\sigma\\}}$ component-wise.  \n",
    "3. Applies transformation $x \\mapsto x +b$, $b \\in \\mathbb{R}^d$ trainable.\n",
    "4. Applies the diagonalization map to that output: $ \\left(x_1,\\dots,x_d\\right)\\mapsto\n",
    "                \\begin{pmatrix}\n",
    "                x_1 & & 0\\\\\n",
    "                &\\ddots &\\\\\n",
    "                0 & & x_d\\\\\n",
    "                \\end{pmatrix}.$\n",
    "5. Applies map $X \\mapsto XA$, $A$ is a trainable $d\\times d$ matrix.\n",
    "6. Applies matrix exponential.\n",
    "7. Multiplies output with result of (1).\n",
    "8. Re-centers output to $x +c$ where $c$ is as in (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Build and Training NEU Units (Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit(tf.keras.layers.Layer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "    super(Reconfiguration_unit_steps, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.a = self.add_weight(name='location_in',\n",
    "                                     shape=[1],\n",
    "                                     initializer='GlorotUniform',\n",
    "                                     trainable=True)\n",
    "        self.b = self.add_weight(name='location_out',\n",
    "                                     shape=[1],\n",
    "                                     initializer='GlorotUniform',\n",
    "                                     trainable=True)\n",
    "        \n",
    "        \n",
    "        ###########################################################################################\n",
    "        # Tangential ffNN\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1 ',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2 ',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',#Previously 'ones'\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_2',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='ones',#Previously 'ones'\n",
    "                                    trainable=True)\n",
    "        \n",
    "        \n",
    "def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network\n",
    "        #---------------------------------------#\n",
    "        tangential_ffNN = tf.matmul(inputs, self.Tw1) + self.Tb1\n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.matmul(inputs, self.Tw2) + self.Tb2\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf.constant(np.array([1,2,3,4]))\n",
    "tf.reshape(test,[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit_steps(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Reconfiguration_unit_steps, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.location_in = self.add_weight(name='location_in',\n",
    "                                     shape=input_shape[1:],\n",
    "                                     initializer='GlorotUniform',\n",
    "                                     trainable=True)\n",
    "        self.location_out = self.add_weight(name='location_out',\n",
    "                                     shape=input_shape[1:],\n",
    "                                     initializer='GlorotUniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "###########################################################################################\n",
    "        # Tangential ffNN\n",
    "        self.tangential_ffNN_W1 = self.add_weight(name='TWeights1 ',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        self.tangential_ffNN_b1 = self.add_weight(name='TBiases1',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',#Previously 'ones'\n",
    "                                    trainable=True)\n",
    "        \n",
    "        self.tangential_ffNN_W2 = self.add_weight(name='TWeights2 ',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.tangential_ffNN_b2 = self.add_weight(name='TBiases2',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='ones',#Previously 'ones'\n",
    "                                    trainable=True)\n",
    "###########################################################################################\n",
    "        # Bump Function\n",
    "        self.sigma = self.add_weight(\n",
    "                        name='sigma',\n",
    "                        shape=[1],\n",
    "                        initializer=RandomUniform(minval=.95, maxval=1.05),\n",
    "                        trainable=True,\n",
    "                        constraint=tf.keras.constraints.NonNeg()\n",
    "                        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - tf.math.pow(x, 2)))\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Input Processing\n",
    "        #--------------------#\n",
    "        recentered_input = input + self.location_in\n",
    "        norm_at_recenter = tf.math.sqrt(tf.math.reduce_sum((recentered_input*recentered_input)))\n",
    "        \n",
    "        # Tangential ffNN: so_{d\\times }\n",
    "        #------------------------#\n",
    "        # Tangential ffNN: Build\n",
    "        T_ffNN = (norm_at_recenter*self.tangential_ffNN_W1) + self.tangential_ffNN_b1\n",
    "        #T_ffNN = tf.linalg.matvec(self.tangential_ffNN_W1,recentered_input) + self.tangential_ffNN_b1\n",
    "        T_ffNN = tf.nn.relu(T_ffNN)\n",
    "        T_ffNN = tf.linalg.matmul(self.tangential_ffNN_W2,T_ffNN) + self.tangential_ffNN_b2\n",
    "\n",
    "        # Anti-Symmetrize\n",
    "        #T_ffNN = tf.linalg.diag(T_ffNN)\n",
    "        T_ffNN_so = .5*(T_ffNN - tf.transpose(T_ffNN))\n",
    "        #x_out = T_ffNN\n",
    "        \n",
    "        # Apply Bump Function\n",
    "        #------------------------------#\n",
    "        # Logic: When to bump or not to bump\n",
    "        greater = tf.math.greater(norm_at_recenter, -1)\n",
    "        less = tf.math.less(norm_at_recenter, 1)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bumper = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(tf.where(condition, norm_at_recenter, 0.0)),\n",
    "            0.0)\n",
    "       \n",
    "        T_ffNN_so = bumper*T_ffNN_so \n",
    "        # Little Bump to smooth numerical issues\n",
    "        x_out = T_ffNN_so \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Map so_d ->> SO_d\n",
    "        #------------------------------#\n",
    "        # **Note/fact:** TF uses Sylvester's method to for (fast) computation but this requires additional assumptions on the matrix which are typically not satified...\n",
    "        # **Instead:** Use a truncated power series representation (standard definition of expm) of order 4\n",
    "        x_out = tf.linalg.diag(tf.ones(d+D)) + x_out + tf.linalg.matmul(x_out,x_out)/2 + tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out))/6 +tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out)))/24 #+tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out))))/120\n",
    "        #x_out = tf.linalg.expm(T_ffNN_so)\n",
    "        \n",
    "        # 8. Muliply by output of (1)\n",
    "        x_out = tf.linalg.matvec(x_out,recentered_input)\n",
    "        \n",
    "        # 9. Recenter Transformed Data\n",
    "        x_out = x_out + self.location_out\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Projection Layer (For Regression)\n",
    "Maps $\\mathbb{X}\\left((x,f(x))\\mid \\theta \\right) \\in \\mathbb{R}^{d\\times D}$ to an element of $\\mathbb{R}^D$ by post-composing with the second canonical projection\n",
    "$$\n",
    "(x_1,x_2)\\mapsto x_2\n",
    ",\n",
    "$$\n",
    "where $x_1 \\in \\mathbb{R}^d$ and $x_2 \\in \\mathbb{R}^D$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Loss Function\n",
    "This loss function prevents overfitting... it is especially userful for greedy approaches to training...like we use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #samples to get:\n",
    "    samples = Kb.cast(tf.math.floor(p * samples), 'int32')\n",
    "        #you can choose tf.math.ceil above, it depends on whether you want to\n",
    "        #include or exclude one element. Suppose you you want 33% top,\n",
    "        #but it's only possible to get exactly 30% or 40% top:\n",
    "        #floor will get 30% top and ceil will get 40% top.\n",
    "        #(exact matches included in both cases)\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE(p):\n",
    "    def loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions: Compiling and Training NEU-OLS\n",
    "\n",
    "#### First Unit\n",
    "These are helper functions for training the reconfiguration map.\n",
    "\n",
    "Build and greedily-initialize the first reconfiguration unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    # Apply Reconfiguration Unit\n",
    "    reconfigure  = Reconfiguration_unit_steps()\n",
    "    current_layer = reconfigure(input_layer)\n",
    "    # Output\n",
    "    output_layer = projection_layer(current_layer)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = Robust_MSE(uncertainty_level),\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit_steps()\n",
    "    current_layer_new = new_reconfiguration_unit(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(current_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = Robust_MSE(uncertainty_level),\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit_steps()\n",
    "    current_layer_new = new_reconfiguration_unit(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(current_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "\n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-5), momentum=0.01, nesterov=True)\n",
    "    #optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-5), initial_accumulator_value=0.1, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = Robust_MSE(uncertainty_level),\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1791451753478608\n",
      "0.01\n",
      "0.17911932924753896\n",
      "0.02\n",
      "0.17912119063151782\n",
      "0.03\n",
      "0.17912296481913018\n",
      "0.04\n",
      "0.1791254877965141\n",
      "0.05\n",
      "0.17916372371248981\n",
      "0.06\n",
      "0.1791422001999556\n",
      "0.07\n",
      "0.17920290104093406\n",
      "0.08\n",
      "0.1791106402467037\n",
      "0.09\n",
      "0.17916604358634547\n",
      "0.1\n",
      "0.17927768686214948\n",
      "0.11\n",
      "0.17911099773924047\n",
      "0.12\n",
      "0.17922124660331862\n",
      "0.13\n",
      "0.17961945383358416\n",
      "0.14\n",
      "0.17917177845049836\n",
      "0.15\n",
      "0.17922705489098245\n",
      "0.16\n",
      "0.17915405273157206\n",
      "0.17\n",
      "0.17915231190039738\n",
      "0.18\n",
      "0.1792930868485869\n",
      "0.19\n",
      "0.1791610432361636\n",
      "0.2\n",
      "0.1791547960741841\n",
      "0.21\n",
      "0.17916167973341068\n",
      "0.22\n",
      "0.1791905030280374\n",
      "0.23\n",
      "0.17925488479302362\n",
      "0.24\n",
      "0.1792884987250309\n",
      "0.25\n",
      "0.179162593868869\n",
      "0.26\n",
      "0.1791648748846157\n",
      "0.27\n",
      "0.17930388767296446\n",
      "0.28\n",
      "0.1791581258126043\n",
      "0.29\n",
      "0.17915383081651676\n",
      "0.3\n",
      "0.17927224086339066\n",
      "0.31\n",
      "0.17919937442728195\n",
      "0.32\n",
      "0.17917219832187736\n",
      "0.33\n",
      "0.17915484689176986\n",
      "0.34\n",
      "0.17916025389019166\n",
      "0.35\n",
      "0.17923691179466147\n",
      "0.36\n",
      "0.17916437180894687\n",
      "0.37\n",
      "0.17921308054440516\n",
      "0.38\n",
      "0.17931121816741716\n",
      "0.39\n",
      "0.17915428336169056\n",
      "0.4\n",
      "0.17920457976552284\n",
      "0.41\n",
      "0.17928110223459354\n",
      "0.42\n",
      "0.179247372514408\n",
      "0.43\n",
      "0.17915978662720392\n",
      "0.44\n",
      "0.17919334500103964\n",
      "0.45\n",
      "0.17927485369984766\n",
      "0.46\n",
      "0.17922094632066823\n",
      "0.47\n",
      "0.17915382606307861\n",
      "0.48\n",
      "0.17919530374690895\n",
      "0.49\n",
      "0.1791521373456647\n",
      "0.5\n",
      "0.17918972027201677\n",
      "0.51\n",
      "0.1792591048149515\n",
      "0.52\n",
      "0.17916896358382609\n",
      "0.53\n",
      "0.17931643874908304\n",
      "0.54\n",
      "0.17922494952469942\n",
      "0.55\n",
      "0.17934958713370816\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "model = get_base_model(data_NEU,NEU_targets,Pre_Epochs)\n",
    "\n",
    "# Greedy Initialization\n",
    "NEU_OLS_Greedy_init = get_base_model(data_NEU,NEU_targets,Pre_Epochs)\n",
    "for i in range(N_Reconfigurations):\n",
    "    # Update Model\n",
    "    NEU_OLS_Greedy_init_temp = add_reconfiguration_unit_greedily(NEU_OLS_Greedy_init,data_NEU,NEU_targets,Pre_Epochs)\n",
    "    \n",
    "    # Check for Blowup\n",
    "    if math.isnan(np.mean(NEU_OLS_Greedy_init.predict(data_NEU))):\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init\n",
    "        break\n",
    "    else: #Update Model if not explosion\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init_temp\n",
    "    \n",
    "    print(np.mean((NEU_OLS_Greedy_init.predict(data_NEU) - data_y)**2))\n",
    "    \n",
    "    # Update User on Status of Initialization\n",
    "    print(((i+1)/N_Reconfigurations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Full Model Using Initializatoins\n",
    "NEU_OLS = NEU_OLS_Greedy_init\n",
    "NEU_OLS = build_reconfiguration(NEU_OLS,data_NEU,NEU_targets,Full_Epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictions (for comparison: TEMP)\n",
    "NEU_OLS_prediction = NEU_OLS(data_NEU)\n",
    "NEU_OLS_single_unit_prediction = model.predict(data_NEU)\n",
    "NEU_OLS_greedy_initializations = NEU_OLS_Greedy_init.predict(data_NEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Figure Details\n",
    "plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Data Plot\n",
    "plt.plot(data_x,true_y,color='k',label='true',linestyle='--')\n",
    "\n",
    "# Plot Models\n",
    "plt.plot(data_x,model_pred_y,color='r',label='OLS')\n",
    "plt.plot(data_x,NEU_OLS_single_unit_prediction,color='b',label='NEU_Unit')\n",
    "plt.plot(data_x,NEU_OLS_greedy_initializations,color='g',label='NEU_Greedy_Init')\n",
    "plt.plot(data_x,NEU_OLS_prediction,color='Aqua',label='NEU-OLS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
