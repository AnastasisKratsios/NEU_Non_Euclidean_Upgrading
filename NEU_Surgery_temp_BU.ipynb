{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on current version:\n",
    "recentered_input vs norm_at_center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU (Reconfigurations Map and Related Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Algorithm (NEU-OLS)\n",
    "\n",
    "1. Perform Basic Algorithm (in this case OLS)\n",
    "2. Map predictions to their graph; ie $x\\mapsto (x,\\hat{f}_{OLS}(x))$ where $\\hat{f}_{OLS}$ is the least-squares regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning & ML\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras as K\n",
    "import keras.backend as Kb\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import utils as np_utils\n",
    "from scipy import linalg as scila\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Alerts\n",
    "import os as beepsnd\n",
    "\n",
    "# Others\n",
    "import math\n",
    "\n",
    "# General Outputs\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_Reconfigurations = 20\n",
    "d = 1 # Dimension of X\n",
    "D = 1 # Dimension of Y\n",
    "\n",
    "\n",
    "# Data Meta-Parameters\n",
    "noise_level = 0.1\n",
    "uncertainty_level= 0.9\n",
    "\n",
    "# Training meta-parameters\n",
    "Pre_Epochs = 10\n",
    "Full_Epochs = 100\n",
    "\n",
    "# Height Per Reconfiguration\n",
    "Height_factor_Per_reconfig = 2\n",
    "\n",
    "# Number of Datapoints\n",
    "N_data = 10**3\n",
    "# Unknown Function\n",
    "def unknown_f(x):\n",
    "    return np.sin(x) #+ (x % 2)\n",
    "\n",
    "# Generate Data\n",
    "%run Data_Generator.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for NEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Data Into Compatible Shape\n",
    "data_x = np.array(data_x).reshape(-1,d)\n",
    "data_y = np.array(data_y)\n",
    "# Perform OLS Regression\n",
    "linear_model = LinearRegression()\n",
    "reg = linear_model.fit(data_x, data_y)\n",
    "model_pred_y = linear_model.predict(data_x)\n",
    "# Map to Graph\n",
    "data_NEU = np.concatenate((data_x,model_pred_y.reshape(-1,D)),1)\n",
    "NEU_targets  = data_y.reshape(-1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bump Function\n",
    "# def bump_function(self, x):\n",
    "#         return tf.math.exp(-self.sigma / (self.sigma - tf.math.pow(x, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reconfiguration Unit\n",
    "$$\n",
    "x \\mapsto \\exp\\left(\n",
    "\\psi(x-c;\\sigma)\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) (x-c) + c\n",
    "$$\n",
    "where:\n",
    "#### Workflow\n",
    "1. Shifts $x \\in \\mathbb{R}^d$ to $x- c$; c trainable.\n",
    "2. Applies the map $\\psi(x;\\sigma)\\triangleq e^{\\frac{\\sigma}{\\sigma-|x|}}I_{\\{|x|<\\sigma\\}}$ component-wise.  \n",
    "3. Applies transformation $x \\mapsto x +b$, $b \\in \\mathbb{R}^d$ trainable.\n",
    "4. Applies the diagonalization map to that output: $ \\left(x_1,\\dots,x_d\\right)\\mapsto\n",
    "                \\begin{pmatrix}\n",
    "                x_1 & & 0\\\\\n",
    "                &\\ddots &\\\\\n",
    "                0 & & x_d\\\\\n",
    "                \\end{pmatrix}.$\n",
    "5. Applies map $X \\mapsto XA$, $A$ is a trainable $d\\times d$ matrix.\n",
    "6. Applies matrix exponential.\n",
    "7. Multiplies output with result of (1).\n",
    "8. Re-centers output to $x +c$ where $c$ is as in (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Build and Training NEU Units (Core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit_steps(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Reconfiguration_unit_steps, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.location_in = self.add_weight(name='location_in',\n",
    "                                     shape=input_shape[1:],\n",
    "                                     initializer='GlorotUniform',\n",
    "                                     trainable=True)\n",
    "        self.location_out = self.add_weight(name='location_out',\n",
    "                                     shape=input_shape[1:],\n",
    "                                     initializer='GlorotUniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "###########################################################################################\n",
    "        # Tangential ffNN\n",
    "        self.tangential_ffNN_W1 = self.add_weight(name='TWeights1 ',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        self.tangential_ffNN_b1 = self.add_weight(name='TBiases1',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',#Previously 'ones'\n",
    "                                    trainable=True)\n",
    "        \n",
    "        self.tangential_ffNN_W2 = self.add_weight(name='TWeights2 ',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='GlorotUniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.tangential_ffNN_b2 = self.add_weight(name='TBiases2',\n",
    "                                    shape=(input_shape[1], input_shape[1]),\n",
    "                                    initializer='ones',#Previously 'ones'\n",
    "                                    trainable=True)\n",
    "###########################################################################################\n",
    "        # Bump Function\n",
    "        self.sigma = self.add_weight(\n",
    "                        name='sigma',\n",
    "                        shape=[1],\n",
    "                        initializer=RandomUniform(minval=.95, maxval=1.05),\n",
    "                        trainable=True,\n",
    "                        constraint=tf.keras.constraints.NonNeg()\n",
    "                        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - tf.math.pow(x, 2)))\n",
    "    \n",
    "    def call(self, input):\n",
    "        # Input Processing\n",
    "        #--------------------#\n",
    "        recentered_input = input + self.location_in\n",
    "        norm_at_recenter = tf.math.sqrt(tf.math.reduce_sum((recentered_input*recentered_input)))\n",
    "        \n",
    "        # Tangential ffNN: so_{d\\times }\n",
    "        #------------------------#\n",
    "        # Tangential ffNN: Build\n",
    "        T_ffNN = (norm_at_recenter*self.tangential_ffNN_W1) + self.tangential_ffNN_b1\n",
    "        #T_ffNN = tf.linalg.matvec(self.tangential_ffNN_W1,recentered_input) + self.tangential_ffNN_b1\n",
    "        T_ffNN = tf.nn.relu(T_ffNN)\n",
    "        T_ffNN = tf.linalg.matmul(self.tangential_ffNN_W2,T_ffNN) + self.tangential_ffNN_b2\n",
    "\n",
    "        # Anti-Symmetrize\n",
    "        #T_ffNN = tf.linalg.diag(T_ffNN)\n",
    "        T_ffNN_so = .5*(T_ffNN - tf.transpose(T_ffNN))\n",
    "        #x_out = T_ffNN\n",
    "        \n",
    "        # Apply Bump Function\n",
    "        #------------------------------#\n",
    "        # Logic: When to bump or not to bump\n",
    "        greater = tf.math.greater(norm_at_recenter, -1)\n",
    "        less = tf.math.less(norm_at_recenter, 1)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bumper = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(tf.where(condition, norm_at_recenter, 0.0)),\n",
    "            0.0)\n",
    "       \n",
    "        T_ffNN_so = bumper*T_ffNN_so \n",
    "        # Little Bump to smooth numerical issues\n",
    "        x_out = T_ffNN_so \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Map so_d ->> SO_d\n",
    "        #------------------------------#\n",
    "        # **Note/fact:** TF uses Sylvester's method to for (fast) computation but this requires additional assumptions on the matrix which are typically not satified...\n",
    "        # **Instead:** Use a truncated power series representation (standard definition of expm) of order 4\n",
    "        x_out = tf.linalg.diag(tf.ones(d+D)) + x_out + tf.linalg.matmul(x_out,x_out)/2 + tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out))/6 +tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out)))/24 #+tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,tf.linalg.matmul(x_out,x_out))))/120\n",
    "        #x_out = tf.linalg.expm(T_ffNN_so)\n",
    "        \n",
    "        # 8. Muliply by output of (1)\n",
    "        x_out = tf.linalg.matvec(x_out,recentered_input)\n",
    "        \n",
    "        # 9. Recenter Transformed Data\n",
    "        x_out = x_out + self.location_out\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Projection Layer (For Regression)\n",
    "Maps $\\mathbb{X}\\left((x,f(x))\\mid \\theta \\right) \\in \\mathbb{R}^{d\\times D}$ to an element of $\\mathbb{R}^D$ by post-composing with the second canonical projection\n",
    "$$\n",
    "(x_1,x_2)\\mapsto x_2\n",
    ",\n",
    "$$\n",
    "where $x_1 \\in \\mathbb{R}^d$ and $x_2 \\in \\mathbb{R}^D$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Loss Function\n",
    "This loss function prevents overfitting... it is especially userful for greedy approaches to training...like we use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #samples to get:\n",
    "    samples = Kb.cast(tf.math.floor(p * samples), 'int32')\n",
    "        #you can choose tf.math.ceil above, it depends on whether you want to\n",
    "        #include or exclude one element. Suppose you you want 33% top,\n",
    "        #but it's only possible to get exactly 30% or 40% top:\n",
    "        #floor will get 30% top and ceil will get 40% top.\n",
    "        #(exact matches included in both cases)\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE(p):\n",
    "    def loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions: Compiling and Training NEU-OLS\n",
    "\n",
    "#### First Unit\n",
    "These are helper functions for training the reconfiguration map.\n",
    "\n",
    "Build and greedily-initialize the first reconfiguration unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    # Apply Reconfiguration Unit\n",
    "    reconfigure  = Reconfiguration_unit_steps()\n",
    "    current_layer = reconfigure(input_layer)\n",
    "    # Output\n",
    "    output_layer = projection_layer(current_layer)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = Robust_MSE(uncertainty_level),\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit_steps()\n",
    "    current_layer_new = new_reconfiguration_unit(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(current_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = Robust_MSE(uncertainty_level),\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit_steps()\n",
    "    current_layer_new = new_reconfiguration_unit(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(current_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "\n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-5), momentum=0.01, nesterov=True)\n",
    "    #optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-5), initial_accumulator_value=0.1, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = Robust_MSE(uncertainty_level),\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1717893805810051\n",
      "0.05\n",
      "0.17168614623215142\n",
      "0.1\n",
      "0.17167537757360082\n",
      "0.15\n",
      "0.17168669466761646\n",
      "0.2\n",
      "0.17167671198938977\n",
      "0.25\n",
      "0.17203800039631806\n",
      "0.3\n",
      "0.1718904351093933\n",
      "0.35\n",
      "0.17167190919912467\n",
      "0.4\n",
      "0.1716746947196948\n",
      "0.45\n",
      "0.1716741851421072\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "model = get_base_model(data_NEU,NEU_targets,Pre_Epochs)\n",
    "\n",
    "# Greedy Initialization\n",
    "NEU_OLS_Greedy_init = get_base_model(data_NEU,NEU_targets,Pre_Epochs)\n",
    "for i in range(N_Reconfigurations):\n",
    "    # Update Model\n",
    "    NEU_OLS_Greedy_init_temp = add_reconfiguration_unit_greedily(NEU_OLS_Greedy_init,data_NEU,NEU_targets,Pre_Epochs)\n",
    "    \n",
    "    # Check for Blowup\n",
    "    if math.isnan(np.mean(NEU_OLS_Greedy_init.predict(data_NEU))):\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init\n",
    "        break\n",
    "    else: #Update Model if not explosion\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init_temp\n",
    "    \n",
    "    print(np.mean((NEU_OLS_Greedy_init.predict(data_NEU) - data_y)**2))\n",
    "    \n",
    "    # Update User on Status of Initialization\n",
    "    print(((i+1)/N_Reconfigurations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Full Model Using Initializatoins\n",
    "NEU_OLS = NEU_OLS_Greedy_init\n",
    "NEU_OLS = build_reconfiguration(NEU_OLS,data_NEU,NEU_targets,Full_Epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictions (for comparison: TEMP)\n",
    "NEU_OLS_prediction = NEU_OLS(data_NEU)\n",
    "NEU_OLS_single_unit_prediction = model.predict(data_NEU)\n",
    "NEU_OLS_greedy_initializations = NEU_OLS_Greedy_init.predict(data_NEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Figure Details\n",
    "plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Data Plot\n",
    "plt.plot(data_x,true_y,color='k',label='true',linestyle='--')\n",
    "\n",
    "# Plot Models\n",
    "plt.plot(data_x,model_pred_y,color='r',label='OLS')\n",
    "plt.plot(data_x,NEU_OLS_single_unit_prediction,color='b',label='NEU_Unit')\n",
    "plt.plot(data_x,NEU_OLS_greedy_initializations,color='g',label='NEU_Greedy_Init')\n",
    "plt.plot(data_x,NEU_OLS_prediction,color='Aqua',label='NEU-OLS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
