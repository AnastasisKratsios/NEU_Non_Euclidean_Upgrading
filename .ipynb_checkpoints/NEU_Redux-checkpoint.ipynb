{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on current version:\n",
    "**To Try**:\n",
    "- [Cayley parameterization](https://planetmath.org/cayleysparameterizationoforthogonalmatrices) of $SU_d$ (since this is really all we need)...*will it be more stable than Lie's parameterization?* Note: it is a homeomorphism so this is great for UAP!\n",
    "- SVD approach to pre-trainining \n",
    "  - (here for procrustes problem)[https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem]\n",
    "  - [here for complexity](https://mathoverflow.net/questions/161252/what-is-the-time-complexity-of-truncated-svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU (Reconfigurations Map and Related Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Algorithm (NEU-OLS)\n",
    "\n",
    "1. Perform Basic Algorithm (in this case OLS)\n",
    "2. Map predictions to their graph; ie $x\\mapsto (x,\\hat{f}_{OLS}(x))$ where $\\hat{f}_{OLS}$ is the least-squares regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning & ML\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras as K\n",
    "import keras.backend as Kb\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import utils as np_utils\n",
    "from scipy import linalg as scila\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Alerts\n",
    "import os as beepsnd\n",
    "\n",
    "# Others\n",
    "import math\n",
    "\n",
    "# General Outputs\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_Reconfigurations = 20\n",
    "d = 1 # Dimension of X\n",
    "D = 1 # Dimension of Y\n",
    "\n",
    "\n",
    "# Data Meta-Parameters\n",
    "noise_level = 0.1\n",
    "uncertainty_level= 0.5\n",
    "\n",
    "# Training meta-parameters\n",
    "Pre_Epochs = 50\n",
    "Full_Epochs = 100\n",
    "\n",
    "# # Height Per Reconfiguration\n",
    "# Height_factor_Per_reconfig = d+D\n",
    "\n",
    "# Number of Datapoints\n",
    "N_data = 10**3\n",
    "# Unknown Function\n",
    "def unknown_f(x):\n",
    "    return np.sin(x) #+ (x % 2)\n",
    "\n",
    "# Generate Data\n",
    "%run Data_Generator.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for NEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Data Into Compatible Shape\n",
    "data_x = np.array(data_x).reshape(-1,d)\n",
    "data_y = np.array(data_y)\n",
    "# Perform OLS Regression\n",
    "linear_model = LinearRegression()\n",
    "reg = linear_model.fit(data_x, data_y)\n",
    "model_pred_y = linear_model.predict(data_x)\n",
    "# Map to Graph\n",
    "data_NEU = np.concatenate((data_x,model_pred_y.reshape(-1,D)),1)\n",
    "NEU_targets  = data_y.reshape(-1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #samples to get:\n",
    "        #you can choose tf.math.ceil above, it depends on whether you want to\n",
    "        #include or exclude one element. Suppose you you want 33% top,\n",
    "        #but it's only possible to get exactly 30% or 40% top:\n",
    "        #floor will get 30% top and ceil will get 40% top.\n",
    "        #(exact matches included in both cases)\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE(p):\n",
    "    def loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reconfiguration Unit\n",
    "*Lie Version:* $$\n",
    "x \\mapsto \\exp\\left(\n",
    "%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) x.\n",
    "$$\n",
    "\n",
    "*Cayley version:*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x \\mapsto & \\left[(I_d + A(x))(I- A(x))^{-1}\\right]x\n",
    "\\\\\n",
    "A(x)\\triangleq &%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "tf.linalg.inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Reconfiguration_unit, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(self.units,),\n",
    "                                    initializer='random_normal',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "#         self.sigma = self.add_weight(name='bump_threshfold',\n",
    "#                                         shape=[1],\n",
    "#                                         initializer=RandomUniform(minval=.5, maxval=1),\n",
    "#                                         trainable=True,\n",
    "#                                         constraint=tf.keras.constraints.NonNeg())\n",
    "#         self.a = self.add_weight(name='bump_scale',\n",
    "#                                         shape=[1],\n",
    "#                                         initializer='ones',\n",
    "#                                         trainable=True)\n",
    "#         self.b = self.add_weight(name='bump_location',\n",
    "#                                         shape=[1],\n",
    "#                                         initializer='zeros',\n",
    "#                                         trainable=True)\n",
    "\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1 ',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2 ',shape=(input_shape[-1],input_shape[-1]),initializer='GlorotUniform',trainable=True)\n",
    "\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',shape=(input_shape[-1],input_shape[-1]),initializer='GlorotUniform',trainable=True)\n",
    "        self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',shape=[1],initializer=RandomUniform(minval=0.0, maxval=0.01),trainable=True,constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.math.reduce_sum((input*input)) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "#         bump_input = self.a *norm_inputs + self.b\n",
    "#         greater = tf.math.greater(bump_input, -self.sigma)\n",
    "#         less = tf.math.less(bump_input, self.sigma)\n",
    "#         condition = tf.logical_and(greater, less)\n",
    "\n",
    "#         output_bump = tf.where(\n",
    "#             condition, \n",
    "#             self.bump_function(bump_input),\n",
    "#             0.0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        tangential_ffNN = norm_inputs*self.Tw1\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)  \n",
    "        tangential_ffNN = tf.matmul(tangential_ffNN, self.Tw2) \n",
    "    \n",
    "        # Scale DNN by Radial-Data\n",
    "#         tangential_ffNN = output_bump*tangential_ffNN\n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "#         tangential_ffNN = output_bump*tangential_ffNN\n",
    "        tangential_ffNN = tangential_ffNN + self.num_stab_param*tf.linalg.diag(tf.ones(d+D))\n",
    "    \n",
    "        # Lie Parameterization (Problematic)\n",
    "        #tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        # Cayley Transformation (Experimental)\n",
    "        tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.inv(self.Id - tangential_ffNN))\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,(input-self.location)) + self.location\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    # Apply Reconfiguration Unit\n",
    "    reconfiguration_unit  = Reconfiguration_unit(d+D)(input_layer)\n",
    "    # Output\n",
    "    output_layer = projection_layer(reconfiguration_unit)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit(d+D)(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(new_reconfiguration_unit)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit(d+D)(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(new_reconfiguration_unit)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "\n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-5), momentum=0.01, nesterov=True)\n",
    "    #optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-5), initial_accumulator_value=0.1, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16280420799127632\n",
      "0.05\n",
      "1.688155962852042\n",
      "0.1\n",
      "0.17338953960345907\n",
      "0.15\n",
      "1.621771370555208\n",
      "0.2\n",
      "0.2051515986259346\n",
      "0.25\n",
      "1.5187931780542108\n",
      "0.3\n",
      "0.21936582982169103\n",
      "0.35\n",
      "1.4850257189489218\n",
      "0.4\n",
      "0.19887790069214356\n",
      "0.45\n",
      "1.6195511188159002\n",
      "0.5\n",
      "0.22129900140730083\n",
      "0.55\n",
      "1.7350823909832949\n",
      "0.6\n",
      "0.19989856777471932\n",
      "0.65\n",
      "1.8469547827265127\n",
      "0.7\n",
      "0.18779191783268243\n",
      "0.75\n",
      "1.946630343148663\n",
      "0.8\n",
      "0.18891787067719026\n",
      "0.85\n",
      "2.0296236124438836\n",
      "0.9\n",
      "0.2078382249714764\n",
      "0.95\n",
      "2.0774462952316526\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "model = get_base_model(data_NEU,NEU_targets,10)\n",
    "\n",
    "# Greedy Initialization\n",
    "NEU_OLS_Greedy_init = model\n",
    "for i in range(N_Reconfigurations):\n",
    "    # Update Model\n",
    "    NEU_OLS_Greedy_init_temp = add_reconfiguration_unit_greedily(NEU_OLS_Greedy_init,\n",
    "                                                                 data_NEU,\n",
    "                                                                 NEU_targets,\n",
    "                                                                 Pre_Epochs)\n",
    "    \n",
    "    # Check for Blowup\n",
    "    if math.isnan(np.mean(NEU_OLS_Greedy_init.predict(data_NEU))):\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init\n",
    "        break\n",
    "    else: #Update Model if not explosion\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init_temp\n",
    "    \n",
    "    print(np.mean((NEU_OLS_Greedy_init.predict(data_NEU) - data_y)**2))\n",
    "    \n",
    "    # Update User on Status of Initialization\n",
    "    print(((i+1)/N_Reconfigurations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1232228090473062\n"
     ]
    }
   ],
   "source": [
    "# Report Train-set Quality\n",
    "#-------------------------#\n",
    "print(np.mean(np.abs(model.predict(data_NEU) - NEU_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.4766 - mse: 0.4766\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 194us/sample - loss: 0.4565 - mse: 0.4565\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 189us/sample - loss: 0.4651 - mse: 0.4651\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 206us/sample - loss: 0.4601 - mse: 0.4601\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 198us/sample - loss: 0.4549 - mse: 0.4549\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 229us/sample - loss: 0.4687 - mse: 0.4687\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 193us/sample - loss: 0.4630 - mse: 0.4630\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 179us/sample - loss: 0.4552 - mse: 0.4552\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 205us/sample - loss: 0.4643 - mse: 0.4643\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 180us/sample - loss: 0.4461 - mse: 0.4461\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 186us/sample - loss: 0.4385 - mse: 0.4385\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 208us/sample - loss: 0.4572 - mse: 0.4572\n",
      "Epoch 13/100\n",
      " 576/1000 [================>.............] - ETA: 0s - loss: 0.4224 - mse: 0.4224"
     ]
    }
   ],
   "source": [
    "# # Train Full Model Using Initializatoins\n",
    "NEU_OLS = NEU_OLS_Greedy_init\n",
    "NEU_OLS = build_reconfiguration(NEU_OLS_Greedy_init,data_NEU,NEU_targets,Full_Epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictions (for comparison: TEMP)\n",
    "NEU_OLS_prediction = NEU_OLS(data_NEU)\n",
    "NEU_OLS_single_unit_prediction = model.predict(data_NEU)\n",
    "NEU_OLS_greedy_initializations = NEU_OLS_Greedy_init.predict(data_NEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Figure Details\n",
    "plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Data Plot\n",
    "plt.plot(data_x,true_y,color='k',label='true',linestyle='--')\n",
    "\n",
    "# Plot Models\n",
    "plt.plot(data_x,model_pred_y,color='r',label='OLS')\n",
    "plt.plot(data_x,NEU_OLS_single_unit_prediction,color='b',label='NEU_Unit')\n",
    "# plt.plot(data_x,NEU_OLS_greedy_initializations,color='g',label='NEU_Greedy_Init')\n",
    "plt.plot(data_x,NEU_OLS_prediction,color='Aqua',label='NEU-OLS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
