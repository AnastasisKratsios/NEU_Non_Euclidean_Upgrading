{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions Depot\n",
    "This little script contains all the custom helper functions required to run any segment of the NEU and its benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# # Load Dependances and makes path(s)\n",
    "# exec(open('Initializations_Dump.py').read())\n",
    "# # Load Hyper( and meta) parameter(s)\n",
    "# exec(open('HyperParameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE, between 0 and 100\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    y_true.shape = (y_true.shape[0], 1)\n",
    "    y_pred.shape = (y_pred.shape[0], 1)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Shortfall Loss\n",
    "*These loss functions also have a robust representation but are only used for debugging/sanitychecking and are not included in the final version of the code or the paper itself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE_ES(p):\n",
    "    def ES_p_loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust MSE\n",
    "These next loss-functions are the ones used in the paper; they solve the approximate robust MSE problem:\n",
    "$$\n",
    "MSE_{robust}(x,y)\\triangleq \\operatorname{argmax}_{w\\in \\Delta_N} \\sum_{n=1}^N w_n\\|x_n-y_n\\|^2 - \\lambda \\sum_{n=1}^N w_n \\log\\left(\\frac1{N}\\right)\n",
    ";\n",
    "$$\n",
    "where $\\Delta_N\\triangleq \\left\\{w \\in \\mathbb{R}^N:\\, 0\\leq w_n\\leq 1\\mbox{ and } \\sum_{n=1}^N w_n =1\\right\\}$ is the probability simplex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Version (Formulation with same arg-minima)\n",
    "# @tf.function\n",
    "def Entropic_Risk(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = tf.math.abs((y_true - y_pred))\n",
    "    loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "    loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "    # Return Value\n",
    "    return loss_out\n",
    "\n",
    "# def Robust_MSE(y_true, y_pred):\n",
    "#     # Compute Exponential Utility\n",
    "#     loss_out = tf.math.abs((y_true - y_pred))\n",
    "#     loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "#     loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "#     # Return Value\n",
    "#     return loss_out\n",
    "def Robust_MSE(robustness_parameter=0.05):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Initialize Loss\n",
    "        absolute_errors_eval = tf.math.abs((y_true - y_pred))\n",
    "\n",
    "        # Compute Exponential        \n",
    "        loss_out_expweights = tf.math.exp(robustness_parameter*absolute_errors_eval)\n",
    "        loss_out_expweights_totals = tf.math.reduce_sum(loss_out_expweights)\n",
    "        loss_out_weighted = loss_out_expweights/tf.math.reduce_sum(loss_out_expweights)\n",
    "        loss_out_weighted = loss_out_weighted*absolute_errors_eval\n",
    "        loss_out_weighted = tf.math.reduce_sum(loss_out_weighted)\n",
    "\n",
    "        # Compute Average Loss\n",
    "        #loss_average = tf.math.reduce_mean(absolute_errors_eval)\n",
    "\n",
    "        # Return Value\n",
    "        loss_out = loss_out_weighted# - loss_average\n",
    "\n",
    "        return loss_out\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Numpy Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Version (Full dual Version)\n",
    "def Robust_MSE_numpy(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = np.abs((y_true - y_pred))\n",
    "    loss_out = np.exp(robustness_parameter*loss_out)\n",
    "    loss_out = np.mean(loss_out)\n",
    "    loss_out = np.log(loss_out)/robustness_parameter\n",
    "    # Return Value\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Custom Layers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Feed-Forward Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homeomorphism Layers:\n",
    "- Shift\n",
    "- Euclidean Group\n",
    "- Special Affine Group\n",
    "- Affine Group\n",
    "\n",
    "- *Reconfiguration Unit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift $\\mathbb{R}^d$  Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\mapsto x +b$ for some trainable $b\\in \\mathbb{R}^{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shift_Layers(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Shift_Layers, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{E}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{O}_{d}(\\mathbb{R})$  Layers\n",
    "This is the group of all isometries of $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Euclidean_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Euclidean_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        On = tf.linalg.matmul((self.Id + self.glw),tf.linalg.inv(self.Id - self.glw))\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(On,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{SAff}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{SL}_{d}(\\mathbb{R})$  Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $A \\in \\operatorname{SL}_d(\\mathbb{R})$ if and only if $A=\\frac1{\\sqrt[d]{\\det(\\exp(X))}} \\exp(X)$ for some $d\\times d$ matrix $X$.  \n",
    "\n",
    "*Why?*... We use the fact that $\\det(k A) = k^d \\det(A)$ for any $k \\in \\mathbb{R}$ and any $d\\times d$ matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Special_Affine_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Special_Affine_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN_det = tf.linalg.det(GLN)\n",
    "        GLN_det = tf.pow(tf.abs(GLN_det),(1/(d+D)))\n",
    "        SLN = tf.math.divide(GLN,GLN_det)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(SLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep GLd Layer:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Deep-GL}_d(x) \\triangleq& f^{Depth}\\circ \\dots f^1(x)\\\\\n",
    "f^i(x)\\triangleq &\\exp(A_2) \\operatorname{Leaky-ReLU}\\left(\n",
    "\\exp(A_1)x + b_1\n",
    "\\right)+ b_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $A_i$ are $d\\times d$ matrices and $b_i \\in \\mathbb{R}^d$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_GLd_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Deep_GLd_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "#         self.Id = self.add_weight(name='Identity_Matrix',\n",
    "#                                    shape=(input_shape[-1],input_shape[-1]),\n",
    "#                                    initializer='identity',\n",
    "#                                    trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "#         Element of gl_d\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.glw2 = self.add_weight(name='Tangential_Weights2',\n",
    "                                       shape=(input_shape[-1],input_shape[-1]),\n",
    "                                       initializer='GlorotUniform',\n",
    "                                       trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        self.b2 = self.add_weight(name='location_parameter2',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN2 = tf.linalg.expm(self.glw2)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "\n",
    "        x_out = tf.linalg.matvec(GLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        x_out = tf.nn.leaky_relu(x_out)\n",
    "        x_out = tf.linalg.matvec(GLN2,x_out)\n",
    "        x_out = x_out + self.b2\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple version is the following:\n",
    "This is the code use in the [background article](https://arxiv.org/abs/2006.02341)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense_Invertible(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense_Invertible, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        # Numerical Stability Parameter(s)\n",
    "        #----------------------------------#\n",
    "        self.num_stab_param = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ## Initiality Numerical Stability\n",
    "        numerical_stabilizer = tf.eye(D)*(self.num_stab_param*10**(-3))\n",
    "        expw_log = self.w + numerical_stabilizer\n",
    "#         rescaler = tf.norm(expw_log)\n",
    "        # Cayley Transform\n",
    "        expw = tf.linalg.matmul((self.Id + self.w),tf.linalg.inv(self.Id - self.w)) \n",
    "        # Lie Version\n",
    "#         expw_log = expw_log /rescaler\n",
    "        expw = tf.linalg.expm(expw_log)\n",
    "        return tf.matmul(inputs, expw) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## NEU Layers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New NEU\n",
    "- Feature map version (only differs up to dimension constant)\n",
    "- Readout map version\n",
    "- Constrained parametereized swish with $\\beta \\in \\left[-\\frac1{2},\\frac1{2}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implemented Version:**: *Lie Version:* $$\n",
    "x \\mapsto \\exp\\left(\n",
    "%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) x.\n",
    "$$\n",
    "\n",
    "**Non-Implemented Variant:**  *Cayley version:*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}(A(x)):\\,x \\mapsto & \\left[(I_d + A(x))(I_d- A(x))^{-1}\\right]x\n",
    "\\\\\n",
    "A(x)\\triangleq &%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the inverse of the Cayley transform of $A(x)$ is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}^{-1}(A(x)):\\,x \\mapsto & \\left[(I_d - A(x))(I_d+ A(x))^{-1}\\right]x\n",
    ".\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32, home_space_dim = d):\n",
    "        super(Reconfiguration_unit, self).__init__()\n",
    "        self.units = units\n",
    "        self.home_space_dim = home_space_dim\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(input_shape[-1],),\n",
    "                                    initializer='random_normal',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #                                  Decay Rates                                       #\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.sigma = self.add_weight(name='bump_threshfold',\n",
    "                                        shape=[1],\n",
    "                                        initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                        trainable=False,\n",
    "                                        constraint=tf.keras.constraints.NonNeg())\n",
    "        self.a = self.add_weight(name='bump_scale',\n",
    "                                        shape=[1],\n",
    "                                        initializer='ones',\n",
    "                                        trainable=False)\n",
    "        self.b = self.add_weight(name='bump_location',\n",
    "                                        shape=[1],\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=False)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Exponential Decay\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.exponential_decay = self.add_weight(name='exponential_decay_rate',\n",
    "                                                 shape=[1],\n",
    "                                                 initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                                 trainable=True,\n",
    "                                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Mixture\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.m_w1 = self.add_weight(name='no_decay',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w2 = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w3 = self.add_weight(name='bump',\n",
    "                                     shape=[1],\n",
    "                                     initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                     trainable=True,\n",
    "                                     constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # No Decay\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1',\n",
    "                                   shape=(self.units,(self.home_space_dim**2)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Exponential Decay\n",
    "        self.Tw1_b = self.add_weight(name='Tangential_Weights_1_b',\n",
    "                           shape=(self.units,(self.home_space_dim**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_b = self.add_weight(name='Tangential_Weights_2_b',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Bump\n",
    "        self.Tw1_c = self.add_weight(name='Tangential_Weights_1_c',\n",
    "                           shape=(self.units,(self.home_space_dim**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_c = self.add_weight(name='Tangential_Weights_2_c',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "        # Numerical Stability Parameter(s)\n",
    "        #----------------------------------#\n",
    "        self.num_stab_param = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # C^{\\infty} bump function (numerically unstable...) #\n",
    "    #----------------------------------------------------#\n",
    "#     def bump_function(self, x):\n",
    "#         return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "    # C^1 bump function (numerically stable??) #\n",
    "    #----------------------------------------------------#\n",
    "    def bump_function(self, x):\n",
    "#         return tf.math.pow(x-self.sigma,2)*tf.math.pow(x+self.sigma,2)\n",
    "        bump_out = tf.math.pow(x-self.sigma,2)*tf.math.pow(x+self.sigma,2)\n",
    "        bump_out = tf.math.pow(bump_out,(1/4))\n",
    "        return bump_out\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.norm(input) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Decay Rate Functions\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function (Local Behaviour)\n",
    "        bump_input = self.a*norm_inputs + self.b\n",
    "        greater = tf.math.greater(bump_input, -self.sigma)\n",
    "        less = tf.math.less(bump_input, self.sigma)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bump_decay = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(bump_input),\n",
    "            0.0)\n",
    "        bump_decay = 1\n",
    "        \n",
    "        # Exponential Decay\n",
    "        exp_decay = tf.math.exp(-self.exponential_decay*norm_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = norm_inputs*self.Id\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        \n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw1,tangential_ffNN)         \n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw2,tangential_ffNN)\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb2\n",
    "        \n",
    "        # Exponential Decay\n",
    "        tangential_ffNN_b = norm_inputs*exp_decay*self.Id\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb1_b\n",
    "        \n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw1_b,tangential_ffNN_b)         \n",
    "        tangential_ffNN_b = tf.nn.relu(tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw2_b,tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb2_b\n",
    "        \n",
    "        # Bump (Local Aspect)\n",
    "        tangential_ffNN_c = bump_decay*norm_inputs*self.Id\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb1_c\n",
    "        \n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw1_c,tangential_ffNN_c)         \n",
    "        tangential_ffNN_c = tf.nn.relu(tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw2_c,tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb2_c\n",
    "    \n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "        tangential_ffNN_b = (tf.transpose(tangential_ffNN_b) - tangential_ffNN_b) \n",
    "        tangential_ffNN_c = (tf.transpose(tangential_ffNN_c) - tangential_ffNN_c) \n",
    "        # Decay\n",
    "        tangential_ffNN = (self.m_w1*tangential_ffNN) + (self.m_w2*tangential_ffNN_b) + (self.m_w3*tangential_ffNN_c) \n",
    "            \n",
    "            \n",
    "        # NUMERICAL STABILIZER\n",
    "#         tangential_ffNN = tangential_ffNN + tf.eye(self.home_space_dim) *(self.num_stab_param*10**(-3))\n",
    "        tangential_ffNN = tf.math.maximum(tf.math.minimum(-tangential_ffNN,10**(15)),-(10**(15)))\n",
    "        # Lie Parameterization:  \n",
    "        tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        # Cayley Transformation (Stable):\n",
    "#         tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.pinv(self.Id - tangential_ffNN)) \n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,input) + self.location\n",
    "#         x_out = tf.linalg.matvec(tangential_ffNN,input)\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully-connected Feed-forward layer with $GL_{d}$-connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense_Invertible(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense_Invertible, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        # Numerical Stability Parameter(s)\n",
    "        #----------------------------------#\n",
    "        self.num_stab_param = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ## Initiality Numerical Stability\n",
    "        numerical_stabilizer = tf.eye(D)*(self.num_stab_param*10**(-3))\n",
    "        expw_log = self.w + numerical_stabilizer\n",
    "#         rescaler = tf.norm(expw_log)\n",
    "        # Cayley Transform\n",
    "#         expw = tf.linalg.matmul((self.Id + self.w),tf.linalg.inv(self.Id - self.w)) \n",
    "        # Lie Version\n",
    "#         expw_log = expw_log /rescaler\n",
    "        expw = tf.linalg.expm(expw_log)\n",
    "        return tf.matmul(inputs, expw) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma_{\\operatorname{rescaled-swish-trainable}}:x\\mapsto 2 \\frac{x}{1+ \\exp(-\\beta x)};\\qquad \\beta \\in [0,\\infty)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rescaled_swish_trainable(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(rescaled_swish_trainable, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.relulevel = self.add_weight(name='relu_level',\n",
    "                                 shape=[1],\n",
    "                                 initializer='ones',\n",
    "                                 trainable=True,\n",
    "                                 constraint=tf.keras.constraints.NonNeg())\n",
    "#                                  constraint=tf.keras.constraints.MinMaxNorm(min_value=-0.5, max_value=0.5))\n",
    "                                \n",
    "    def call(self,inputs):\n",
    "        swish_numerator_rescaled = 2*inputs\n",
    "        parameter = self.relulevel\n",
    "        swish_denominator_trainable = tf.math.sigmoid(parameter*inputs)\n",
    "        swish_trainable_out = tf.math.multiply(swish_numerator_rescaled,swish_denominator_trainable)\n",
    "        return swish_trainable_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Layers\n",
    "This layer maps $(x,y)\\mapsto y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Architecture Building Utilities:\n",
    "## NEU-Related:\n",
    "### Feature Extractor:\n",
    "This little function pops the final layer of a tensorflow model and replaces it with the identity.  When we apply it the the NEU-OLS we obtain only the trained linearizing feature map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trained_feature_map(model):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = tf.identity(layers[len(layers)-2].output)\n",
    "\n",
    "    for i in range(len(layers)-1):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    \n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reporters and Summarizers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------#\n",
    "#=### Results & Summarizing ###=#\n",
    "#-------------------------------#\n",
    "def reporter(y_train_hat_in,y_test_hat_in,y_train_in,y_test_in):\n",
    "    # Training Performance\n",
    "    Training_performance = np.array([mean_absolute_error(y_train_hat_in,y_train_in),\n",
    "                                mean_squared_error(y_train_hat_in,y_train_in),\n",
    "                                   mean_absolute_percentage_error(y_train_hat_in,y_train_in)])\n",
    "    # Testing Performance\n",
    "    Test_performance = np.array([mean_absolute_error(y_test_hat_in,y_test_in),\n",
    "                                mean_squared_error(y_test_hat_in,y_test_in),\n",
    "                                   mean_absolute_percentage_error(y_test_hat_in,y_test_in)])\n",
    "    # Organize into Dataframe\n",
    "    Performance_dataframe = pd.DataFrame({'train': Training_performance,'test': Test_performance})\n",
    "    Performance_dataframe.index = [\"MAE\",\"MSE\",\"MAPE\"]\n",
    "    # return output\n",
    "    return Performance_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
