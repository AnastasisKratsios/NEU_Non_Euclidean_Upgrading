{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions Depot\n",
    "This little script contains all the custom helper functions required to run any segment of the NEU and its benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Dependances and makes path(s)\n",
    "exec(open('Initializations_Dump.py').read())\n",
    "# Load Hyper( and meta) parameter(s)\n",
    "exec(open('HyperParameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Shortfall Loss\n",
    "*These loss functions also have a robust representation but are only used for debugging/sanitychecking and are not included in the final version of the code or the paper itself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE_ES(p):\n",
    "    def ES_p_loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust MSE\n",
    "These next loss-functions are the ones used in the paper; they solve the approximate robust MSE problem:\n",
    "$$\n",
    "MSE_{robust}(x,y)\\triangleq \\operatorname{argmax}_{w\\in \\Delta_N} \\sum_{n=1}^N w_n\\|x_n-y_n\\|^2 - \\lambda \\sum_{n=1}^N w_n \\log\\left(\\frac1{N}\\right)\n",
    ";\n",
    "$$\n",
    "where $\\Delta_N\\triangleq \\left\\{w \\in \\mathbb{R}^N:\\, 0\\leq w_n\\leq 1\\mbox{ and } \\sum_{n=1}^N w_n =1\\right\\}$ is the probability simplex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Version (Formulation with same arg-minima)\n",
    "# @tf.function\n",
    "def Entropic_Risk(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = tf.math.abs((y_true - y_pred))\n",
    "    loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "    loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "    # Return Value\n",
    "    return loss_out\n",
    "\n",
    "# def Robust_MSE(y_true, y_pred):\n",
    "#     # Compute Exponential Utility\n",
    "#     loss_out = tf.math.abs((y_true - y_pred))\n",
    "#     loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "#     loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "#     # Return Value\n",
    "#     return loss_out\n",
    "def Robust_MSE(y_true, y_pred):\n",
    "    # Initialize Loss\n",
    "    absolute_errors_eval = tf.math.abs((y_true - y_pred))\n",
    "\n",
    "    # Compute Exponential        \n",
    "    loss_out_expweights = tf.math.exp(robustness_parameter*absolute_errors_eval)\n",
    "    loss_out_expweights_totals = tf.math.reduce_sum(loss_out_expweights)\n",
    "    loss_out_weighted = loss_out_expweights/tf.math.reduce_sum(loss_out_expweights)\n",
    "    loss_out_weighted = loss_out_weighted*absolute_errors_eval\n",
    "    loss_out_weighted = tf.math.reduce_sum(loss_out_weighted)\n",
    "\n",
    "    # Compute Average Loss\n",
    "    #loss_average = tf.math.reduce_mean(absolute_errors_eval)\n",
    "\n",
    "    # Return Value\n",
    "    loss_out = loss_out_weighted# - loss_average\n",
    "\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Numpy Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Version (Full dual Version)\n",
    "def Robust_MSE_numpy(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = np.abs((y_true - y_pred))\n",
    "    loss_out = np.exp(robustness_parameter*loss_out)\n",
    "    loss_out = np.mean(loss_out)\n",
    "    loss_out = np.log(loss_out)/robustness_parameter\n",
    "    # Return Value\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Custom Layers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Feed-Forward Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homeomorphism Layers:\n",
    "- Shift\n",
    "- Euclidean Group\n",
    "- Special Affine Group\n",
    "- Affine Group\n",
    "\n",
    "- *Reconfiguration Unit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift $\\mathbb{R}^d$  Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\mapsto x +b$ for some trainable $b\\in \\mathbb{R}^{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shift_Layers(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Shift_Layers, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{E}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{O}_{d}(\\mathbb{R})$  Layers\n",
    "This is the group of all isometries of $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Euclidean_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Euclidean_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        On = tf.linalg.matmul((self.Id + self.glw),tf.linalg.inv(self.Id - self.glw))\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(On,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{SAff}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{SL}_{d}(\\mathbb{R})$  Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $A \\in \\operatorname{SL}_d(\\mathbb{R})$ if and only if $A=\\frac1{\\sqrt[d]{\\det(\\exp(X))}} \\exp(X)$ for some $d\\times d$ matrix $X$.  \n",
    "\n",
    "*Why?*... We use the fact that $\\det(k A) = k^d \\det(A)$ for any $k \\in \\mathbb{R}$ and any $d\\times d$ matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Special_Affine_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Special_Affine_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN_det = tf.linalg.det(GLN)\n",
    "        GLN_det = tf.pow(tf.abs(GLN_det),(1/(d+D)))\n",
    "        SLN = tf.math.divide(GLN,GLN_det)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(SLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep GLd Layer:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Deep-GL}_d(x) \\triangleq& f^{Depth}\\circ \\dots f^1(x)\\\\\n",
    "f^i(x)\\triangleq &\\exp(A_2) \\operatorname{Leaky-ReLU}\\left(\n",
    "\\exp(A_1)x + b_1\n",
    "\\right)+ b_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $A_i$ are $d\\times d$ matrices and $b_i \\in \\mathbb{R}^d$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_GLd_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Deep_GLd_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "#         self.Id = self.add_weight(name='Identity_Matrix',\n",
    "#                                    shape=(input_shape[-1],input_shape[-1]),\n",
    "#                                    initializer='identity',\n",
    "#                                    trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "#         Element of gl_d\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.glw2 = self.add_weight(name='Tangential_Weights2',\n",
    "                                       shape=(input_shape[-1],input_shape[-1]),\n",
    "                                       initializer='GlorotUniform',\n",
    "                                       trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        self.b2 = self.add_weight(name='location_parameter2',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN2 = tf.linalg.expm(self.glw2)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "\n",
    "        x_out = tf.linalg.matvec(GLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        x_out = tf.nn.leaky_relu(x_out)\n",
    "        x_out = tf.linalg.matvec(GLN2,x_out)\n",
    "        x_out = x_out + self.b2\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## NEU Layers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconfiguration Unit\n",
    "*Lie Version:* $$\n",
    "x \\mapsto \\exp\\left(\n",
    "%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) x.\n",
    "$$\n",
    "\n",
    "*Cayley version:*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}(A(x)):\\,x \\mapsto & \\left[(I_d + A(x))(I_d- A(x))^{-1}\\right]x\n",
    "\\\\\n",
    "A(x)\\triangleq &%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the inverse of the Cayley transform of $A(x)$ is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}^{-1}(A(x)):\\,x \\mapsto & \\left[(I_d - A(x))(I_d+ A(x))^{-1}\\right]x\n",
    ".\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout Map Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Reconfiguration_unit, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(input_shape[-1],),\n",
    "                                    initializer='random_normal',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #                                  Decay Rates                                       #\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.sigma = self.add_weight(name='bump_threshfold',\n",
    "                                        shape=[1],\n",
    "                                        initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                        trainable=True,\n",
    "                                        constraint=tf.keras.constraints.NonNeg())\n",
    "        self.a = self.add_weight(name='bump_scale',\n",
    "                                        shape=[1],\n",
    "                                        initializer='ones',\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bump_location',\n",
    "                                        shape=[1],\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Exponential Decay\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.exponential_decay = self.add_weight(name='exponential_decay_rate',\n",
    "                                                 shape=[1],\n",
    "                                                 initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                                 trainable=True,\n",
    "                                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Mixture\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.m_w1 = self.add_weight(name='no_decay',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w2 = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w3 = self.add_weight(name='bump',\n",
    "                                     shape=[1],\n",
    "                                     initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                     trainable=True,\n",
    "                                     constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # No Decay\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1',\n",
    "                                   shape=(self.units,((d+D)**2)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2',\n",
    "                                   shape=(((d+D)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=((d+D),(d+D)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Exponential Decay\n",
    "        self.Tw1_b = self.add_weight(name='Tangential_Weights_1_b',\n",
    "                           shape=(self.units,((d+D)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_b = self.add_weight(name='Tangential_Weights_2_b',\n",
    "                                   shape=(((d+D)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=((d+D),(d+D)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Bump\n",
    "        self.Tw1_c = self.add_weight(name='Tangential_Weights_1_c',\n",
    "                           shape=(self.units,((d+D)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_c = self.add_weight(name='Tangential_Weights_2_c',\n",
    "                                   shape=(((d+D)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=((d+D),(d+D)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "        # Stability Parameter(s)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',shape=[1],initializer=RandomUniform(minval=0.0, maxval=0.01),trainable=True,constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.norm(input) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Decay Rate Functions\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function (Local Behaviour)\n",
    "        bump_input = self.a*norm_inputs + self.b\n",
    "        greater = tf.math.greater(bump_input, -self.sigma)\n",
    "        less = tf.math.less(bump_input, self.sigma)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bump_decay = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(bump_input),\n",
    "            0.0)\n",
    "        \n",
    "        # Exponential Decay\n",
    "        exp_decay = tf.math.exp(-self.exponential_decay*norm_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = norm_inputs*self.Id\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[((d+D)**2),1])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        \n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw1,tangential_ffNN)         \n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw2,tangential_ffNN)\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[(d+D),(d+D)])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb2\n",
    "        \n",
    "        # Exponential Decay\n",
    "        tangential_ffNN_b = norm_inputs*exp_decay*self.Id\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[((d+D)**2),1])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb1_b\n",
    "        \n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw1_b,tangential_ffNN_b)         \n",
    "        tangential_ffNN_b = tf.nn.relu(tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw2_b,tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[(d+D),(d+D)])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb2_b\n",
    "        \n",
    "        # Bump (Local Aspect)\n",
    "        tangential_ffNN_c = bump_decay*norm_inputs*self.Id\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[((d+D)**2),1])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb1_c\n",
    "        \n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw1_c,tangential_ffNN_c)         \n",
    "        tangential_ffNN_c = tf.nn.relu(tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw2_c,tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[(d+D),(d+D)])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb2_c\n",
    "    \n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "        tangential_ffNN_b = (tf.transpose(tangential_ffNN_b) - tangential_ffNN_b) \n",
    "        tangential_ffNN_c = (tf.transpose(tangential_ffNN_c) - tangential_ffNN_c) \n",
    "        # Decay\n",
    "        tangential_ffNN = (self.m_w1*tangential_ffNN) + (self.m_w2*tangential_ffNN_b) + (self.m_w3*tangential_ffNN_c) \n",
    "            \n",
    "        # Cayley Transformation (Stable):\n",
    "        tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.inv(self.Id - tangential_ffNN)) # Lie Parameterization (Numerically Unstable):  #tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,input) + self.location\n",
    "#         x_out = tf.linalg.matvec(tangential_ffNN,input)\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Map Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit_Feature(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Reconfiguration_unit_Feature, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(input_shape[-1],),\n",
    "                                    initializer='random_normal',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #                                  Decay Rates                                       #\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.sigma = self.add_weight(name='bump_threshfold',\n",
    "                                        shape=[1],\n",
    "                                        initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                        trainable=True,\n",
    "                                        constraint=tf.keras.constraints.NonNeg())\n",
    "        self.a = self.add_weight(name='bump_scale',\n",
    "                                        shape=[1],\n",
    "                                        initializer='ones',\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bump_location',\n",
    "                                        shape=[1],\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Exponential Decay\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.exponential_decay = self.add_weight(name='exponential_decay_rate',\n",
    "                                                 shape=[1],\n",
    "                                                 initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                                 trainable=True,\n",
    "                                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Mixture\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.m_w1 = self.add_weight(name='no_decay',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w2 = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w3 = self.add_weight(name='bump',\n",
    "                                     shape=[1],\n",
    "                                     initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                     trainable=True,\n",
    "                                     constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # No Decay\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1',\n",
    "                                   shape=(self.units,((d)**2)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2',\n",
    "                                   shape=(((d)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=((d),(d)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Exponential Decay\n",
    "        self.Tw1_b = self.add_weight(name='Tangential_Weights_1_b',\n",
    "                           shape=(self.units,((d)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_b = self.add_weight(name='Tangential_Weights_2_b',\n",
    "                                   shape=(((d)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=((d),(d)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Bump\n",
    "        self.Tw1_c = self.add_weight(name='Tangential_Weights_1_c',\n",
    "                           shape=(self.units,((d)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_c = self.add_weight(name='Tangential_Weights_2_c',\n",
    "                                   shape=(((d)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=((d),(d)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "        # Stability Parameter(s)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',shape=[1],initializer=RandomUniform(minval=0.0, maxval=0.01),trainable=True,constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.norm(input) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Decay Rate Functions\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function (Local Behaviour)\n",
    "        bump_input = self.a*norm_inputs + self.b\n",
    "        greater = tf.math.greater(bump_input, -self.sigma)\n",
    "        less = tf.math.less(bump_input, self.sigma)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bump_decay = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(bump_input),\n",
    "            0.0)\n",
    "        \n",
    "        # Exponential Decay\n",
    "        exp_decay = tf.math.exp(-self.exponential_decay*norm_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = norm_inputs*self.Id\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[((d)**2),1])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        \n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw1,tangential_ffNN)         \n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw2,tangential_ffNN)\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[(d),(d)])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb2\n",
    "        \n",
    "        # Exponential Decay\n",
    "        tangential_ffNN_b = norm_inputs*exp_decay*self.Id\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[((d)**2),1])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb1_b\n",
    "        \n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw1_b,tangential_ffNN_b)         \n",
    "        tangential_ffNN_b = tf.nn.relu(tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw2_b,tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[(d),(d)])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb2_b\n",
    "        \n",
    "        # Bump (Local Aspect)\n",
    "        tangential_ffNN_c = bump_decay*norm_inputs*self.Id\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[((d)**2),1])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb1_c\n",
    "        \n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw1_c,tangential_ffNN_c)         \n",
    "        tangential_ffNN_c = tf.nn.relu(tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw2_c,tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[(d),(d)])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb2_c\n",
    "    \n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "        tangential_ffNN_b = (tf.transpose(tangential_ffNN_b) - tangential_ffNN_b) \n",
    "        tangential_ffNN_c = (tf.transpose(tangential_ffNN_c) - tangential_ffNN_c) \n",
    "        # Decay\n",
    "        tangential_ffNN = (self.m_w1*tangential_ffNN) + (self.m_w2*tangential_ffNN_b) + (self.m_w3*tangential_ffNN_c) \n",
    "            \n",
    "        # Cayley Transformation (Stable):\n",
    "        tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.inv(self.Id - tangential_ffNN)) # Lie Parameterization (Numerically Unstable):  #tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,input) + self.location\n",
    "#         x_out = tf.linalg.matvec(tangential_ffNN,input)\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Layers\n",
    "This layer maps $(x,y)\\mapsto y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archictecture Builders\n",
    "This next snippet builds a (Vanilla) feed-forward network.\n",
    "- **Inputs**: *Height, Depth, Learning Rate + Input/Output Dimension Specifications.*\n",
    "\n",
    "#### Compiles Feed-forward network with usual MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark ffNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "   \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiles Feed-forward network with Robust MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "   \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Geometric Deep Learning Feed-Forward Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NAIVE_NEU_ffNN(feature_map_depth, feature_map_height, ## NEU-Feature Map Hyper-Parameter(s)\n",
    "                       height, depth, ## ffNN Parameter(s)\n",
    "                       readout_map_depth, readout_map_height): ## NEU-Feature Map Hyper-Parameter(s)\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(d,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    \n",
    "    # Initial Features\n",
    "    inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN)\n",
    "    # Higher-Order Feature Depth\n",
    "    if feature_map_depth > 0:\n",
    "        inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN)\n",
    "\n",
    "    \n",
    "    \n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    # First ffNN Layer: Reconfigured inputs -> Hidden Neurons\n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN_feature)\n",
    "    # Higher-Order Deep Layers: Hidden Neurons -> Hidden Neurons\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "    # Last ffNN Layer: Hidden Neurons -> Output Space\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)     \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###########-#\n",
    "    # Readout Map #\n",
    "    #-###########-#\n",
    "    # Input -> Input x ffNN output\n",
    "    output_layer_new = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    # Add Depth to Readout Map\n",
    "    if readout_map_depth > 0:\n",
    "        output_layer_new = Deep_GLd_Layer(d+D)(output_layer_new)\n",
    "\n",
    "    # Project down from graph space to output space (from: Input x Outputs -> Outputs)\n",
    "    output_layer = projection_layer(output_layer_new)\n",
    "    \n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    ffNN.compile(optimizer=opt, loss=Robust_MSE, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return ffNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "## NEU - Network Builder(s)\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEU-ffNN Builder\n",
    "This next snippet builds the NEU for the feed-forward network; i.e.:\n",
    "$$\n",
    "f_{NEU} \\triangleq \\rho \\circ f_{ffNN}\\circ \\phi\n",
    ",\n",
    "$$\n",
    "where $\\rho=p\\circ \\xi$, $\\xi,\\phi$ are reconfiguration networks, and $f_{ffNN}$ is a feed-forward network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_ffNN(feature_map_depth, feature_map_height, ## NEU-Feature Map Hyper-Parameter(s)\n",
    "                 height, depth, ## ffNN Parameter(s)\n",
    "                 readout_map_depth, readout_map_height): ## NEU-Feature Map Hyper-Parameter(s)\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(d,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    \n",
    "    # Initial Features\n",
    "    inputs_ffNN_feature = Shift_Layers(d)(inputs_ffNN)\n",
    "    inputs_ffNN_feature  = Reconfiguration_unit_Feature(feature_map_height)(inputs_ffNN_feature)\n",
    "    inputs_ffNN_feature = Shift_Layers(d)(inputs_ffNN_feature)\n",
    "    # Higher-Order Feature Depth\n",
    "    if feature_map_depth > 0:\n",
    "        inputs_ffNN_feature = Shift_Layers(d)(inputs_ffNN_feature)\n",
    "        inputs_ffNN_feature  = Reconfiguration_unit_Feature(feature_map_height)(inputs_ffNN_feature)\n",
    "        inputs_ffNN_feature = Shift_Layers(d)(inputs_ffNN_feature)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    # First ffNN Layer: Reconfigured inputs -> Hidden Neurons\n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN_feature)\n",
    "    # Higher-Order Deep Layers: Hidden Neurons -> Hidden Neurons\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "    # Last ffNN Layer: Hidden Neurons -> Output Space\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)     \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###########-#\n",
    "    # Readout Map #\n",
    "    #-###########-#\n",
    "    # Input -> Input x ffNN output\n",
    "    output_layer_new = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    # Add Depth to Readout Map\n",
    "    if readout_map_depth > 0:\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "        output_layer_new  = Reconfiguration_unit(readout_map_height)(output_layer_new)\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "\n",
    "    # Project down from graph space to output space (from: Input x Outputs -> Outputs)\n",
    "    output_layer = projection_layer(output_layer_new)\n",
    "    \n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    ffNN.compile(optimizer=opt, loss=Robust_MSE, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return ffNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEU-ffNN Builder - $\\operatorname{GL}_d$-Variant\n",
    "This next snippet builds the NEU for the feed-forward network; i.e.:\n",
    "$$\n",
    "f_{NEU} \\triangleq \\rho \\circ f_{ffNN}\\circ \\phi\n",
    ",\n",
    "$$\n",
    "where $\\rho=p\\circ \\xi$, $\\xi,\\phi$ are reconfiguration networks, and $f_{ffNN}$ is a feed-forward network.  \n",
    "\n",
    "The difference with the above is that $\\xi$ and $\\phi$ have $GL_d$ Layers instead of shifts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_ffNN_w_GLd(feature_map_depth, feature_map_height, ## NEU-Feature Map Hyper-Parameter(s)\n",
    "                       height, depth, ## ffNN Parameter(s)\n",
    "                       readout_map_depth, readout_map_height): ## NEU-Feature Map Hyper-Parameter(s)\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(d,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    \n",
    "    # Initial Features\n",
    "    inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN)\n",
    "    inputs_ffNN_feature  = Reconfiguration_unit_Feature(feature_map_height)(inputs_ffNN_feature)\n",
    "    inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN_feature)\n",
    "    # Higher-Order Feature Depth\n",
    "    if feature_map_depth > 0:\n",
    "        inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN_feature)\n",
    "        inputs_ffNN_feature  = Reconfiguration_unit_Feature(feature_map_height)(inputs_ffNN_feature)\n",
    "        inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN_feature)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    # First ffNN Layer: Reconfigured inputs -> Hidden Neurons\n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN_feature)\n",
    "    # Higher-Order Deep Layers: Hidden Neurons -> Hidden Neurons\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "    # Last ffNN Layer: Hidden Neurons -> Output Space\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)     \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###########-#\n",
    "    # Readout Map #\n",
    "    #-###########-#\n",
    "    # Input -> Input x ffNN output\n",
    "    output_layer_new = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    # Add Depth to Readout Map\n",
    "    if readout_map_depth > 0:\n",
    "        output_layer_new = Deep_GLd_Layer(d+D)(output_layer_new)\n",
    "        output_layer_new  = Reconfiguration_unit(readout_map_height)(output_layer_new)\n",
    "        output_layer_new = Deep_GLd_Layer(d+D)(output_layer_new)\n",
    "\n",
    "    # Project down from graph space to output space (from: Input x Outputs -> Outputs)\n",
    "    output_layer = projection_layer(output_layer_new)\n",
    "    \n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    ffNN.compile(optimizer=opt, loss=Robust_MSE, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return ffNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Trainers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------#\n",
    "#=### Results & Summarizing ###=#\n",
    "#-------------------------------#\n",
    "def reporter(y_train_hat_in,y_test_hat_in,y_train_in,y_test_in):\n",
    "    # Training Performance\n",
    "    Training_performance = np.array([mean_absolute_error(y_train_hat_in,y_train_in),\n",
    "                                mean_squared_error(y_train_hat_in,y_train_in),\n",
    "                                   mean_absolute_percentage_error(y_train_hat_in,y_train_in)])\n",
    "    # Testing Performance\n",
    "    Test_performance = np.array([mean_absolute_error(y_test_hat_in,y_test_in),\n",
    "                                mean_squared_error(y_test_hat_in,y_test_in),\n",
    "                                   mean_absolute_percentage_error(y_test_hat_in,y_test_in)])\n",
    "    # Organize into Dataframe\n",
    "    Performance_dataframe = pd.DataFrame({'train': Training_performance,'test': Test_performance})\n",
    "    Performance_dataframe.index = [\"MAE\",\"MSE\",\"MAPE\"]\n",
    "    # return output\n",
    "    return Performance_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
