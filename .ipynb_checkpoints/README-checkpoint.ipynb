{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU: Non-Euclidean Upgrading\n",
    "Let us briefly discuss the problem of obtaining an optimal minimal-dimensional linearizing (co-)representation.\n",
    "\n",
    "## Minimal dimensional linearization problem:\n",
    "The learning phase of many learning problems is summarized by the optimization problem:\n",
    "$$\n",
    "\\min \\sum_{x \\in \\mathbb{X}} L(\\hat{f}(x),f(x),x) + P(\\hat{f}),\n",
    "$$\n",
    "where $f$ is the unknown function quantifying the input-output pattern which we would like to learn, $\\mathbb{X}$ is a given set of training data, $L$ is a loss-function, $P$ is a penalty encoding regularity into out learing model $\\hat{f}$, and the optimization is performed over our hypothesis class $\\mathcal{F}$.  \n",
    "\n",
    "**Improving model compatability with the input space:**\n",
    "\n",
    "Suppose we are given a class of learning model $\\mathscr{F}$ with inputs in $\\mathbb{R}^d$ and outputs in $\\mathbb{R}^D$.  Given any one particular model $\\hat{f}$ in our hypothesis class $\\mathscr{F}$, it is likely that there exists an alternative representation of the input space which is better suited to $\\hat{f}$, in the sense that:\n",
    "$$\n",
    "\\sum_{x \\in \\mathbb{X}} L(\\hat{f}\\circ \\phi(x),f(x),x) + P(\\hat{f})\n",
    "<\n",
    "\\sum_{x \\in \\mathbb{X}} L(\\hat{f}(x),f(x),x) + P(\\hat{f})\n",
    ".\n",
    "$$\n",
    "We are not interested in any feature map $\\phi$ but only those which do not disrupt the approximation capabilities of the hypothesis class $\\mathscr{F}$.  Specifically we require that $\\phi$ satisfy:\n",
    " 1. If $\\mathscr{F}$ is a universal approximator then so is $\\mathscr{F}\\circ \\phi$,\n",
    " 2. If $f\\in\\mathscr{F}$ is not $0$ then so is $f\\circ \\phi$,\n",
    " 3. $\\phi$ is continuous.\n",
    " \n",
    "**Note**:  The implicit requirement that $\\phi$ is compatable with $\\hat{f}$, in the sense that $\\hat{f}\\circ \\phi$ is well-defined, implies that $\\phi$ preseves the dimension of the input space.  This is in contrast to the Kernel methods where the implicit feature map requires a high-dimensional feature space to perform its linearizing representation.  \n",
    "\n",
    "If such a $\\phi$ exists, then it is a-priori unknown.  We seek a class of algorithmically generated feature maps which satisfy $1-3$ and can approximate any other feature map satisfying $1-3$.  We call this the *universal linearization property* and we show that NEU embeds this property into $\\mathscr{F}$.  \n",
    "\n",
    "\n",
    "**Improving the input$\\times$output relationship:**\n",
    "\n",
    "Another challenged faced by some learning models, especially classical models such as ordinary linear regression, is that they my not be expressive enough to represent the unknown function $f$.  For example, if $\\mathscr{F}$ is the set of linear models $\\hat{f}(x) = Ax +b$ and if $f(x) = x_n^2$ then one can verify that\n",
    "$$\n",
    "\\min_{A,b}\\max_{\\|x\\|\\leq 1} \\|\\hat{f}(x) - f(x)\\|>0.\n",
    "$$\n",
    "Suppose that $\\hat{f}$ has been selected and is therefore fixed.  \n",
    "\n",
    "One may ask if there is a way to add enough flexibility to $\\hat{f}$ so as that it can describe *any input-output relation* between the relevant input-output spaces; while not impeeding the expressivness of the model class $\\mathscr{F}$.  That is, we seek a class of *structure maps* on the input-output space $\\mathbb{R}^d\\times \\mathbb{R}^D\\rightarrow \\mathbb{R}^D$ satisfying:\n",
    " \n",
    " 1. Given any $\\hat{f}\\in \\mathscr{F}$, there is a structure map $\\rho$ satisfying $\\rho(\\hat{f}(x),x)\\approx f(x)$\n",
    " 2. For every structure map $\\rho$, if $\\mathscr{F}$ is a universal approximator then so is the set $\\left\\{\\rho(\\hat{f}(\\cdot),\\cdot):\\hat{f}\\in \\mathscr{F}\\right\\}$.\n",
    "\n",
    "We call this the *universal approximation embedding property*.  \n",
    "\n",
    "**What is NEU:**\n",
    "Non-Euclidean upgrading, or NEU, is a tranformation of the learning problem which embues the original model class with the *universal linearization* and with the *universal approximation embedding* properties while also modifying the loss-function so as to improve the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and Solution of the minimal dimensional linearization problem.\n",
    "### Why not used narrow DNNs to achieve low dimensional (co-)representations?\n",
    "Typical feed-forward layers are of the form:\n",
    "$$\n",
    "x\\mapsto \\sigma\\left(Ax -c\\right),\n",
    "$$\n",
    "where $A$ is a $d_i\\times d_{i+1}$ matrix ($d_i,d_{i+1}\\in \\mathbb{N}_+$).  However, these layers fail to have many of the desirable properties a feature (or dually a readout) map should have in order to acheive a generic low-dimensional representation of the input space (output space).  \n",
    "\n",
    "---\n",
    "In [the article](https://arxiv.org/abs/2006.02341) it was shown that many of the desirable of a feature (resp. readout) map are satisfies by a feed-forward layer if:\n",
    " 1. $\\sigma$ is a homeomorphism (such as is the case for the [Leaky-ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLU) or for the [Swish](https://www.tensorflow.org/api_docs/python/tf/keras/activations/swish) activation functions.\n",
    " 2. The connection, determined by the matrix $A$, are structured in a way that $A$ is invertible (and in particular a square matrix).  \n",
    "---\n",
    "### Why now use narrow DNNs with well-behaved connections and good activation function?\n",
    "Even if conditions $1$ and $2$ are satisfied there is no reason that such a network is *universal* amongst the class of feature (resp. readout) maps withthe \"good\" properties outlined in the article.  However, if allow $A$ to depend on the input space in a very precise way, then we can extend this class of DNNs to a larger neural network architecture, called a *reconfiguration network*, which:\n",
    " - Satisfies the \"good\" feature (resp. readout) map conditions\n",
    " - Is universal amongst the class of feature (resp. readout) maps satisfying these condition.\n",
    "\n",
    "### Solution: Reconfiguration Units\n",
    "The precise desription of the layers in this network are:\n",
    "$$\n",
    "x\\mapsto \\sigma_{\\alpha}\\left(\n",
    "A(x)(x-c)\n",
    "\\right) +b\n",
    "$$\n",
    "where $A:\\mathbb{R}^d\\rightarrow \\operatorname{GL}_{d\\times d}(\\mathbb{R})$ is the invertible-matrix valued function defined as:\n",
    "$$\n",
    "A(x)\\triangleq \\exp\\left(\n",
    "A_0 + \\operatorname{Skw}(f_1)(x) (\\|x-c\\|^2-\\sigma)(\\|x-c\\|^2+\\sigma)I_{\\|x-c\\|^2<\\sigma}\n",
    "+ \\operatorname{Skw}(f_2)(x) e^{-\\gamma \\|x-c\\|^2}\n",
    "\\right)\n",
    "$$\n",
    "and $\\sigma_a$ is an $1$-parameter activation function, such as the *swish activation function* with trainable parameter, which satisfies 1. for each value of $a$ and for which $\\sigma_0(x)=x$; thus it has the capability of not further altering the learning model's inputs (resp. output) if so desired.  \n",
    "\n",
    "---\n",
    "\n",
    "## Description of NEU:\n",
    "The NEU meta-algorithm learns a geometry for the input and (input $\\times$ output) spaces by deforming them with a universal class of homeomorphisms + robustifies the involved loss functions to improve generalizability of the new and very flexible model.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "f \\mapsto& \\, \\rho \\circ f \\circ \\phi\\\\\n",
    "\\mathbb{E}_{\\mathbb{P}}[\\ell(f(X))] \\mapsto & \\,\\max_{\\mathbb{Q}\\sim \\mathbb{P}}\\, \\mathbb{E}_{\\mathbb{Q}}[\\ell(\\rho(\\phi(X), f\\circ \\phi(X)))].\n",
    "\\end{aligned}\n",
    "$$\n",
    "$\\rho=\\pi\\circ \\tilde{\\rho}$, and $\\tilde{\\rho}$ and $\\phi$ are \"universal homeomorphisms\" on $\\operatorname{dom}(f)$ and on $\\operatorname{dom}(f)\\times \\operatorname{co-dom}(f)$, respectively.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does this repository contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Simulated Regression Problem Benchmarking/Scrutinizing NEU's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Description of regression problem: \n",
    "In this notebook we implement the regression problem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_i =&  \\,f(x_i)\\delta_i + \\epsilon_i, \\qquad i=1,\\dots,N\\\\\n",
    "\\epsilon_i \\sim &\\, \\mathcal{N}(0,\\sigma),\\\\\n",
    "\\delta_i\\sim &  \\,U(1-D,1+D),\n",
    "\\end{aligned}\n",
    "$$\n",
    "for some *variance* $\\sigma>0$ and *degree of model misspecification level* $0<D<1$.  \n",
    "The quantity $\\epsilon$ can be understood as, classical, additive noise while the quantity $\\delta$ represents multiplicative noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Functions from the paper:\n",
    " - 1) $\\min\\{\\exp(\\frac{-1}{(1+x)^2}),x+\\cos(x)\\}$. Reason: Evaluate performance for pasted functions and general badness.\n",
    " - 2) $\\cos(\\exp(-x))$.  Reason: Evaluate performance for non-periodic osculations.\n",
    " - 3) $I_{(-\\infty,\\frac1{2})}$.  Reason: Evaluation performance on a single jump.  \n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
