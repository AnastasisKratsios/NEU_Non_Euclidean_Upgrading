{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility of Functions for Building NEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building NEU\n",
    "Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in, depth, height):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    \n",
    "    # Apply Reconfiguration Unit\n",
    "    output_layer  = Reconfiguration_unit(height)(input_layer)\n",
    "    \n",
    "    if depth > 0:\n",
    "        output_layer = Shift_Layers(d+D)(output_layer)\n",
    "        output_layer  = Reconfiguration_unit(height)(output_layer)\n",
    "        output_layer = Shift_Layers(d+D)(output_layer)\n",
    "    \n",
    "    # Output\n",
    "#     output_layer = projection_layer(output_layer)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in, depth, height):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = Reconfiguration_unit(d+D)(layers[len(layers)-2].output)\n",
    "\n",
    "    if depth > 0:\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "        output_layer_new  = Reconfiguration_unit(height)(output_layer_new)\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in, height):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = Euclidean_Layer(d+D)(layers[len(layers)-2].output)\n",
    "    output_layer_new  = Reconfiguration_unit(height)(output_layer_new)\n",
    "    output_layer_new  = Euclidean_Layer(d+D)(output_layer_new)\n",
    "\n",
    "    # Output Layer\n",
    "#     output_layer_new = projection_layer(output_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "\n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-5), momentum=0.01, nesterov=True)\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "# def Robust_MSE(p):\n",
    "#     def loss(y_true, y_predicted):\n",
    "#         ses = Kb.pow(y_true-y_predicted,2)\n",
    "#         above = above_percentile(Kb.flatten(ses), p)\n",
    "#         return Kb.mean(above)\n",
    "#     return loss\n",
    "def Robust_MSE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    y_true.shape = (y_true.shape[0], 1)\n",
    "    y_pred.shape = (y_pred.shape[0], 1)\n",
    "\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = np.abs((y_true - y_pred))\n",
    "    loss_out = np.math.exp(-p*loss_out)\n",
    "    loss_out = np.mean(loss_out)\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Robust_MSE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    y_true.shape = (y_true.shape[0], 1)\n",
    "    y_pred.shape = (y_pred.shape[0], 1)\n",
    "\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = np.abs((y_true - y_pred))\n",
    "    loss_out = np.math.exp(-p*loss_out)\n",
    "    loss_out = np.mean(loss_out)\n",
    "    return loss_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
