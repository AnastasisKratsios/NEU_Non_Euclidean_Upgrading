{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Utility\n",
    "\n",
    "This notebook contains the benchmark models to compare NEU against:\n",
    "\n",
    " - Deep $\\operatorname{GL}_d$-Nets are the largest sub-class of deep feed-forward networks with invertible weights. \n",
    " - Deep $\\operatorname{E}_d$-Nets are the largest sub-class of deep feed-forward networks with isometric weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Custom Layer(s)\n",
    "%run Special_Layers_Homeomorphic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep General Linear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model_deep_GLd(trainx, trainy, Full_Epochs_in, depth, height):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    inputs_ffNN = tf.keras.Input(shape=[d])\n",
    "    \n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN)\n",
    "    x_ffNN = tf.nn.relu(x_ffNN)\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)\n",
    "    x_ffNN = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    if depth > 0:\n",
    "        x_ffNN = tf.nn.leaky_relu(x_ffNN)\n",
    "        x_ffNN = Deep_GLd_Layer(d+D)(x_ffNN)\n",
    "        \n",
    "    output_layer = Deep_GLd_Layer(d+D)(x_ffNN)\n",
    "    \n",
    "    # Output\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[inputs_ffNN], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=10)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code builds implements NEU using the $GL_d$-feed-forward architecture with Leaky-ReLU, *the largest invertible sub-architecture of the feed-forward architecture.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration_GLd(model_greedy_initialized, trainx, trainy, Full_Epochs_in, depth):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Add GLd Layers\n",
    "    output_layer_new = tf.nn.leaky_relu(layers[len(layers)-1].output)\n",
    "    output_layer_new  = Deep_GLd_Layer(d+D)(output_layer_new)\n",
    "    \n",
    "    if depth > 1:\n",
    "        output_layer_new = tf.nn.leaky_relu(output_layer_new)\n",
    "        output_layer_new = Deep_GLd_Layer(d+D)(output_layer_new)\n",
    "    \n",
    "    # Add Projection Layer (constructs full readout map)\n",
    "    output_layer_new = projection_layer(output_layer_new)   \n",
    "    \n",
    "    # build model\n",
    "    reconfiguration_GLd = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    \n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-4))\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration_GLd.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse','mae'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_GLd.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration_GLd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration_Ed(model_greedy_initialized, trainx, trainy, Full_Epochs_in, depth):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Add GLd Layers\n",
    "    output_layer_new = tf.nn.leaky_relu(layers[len(layers)-1].output)\n",
    "    output_layer_new  = Euclidean_Layer(d+D)(output_layer_new)\n",
    "    \n",
    "    if depth > 1:\n",
    "        output_layer_new = tf.nn.leaky_relu(output_layer_new)\n",
    "        output_layer_new = Euclidean_Layer(d+D)(output_layer_new)\n",
    "    \n",
    "    # Add Projection Layer (constructs full readout map)\n",
    "    output_layer_new = projection_layer(output_layer_new)   \n",
    "    \n",
    "    # build model\n",
    "    reconfiguration_Ed = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    \n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-4))\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration_Ed.compile(loss = 'mse',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse','mae'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_Ed.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration_Ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Euclidean Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model_deep_Euclidean_Networks(trainx, trainy, Full_Epochs_in, depth, height):\n",
    "    # Define Models\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    inputs_ffNN = tf.keras.Input(shape=[d])\n",
    "    \n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN)\n",
    "    x_ffNN = tf.nn.relu(x_ffNN)\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)\n",
    "    x_ffNN = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    if depth > 0:\n",
    "        x_ffNN = tf.nn.leaky_relu(x_ffNN)\n",
    "        x_ffNN = Euclidean_Layer(d+D)(x_ffNN)\n",
    "        \n",
    "    output_layer = Euclidean_Layer(d+D)(x_ffNN)\n",
    "    \n",
    "    # Output\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[inputs_ffNN], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=10)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2020)\n",
    "\n",
    "# define and fit the base model\n",
    "def get_OLS(trainx, trainy, Full_Epochs_in, height):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d])\n",
    "    \n",
    "    # Apply Shallow NN Layer #\n",
    "    #------------------------#\n",
    "    output_layer = fullyConnected_Dense(height)(input_layer)\n",
    "    output_layer = tf.nn.relu(output_layer)\n",
    "    output_layer = fullyConnected_Dense(D)(output_layer)\n",
    "    \n",
    "#     output_layer = projection_layer(output_layer)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2))\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mae',#Robust_MSE,\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
