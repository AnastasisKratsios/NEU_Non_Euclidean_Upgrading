{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility of Functions for Building NEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness Parameter\n",
    "robustness_parameter = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE_ES(p):\n",
    "    def ES_p_loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def Robust_MSE(robustness_parameter):\n",
    "#     # Define Entropic Loss\n",
    "#     def Entropic_loss(y_true, y_pred):\n",
    "#         # Compute Exponential Utility\n",
    "#         loss_out = tf.math.abs((y_true - y_pred))\n",
    "#         loss_out = tf.math.exp(-robustness_parameter*loss_out)\n",
    "#         loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "#         # Return Value\n",
    "#         return loss_out\n",
    "#     return Entropic_loss\n",
    "# # \n",
    "# # def Robust_MSE(y_true, y_pred):\n",
    "# #     # Compute Exponential Utility\n",
    "# #     loss_out = tf.math.abs((y_true - y_pred))\n",
    "# #     loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "# #     loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "# #     # Return Value\n",
    "# #     return loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization Loss-Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Robust_MSE(robustness_parameter):\n",
    "    # Define Entropic Loss\n",
    "    def Entropic_loss(y_true, y_pred):\n",
    "        # Initialize Loss\n",
    "        absolute_errors_eval = tf.math.abs((y_true - y_pred))\n",
    "        \n",
    "        # Compute Exponential        \n",
    "        loss_out_expweights = tf.math.exp(-robustness_parameter*absolute_errors_eval)\n",
    "        loss_out_expweights_totals = tf.math.reduce_sum(loss_out_expweights)\n",
    "        loss_out_weighted = loss_out_expweights/tf.math.reduce_sum(loss_out_expweights)\n",
    "        loss_out_weighted = loss_out_weighted*absolute_errors_eval\n",
    "        loss_out_weighted = tf.math.reduce_sum(loss_out_weighted)\n",
    "        \n",
    "        # Compute Average Loss\n",
    "        loss_average = tf.math.reduce_mean(absolute_errors_eval)\n",
    "\n",
    "        # Return Value\n",
    "        loss_out = loss_out_weighted - loss_average\n",
    "        \n",
    "        return loss_out\n",
    "    return Entropic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building NEU\n",
    "Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in, depth, height):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    \n",
    "    # Apply Reconfiguration Unit\n",
    "    output_layer  = Reconfiguration_unit(height)(input_layer)\n",
    "    \n",
    "    if depth > 0:\n",
    "        output_layer = Shift_Layers(d+D)(output_layer)\n",
    "        output_layer  = Reconfiguration_unit(height)(output_layer)\n",
    "        output_layer = Shift_Layers(d+D)(output_layer)\n",
    "    \n",
    "    # Output\n",
    "#     output_layer = projection_layer(output_layer)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-6))\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mae',#Robust_MSE,\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in, depth, height):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = Reconfiguration_unit(d+D)(layers[len(layers)-2].output)\n",
    "\n",
    "    if depth > 0:\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "        output_layer_new  = Reconfiguration_unit(height)(output_layer_new)\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-6))\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = 'mae',#Robust_MSE,\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in, height):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = Shift_Layers(d+D)(layers[len(layers)-2].output)\n",
    "    output_layer_new  = Reconfiguration_unit(height)(output_layer_new)\n",
    "    output_layer_new  = Shift_Layers(d+D)(output_layer_new)\n",
    "\n",
    "    # Output Layer\n",
    "#     output_layer_new = projection_layer(output_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "\n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-4))\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = Robust_MSE(.3),\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse','mae'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
