{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility of Functions for Building NEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE_ES(p):\n",
    "    def ES_p_loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Version (Formulation with same arg-minima)\n",
    "# @tf.function\n",
    "def Robust_MSE(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = tf.math.abs((y_true - y_pred))\n",
    "    loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "    loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "    # Return Value\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Version (Full dual Version)\n",
    "def Robust_MSE_numpy(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = np.abs((y_true - y_pred))\n",
    "    loss_out = np.exp(robustness_parameter*loss_out)\n",
    "    loss_out = np.mean(loss_out)\n",
    "    loss_out = np.log(loss_out)/robustness_parameter\n",
    "    # Return Value\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Base Models\n",
    "## Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hidden layer neural network\n",
    "def def_model_Regression(trainx, trainy, Pre_Epochs_in, height, depth,learning_rate):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    \n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(d,))\n",
    "    \n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN)\n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        # Sigmoid Activation\n",
    "        #x_ffNN = tf.math.sigmoid(x_ffNN)\n",
    "        # Leaky ReLU Activation\n",
    "        #x_ffNN = Rescaled_Leaky_ReLU()(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #-###################-#\n",
    "    # Define Output Layer #\n",
    "    #-###################-#\n",
    "    outputs_ffNN = fullyConnected_Dense(D)(x_ffNN)        \n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, outputs_ffNN)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "\n",
    "    ffNN.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    \n",
    "    ffNN.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=1)\n",
    "\n",
    "    return ffNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconfigurations and NEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in, depth, height):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    \n",
    "    # Apply Reconfiguration Unit #\n",
    "    #----------------------------#\n",
    "    output_layer  = Reconfiguration_unit(height)(output_layer)\n",
    "    \n",
    "    if depth > 0:\n",
    "        output_layer = Shift_Layers(d+D)(output_layer)\n",
    "        output_layer  = Reconfiguration_unit(height)(output_layer)\n",
    "        output_layer = Shift_Layers(d+D)(output_layer)\n",
    "    \n",
    "#     output_layer = projection_layer(output_layer)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-6))\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mae',#Robust_MSE,\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in, depth, height):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = Reconfiguration_unit(d+D)(layers[len(layers)-1].output)\n",
    "\n",
    "    if depth > 0:\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "        output_layer_new  = Reconfiguration_unit(height)(output_layer_new)\n",
    "        output_layer_new = Shift_Layers(d+D)(output_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-6))\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = 'mae',#Robust_MSE,\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in, height):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = Shift_Layers(d+D)(layers[len(layers)-2].output)\n",
    "    output_layer_new  = Reconfiguration_unit(height)(output_layer_new)\n",
    "    output_layer_new  = Shift_Layers(d+D)(output_layer_new)\n",
    "\n",
    "    # Output Layer\n",
    "#     output_layer_new = projection_layer(output_layer_new)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "    \n",
    "    # Add Projection Layer (constructs full readout map)\n",
    "    output_layer_new = projection_layer(output_layer_new)\n",
    "    \n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-4))\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = Robust_MSE,\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse','mae'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEU - Regerssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hidden layer neural network\n",
    "def get_base_model_Regression(trainx, trainy, Pre_Epochs_in, height, depth, learning_rate):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    \n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(d,))\n",
    "    \n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN)\n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #-###################-#\n",
    "    # Define Output Layer #\n",
    "    #-###################-#\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)     \n",
    "    output_layer = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "\n",
    "    ffNN.compile(optimizer=opt, loss=Robust_MSE, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    \n",
    "    ffNN.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=2)\n",
    "\n",
    "    return ffNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
