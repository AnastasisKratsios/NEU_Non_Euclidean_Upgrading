{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on current version:\n",
    "**To Try**:\n",
    "- [Cayley parameterization](https://planetmath.org/cayleysparameterizationoforthogonalmatrices) of $SU_d$ (since this is really all we need)...*will it be more stable than Lie's parameterization?* Note: it is a homeomorphism so this is great for UAP!\n",
    "- SVD approach to pre-trainining \n",
    "  - (here for procrustes problem)[https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem]\n",
    "  - [here for complexity](https://mathoverflow.net/questions/161252/what-is-the-time-complexity-of-truncated-svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEU (Reconfigurations Map and Related Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Algorithm (NEU-OLS)\n",
    "\n",
    "1. Perform Basic Algorithm (in this case OLS)\n",
    "2. Map predictions to their graph; ie $x\\mapsto (x,\\hat{f}_{OLS}(x))$ where $\\hat{f}_{OLS}$ is the least-squares regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning & ML\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras as K\n",
    "import keras.backend as Kb\n",
    "from keras.layers import *\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import utils as np_utils\n",
    "from scipy import linalg as scila\n",
    "\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.constraints import NonNeg\n",
    "\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Alerts\n",
    "import os as beepsnd\n",
    "\n",
    "# Others\n",
    "import math\n",
    "\n",
    "# General Outputs\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_Reconfigurations = 200\n",
    "d = 1 # Dimension of X\n",
    "D = 1 # Dimension of Y\n",
    "\n",
    "\n",
    "# Data Meta-Parameters\n",
    "noise_level = 0.1\n",
    "uncertainty_level= 0.5\n",
    "\n",
    "# Training meta-parameters\n",
    "Pre_Epochs = 100\n",
    "Full_Epochs = 800\n",
    "\n",
    "# # Height Per Reconfiguration\n",
    "# Height_factor_Per_reconfig = d+D\n",
    "\n",
    "# Number of Datapoints\n",
    "N_data = 10**3\n",
    "# Unknown Function\n",
    "def unknown_f(x):\n",
    "    return np.sin(x) #+ (x % 2)\n",
    "\n",
    "# Generate Data\n",
    "%run Data_Generator.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for NEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape Data Into Compatible Shape\n",
    "data_x = np.array(data_x).reshape(-1,d)\n",
    "data_y = np.array(data_y)\n",
    "# Perform OLS Regression\n",
    "linear_model = LinearRegression()\n",
    "reg = linear_model.fit(data_x, data_y)\n",
    "model_pred_y = linear_model.predict(data_x)\n",
    "# Map to Graph\n",
    "data_NEU = np.concatenate((data_x,model_pred_y.reshape(-1,D)),1)\n",
    "NEU_targets  = data_y.reshape(-1,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #samples to get:\n",
    "        #you can choose tf.math.ceil above, it depends on whether you want to\n",
    "        #include or exclude one element. Suppose you you want 33% top,\n",
    "        #but it's only possible to get exactly 30% or 40% top:\n",
    "        #floor will get 30% top and ceil will get 40% top.\n",
    "        #(exact matches included in both cases)\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "# def Robust_MSE(p):\n",
    "#     def loss(y_true, y_predicted):\n",
    "#         ses = Kb.pow(y_true-y_predicted,2)\n",
    "#         above = above_percentile(Kb.flatten(ses), p)\n",
    "#         return Kb.mean(above)\n",
    "#     return loss\n",
    "def Robust_MSE(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    y_true.shape = (y_true.shape[0], 1)\n",
    "    y_pred.shape = (y_pred.shape[0], 1)\n",
    "\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = np.abs((y_true - y_pred))\n",
    "    loss_out = np.math.exp(-p*loss_out)\n",
    "    loss_out = np.mean(loss_out)\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Reconfiguration Unit\n",
    "*Lie Version:* $$\n",
    "x \\mapsto \\exp\\left(\n",
    "%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) x.\n",
    "$$\n",
    "\n",
    "*Cayley version:*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x \\mapsto & \\left[(I_d + A(x))(I- A(x))^{-1}\\right]x\n",
    "\\\\\n",
    "A(x)\\triangleq &%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "tf.linalg.inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Reconfiguration_unit, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(self.units,),\n",
    "                                    initializer='random_normal',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "#         self.sigma = self.add_weight(name='bump_threshfold',\n",
    "#                                         shape=[1],\n",
    "#                                         initializer=RandomUniform(minval=.5, maxval=1),\n",
    "#                                         trainable=True,\n",
    "#                                         constraint=tf.keras.constraints.NonNeg())\n",
    "#         self.a = self.add_weight(name='bump_scale',\n",
    "#                                         shape=[1],\n",
    "#                                         initializer='ones',\n",
    "#                                         trainable=True)\n",
    "#         self.b = self.add_weight(name='bump_location',\n",
    "#                                         shape=[1],\n",
    "#                                         initializer='zeros',\n",
    "#                                         trainable=True)\n",
    "\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1 ',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2 ',shape=(input_shape[-1],input_shape[-1]),initializer='GlorotUniform',trainable=True)\n",
    "\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',shape=(input_shape[-1],input_shape[-1]),initializer='GlorotUniform',trainable=True)\n",
    "        self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',shape=[1],initializer=RandomUniform(minval=0.0, maxval=0.01),trainable=True,constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.math.reduce_sum((input*input)) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "#         bump_input = self.a *norm_inputs + self.b\n",
    "#         greater = tf.math.greater(bump_input, -self.sigma)\n",
    "#         less = tf.math.less(bump_input, self.sigma)\n",
    "#         condition = tf.logical_and(greater, less)\n",
    "\n",
    "#         output_bump = tf.where(\n",
    "#             condition, \n",
    "#             self.bump_function(bump_input),\n",
    "#             0.0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        tangential_ffNN = norm_inputs*self.Tw1\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)  \n",
    "        tangential_ffNN = tf.matmul(tangential_ffNN, self.Tw2) \n",
    "    \n",
    "        # Scale DNN by Radial-Data\n",
    "#         tangential_ffNN = output_bump*tangential_ffNN\n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "#         tangential_ffNN = output_bump*tangential_ffNN\n",
    "        tangential_ffNN = tangential_ffNN + self.num_stab_param*tf.linalg.diag(tf.ones(d+D))\n",
    "    \n",
    "        # Lie Parameterization (Problematic)\n",
    "        #tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        # Cayley Transformation (Experimental)\n",
    "        tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.inv(self.Id - tangential_ffNN))\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,(input-self.location)) + self.location\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit the base model\n",
    "def get_base_model(trainx, trainy, Pre_Epochs_in):\n",
    "    # Define Model\n",
    "    #----------------#\n",
    "    # Initialize\n",
    "    input_layer = tf.keras.Input(shape=[d+D])\n",
    "    # Apply Reconfiguration Unit\n",
    "    reconfiguration_unit  = Reconfiguration_unit(d+D)(input_layer)\n",
    "    # Output\n",
    "    output_layer = projection_layer(reconfiguration_unit)\n",
    "    reconfiguration_basic = tf.keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # Compile Model\n",
    "    #----------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile\n",
    "    reconfiguration_basic.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "    \n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration_basic.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "        \n",
    "    # Return Output\n",
    "    return reconfiguration_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Initialization of Subsequent Units\n",
    "Build reconfiguration and pre-train using greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reconfiguration_unit_greedily(model, trainx, trainy, Pre_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit(d+D)(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(new_reconfiguration_unit)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-2), momentum=0.01, nesterov=True)\n",
    "    # Compile Model\n",
    "    new_model.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    new_model.fit(trainx, trainy, epochs=Pre_Epochs_in, verbose=0)\n",
    "\n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Compile (entire) reconfiguration using greedy-initializations past from previous helper functions.\n",
    "Train reconfiguration together (initialized by greedy) layer-wise initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_reconfiguration(model_greedy_initialized, trainx, trainy, Full_Epochs_in):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model_greedy_initialized.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    new_reconfiguration_unit  = Reconfiguration_unit(d+D)(layers[len(layers)-2].output)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer_new = projection_layer(new_reconfiguration_unit)\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layers[i].trainable = True\n",
    "\n",
    "\n",
    "    # build model\n",
    "    reconfiguration = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    #new_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Compile new Model\n",
    "    #-------------------#\n",
    "    # Define Optimizer\n",
    "    optimizer_on = tf.keras.optimizers.SGD(learning_rate=10**(-5), momentum=0.01, nesterov=True)\n",
    "    #optimizer_on = tf.keras.optimizers.Adagrad(learning_rate=10**(-5), initial_accumulator_value=0.1, epsilon=1e-07,name='Adagrad')\n",
    "\n",
    "    # Compile Model\n",
    "    reconfiguration.compile(loss = 'mae',\n",
    "                    optimizer = optimizer_on,\n",
    "                    metrics = ['mse'])\n",
    "\n",
    "    # Fit Model\n",
    "    #----------------#\n",
    "    reconfiguration.fit(trainx, trainy, epochs=Full_Epochs_in, verbose=1)\n",
    "\n",
    "    # Return Output\n",
    "    return reconfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14249677155805995\n",
      "0.005\n",
      "1.8868288316444468\n",
      "0.01\n",
      "0.15521904750731227\n",
      "0.015\n",
      "0.15535795629066435\n",
      "0.02\n",
      "0.13181194526577245\n",
      "0.025\n",
      "0.13238689013604213\n",
      "0.03\n",
      "1.4697877369026926\n",
      "0.035\n",
      "0.1567292078784923\n",
      "0.04\n",
      "1.3912593030957079\n",
      "0.045\n",
      "0.10006457664894106\n",
      "0.05\n",
      "1.5601539456466014\n",
      "0.055\n",
      "0.09384572869590664\n",
      "0.06\n",
      "1.6363498128613998\n",
      "0.065\n",
      "0.09732008156543534\n",
      "0.07\n",
      "1.6580994056631202\n",
      "0.075\n",
      "0.10302590635563975\n",
      "0.08\n",
      "0.11028647880751972\n",
      "0.085\n",
      "1.7053652847038276\n",
      "0.09\n",
      "0.11881907618367006\n",
      "0.095\n",
      "1.7289716378665634\n",
      "0.1\n",
      "0.16627547648629584\n",
      "0.105\n",
      "1.4447979915150577\n",
      "0.11\n",
      "0.13258727008601084\n",
      "0.115\n",
      "1.513406758589115\n",
      "0.12\n",
      "0.15907785924890275\n",
      "0.125\n",
      "0.3827125362624159\n",
      "0.13\n",
      "1.985244190488314\n",
      "0.135\n",
      "1.985303074118812\n",
      "0.14\n",
      "0.4068619562795264\n",
      "0.145\n",
      "1.9674522663644678\n",
      "0.15\n",
      "0.3673257113860202\n",
      "0.155\n",
      "2.100787369658784\n",
      "0.16\n",
      "0.35762443188222803\n",
      "0.165\n",
      "2.170135047700476\n",
      "0.17\n",
      "0.3579212237376866\n",
      "0.175\n",
      "2.216267154639846\n",
      "0.18\n",
      "0.387088867301748\n",
      "0.185\n",
      "0.3868274593159905\n",
      "0.19\n",
      "1.9972987019267163\n",
      "0.195\n",
      "0.38746566582843495\n",
      "0.2\n",
      "2.103525267349988\n",
      "0.205\n",
      "0.4092940789924682\n",
      "0.21\n",
      "2.1899152167229636\n",
      "0.215\n",
      "0.3920251938111933\n",
      "0.22\n",
      "2.268479931929316\n",
      "0.225\n",
      "0.3970053098007163\n",
      "0.23\n",
      "2.0574416129014024\n",
      "0.235\n",
      "0.4172785574122598\n",
      "0.24\n",
      "0.6257018533836192\n",
      "0.245\n",
      "1.3198070304300102\n",
      "0.25\n",
      "0.6459386838240755\n",
      "0.255\n",
      "1.3175048818056057\n",
      "0.26\n",
      "0.5798966171848277\n",
      "0.265\n",
      "0.5794846419621004\n",
      "0.27\n",
      "0.5794818708664955\n",
      "0.275\n",
      "0.5803814962356153\n",
      "0.28\n",
      "0.5817667730134278\n",
      "0.285\n",
      "1.363192575010042\n",
      "0.29\n",
      "0.5996024215605816\n",
      "0.295\n",
      "1.3397994314211692\n",
      "0.3\n",
      "0.587833076012508\n",
      "0.305\n",
      "1.3263548854002019\n",
      "0.31\n",
      "0.5878258693696825\n",
      "0.315\n",
      "1.3488434057596133\n",
      "0.32\n",
      "0.5579767141809745\n",
      "0.325\n",
      "1.3516163124117748\n",
      "0.33\n",
      "0.5542186958141581\n",
      "0.335\n",
      "1.3906237596700892\n",
      "0.34\n",
      "0.5326273284436344\n",
      "0.345\n",
      "0.5338692704416704\n",
      "0.35\n",
      "1.3646120926354477\n",
      "0.355\n",
      "0.5483859305574976\n",
      "0.36\n",
      "1.3295866348456507\n",
      "0.365\n",
      "0.5435635138751852\n",
      "0.37\n",
      "1.3478562007401436\n",
      "0.375\n",
      "0.5536652231024143\n",
      "0.38\n",
      "1.3570462771858374\n",
      "0.385\n",
      "0.556728690512233\n",
      "0.39\n",
      "1.414236383615195\n",
      "0.395\n",
      "0.5852891716062292\n",
      "0.4\n",
      "0.4283643769442572\n",
      "0.405\n",
      "2.0648033073694205\n",
      "0.41\n",
      "0.37917451422444165\n",
      "0.415\n",
      "2.227941609325914\n",
      "0.42\n",
      "0.38975434903121164\n",
      "0.425\n",
      "2.2793353929498634\n",
      "0.43\n",
      "0.4896295197437426\n",
      "0.435\n",
      "0.48959153119930066\n",
      "0.44\n",
      "0.4993801635071917\n",
      "0.445\n",
      "1.9830608406579957\n",
      "0.45\n",
      "0.4269561971439846\n",
      "0.455\n",
      "2.0747438639189193\n",
      "0.46\n",
      "0.5156198947438533\n",
      "0.465\n",
      "0.5025586560210651\n",
      "0.47\n",
      "2.023638225578945\n",
      "0.475\n",
      "0.5215606847500489\n",
      "0.48\n",
      "2.03124953584289\n",
      "0.485\n",
      "0.5479723873593666\n",
      "0.49\n",
      "0.5560398421461686\n",
      "0.495\n",
      "1.9747004505380725\n",
      "0.5\n",
      "0.5099307915666041\n",
      "0.505\n",
      "2.0904597292998544\n",
      "0.51\n",
      "0.565632518784781\n",
      "0.515\n",
      "0.5669704397034334\n",
      "0.52\n",
      "1.9778384834090452\n",
      "0.525\n",
      "0.5880166091747235\n",
      "0.53\n",
      "1.9715951687367672\n",
      "0.535\n",
      "0.5145301237516281\n",
      "0.54\n",
      "2.0580971798608245\n",
      "0.545\n",
      "0.4710519178849817\n",
      "0.55\n",
      "0.4717316078304009\n",
      "0.555\n",
      "0.48893910078744257\n",
      "0.56\n",
      "2.230384113877077\n",
      "0.565\n",
      "0.4703051721984596\n",
      "0.57\n",
      "2.3411683779830628\n",
      "0.575\n",
      "0.6011766056065615\n",
      "0.58\n",
      "2.04282698108414\n",
      "0.585\n",
      "0.5533563739899752\n",
      "0.59\n",
      "2.126416070524037\n",
      "0.595\n",
      "0.6116452697896526\n",
      "0.6\n",
      "2.0050668635899274\n",
      "0.605\n",
      "0.6337284774170671\n",
      "0.61\n",
      "2.015267404256175\n",
      "0.615\n",
      "0.6191559901582472\n",
      "0.62\n",
      "1.9584758988196245\n",
      "0.625\n",
      "0.6043908228131057\n",
      "0.63\n",
      "2.0022284680986657\n",
      "0.635\n",
      "0.6932215195538377\n",
      "0.64\n",
      "1.9311295335069993\n",
      "0.645\n",
      "0.6591485716844028\n",
      "0.65\n",
      "2.03388372740962\n",
      "0.655\n",
      "0.6086135604163868\n",
      "0.66\n",
      "2.140741724798261\n",
      "0.665\n",
      "0.6691797446329556\n",
      "0.67\n",
      "2.029296065444636\n",
      "0.675\n",
      "0.6706833286427922\n",
      "0.68\n",
      "1.9699289379138187\n",
      "0.685\n",
      "0.7021323535233769\n",
      "0.69\n",
      "1.9648175680732838\n",
      "0.695\n",
      "0.7031754434838574\n",
      "0.7\n",
      "0.7062974380644356\n",
      "0.705\n",
      "1.9514222245760415\n",
      "0.71\n",
      "1.9513214605011944\n",
      "0.715\n",
      "0.7196225190139224\n",
      "0.72\n",
      "1.9435463138172764\n",
      "0.725\n",
      "0.7010232473321227\n",
      "0.73\n",
      "0.7069625225427942\n",
      "0.735\n",
      "1.942715493463445\n",
      "0.74\n",
      "1.9427154939583433\n",
      "0.745\n",
      "0.7073124569339935\n",
      "0.75\n",
      "1.8812376120506393\n",
      "0.755\n",
      "0.6983050772070836\n",
      "0.76\n",
      "1.9537767517502507\n",
      "0.765\n",
      "0.7007500105909914\n",
      "0.77\n",
      "1.9989533722304222\n",
      "0.775\n",
      "0.6704071839108303\n",
      "0.78\n",
      "1.9594458205827852\n",
      "0.785\n",
      "0.6237924422819396\n",
      "0.79\n",
      "2.0282889761897627\n",
      "0.795\n",
      "0.5967705789032614\n",
      "0.8\n",
      "2.102832689709332\n",
      "0.805\n",
      "2.1032344221462003\n",
      "0.81\n",
      "0.5685379632810814\n",
      "0.815\n",
      "2.17023581062581\n",
      "0.82\n",
      "0.534587010237748\n",
      "0.825\n",
      "2.2219605675842087\n",
      "0.83\n",
      "0.6200860787182353\n",
      "0.835\n",
      "2.1272577912352117\n",
      "0.84\n",
      "0.6662938744677386\n",
      "0.845\n",
      "1.9233262821271415\n",
      "0.85\n",
      "0.6671763191267036\n",
      "0.855\n",
      "1.9080008453784334\n",
      "0.86\n",
      "0.6504077328576293\n",
      "0.865\n",
      "1.9507491830029944\n",
      "0.87\n",
      "0.6693276744816034\n",
      "0.875\n",
      "0.6848990486227088\n",
      "0.88\n",
      "1.907148836427949\n",
      "0.885\n",
      "0.6851081321726907\n",
      "0.89\n",
      "1.9620046863432377\n",
      "0.895\n",
      "0.5880602928787154\n",
      "0.9\n",
      "0.5904726822621659\n",
      "0.905\n",
      "0.6039649253340191\n",
      "0.91\n",
      "2.0698392458084416\n",
      "0.915\n",
      "0.6479109871915716\n",
      "0.92\n",
      "0.6479109857856797\n",
      "0.925\n",
      "0.6479109705892544\n",
      "0.93\n",
      "2.024343900102969\n",
      "0.935\n",
      "0.6819653617285928\n",
      "0.94\n",
      "1.9848255561887824\n",
      "0.945\n",
      "0.686751319552791\n",
      "0.95\n",
      "1.9464134328819087\n",
      "0.955\n",
      "0.6613853881617249\n",
      "0.96\n",
      "0.6815797772395329\n",
      "0.965\n",
      "1.9838969028832594\n",
      "0.97\n",
      "0.6271906075078794\n",
      "0.975\n",
      "2.1119519436671976\n",
      "0.98\n",
      "0.6724354967365516\n",
      "0.985\n",
      "0.6724354966309402\n",
      "0.99\n",
      "1.94739710968629\n",
      "0.995\n",
      "1.9473971094539433\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "model = get_base_model(data_NEU,NEU_targets,10)\n",
    "\n",
    "# Greedy Initialization\n",
    "NEU_OLS_Greedy_init = model\n",
    "for i in range(N_Reconfigurations):\n",
    "    # Update Model\n",
    "    NEU_OLS_Greedy_init_temp = add_reconfiguration_unit_greedily(NEU_OLS_Greedy_init,\n",
    "                                                                 data_NEU,\n",
    "                                                                 NEU_targets,\n",
    "                                                                 Pre_Epochs)\n",
    "    \n",
    "    # Check for Blowup\n",
    "    if math.isnan(np.mean(NEU_OLS_Greedy_init.predict(data_NEU))):\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init\n",
    "        break\n",
    "    else: #Update Model if not explosion\n",
    "        NEU_OLS_Greedy_init = NEU_OLS_Greedy_init_temp\n",
    "    \n",
    "    print(np.mean((NEU_OLS_Greedy_init.predict(data_NEU) - data_y)**2))\n",
    "    \n",
    "    # Update User on Status of Initialization\n",
    "    print(((i+1)/N_Reconfigurations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/800\n",
      "1000/1000 [==============================] - 1s 618us/sample - loss: 1.2628 - mse: 1.8421\n",
      "Epoch 2/800\n",
      "1000/1000 [==============================] - 1s 626us/sample - loss: 1.2614 - mse: 1.8394\n",
      "Epoch 3/800\n",
      "1000/1000 [==============================] - 1s 601us/sample - loss: 1.2610 - mse: 1.8388\n",
      "Epoch 4/800\n",
      "1000/1000 [==============================] - 1s 631us/sample - loss: 1.2605 - mse: 1.8363\n",
      "Epoch 5/800\n",
      "1000/1000 [==============================] - 1s 586us/sample - loss: 1.2648 - mse: 1.8486\n",
      "Epoch 6/800\n",
      "1000/1000 [==============================] - 1s 613us/sample - loss: 1.2633 - mse: 1.8447\n",
      "Epoch 7/800\n",
      "1000/1000 [==============================] - 1s 601us/sample - loss: 1.2614 - mse: 1.8385\n",
      "Epoch 8/800\n",
      "1000/1000 [==============================] - 1s 626us/sample - loss: 1.2606 - mse: 1.8366\n",
      "Epoch 9/800\n",
      "1000/1000 [==============================] - 1s 615us/sample - loss: 1.2646 - mse: 1.8502\n",
      "Epoch 10/800\n",
      "1000/1000 [==============================] - 1s 617us/sample - loss: 1.2606 - mse: 1.8372\n",
      "Epoch 11/800\n",
      "1000/1000 [==============================] - 1s 647us/sample - loss: 1.2643 - mse: 1.8474\n",
      "Epoch 12/800\n",
      "1000/1000 [==============================] - 1s 674us/sample - loss: 1.2656 - mse: 1.8528\n",
      "Epoch 13/800\n",
      "1000/1000 [==============================] - 1s 634us/sample - loss: 1.2623 - mse: 1.8426\n",
      "Epoch 14/800\n",
      "1000/1000 [==============================] - 1s 633us/sample - loss: 1.2617 - mse: 1.8406\n",
      "Epoch 15/800\n",
      "1000/1000 [==============================] - 1s 636us/sample - loss: 1.2652 - mse: 1.8487\n",
      "Epoch 16/800\n",
      "1000/1000 [==============================] - 1s 661us/sample - loss: 1.2620 - mse: 1.8423\n",
      "Epoch 17/800\n",
      "1000/1000 [==============================] - 1s 631us/sample - loss: 1.2607 - mse: 1.8385\n",
      "Epoch 18/800\n",
      "1000/1000 [==============================] - 1s 657us/sample - loss: 1.2602 - mse: 1.8364\n",
      "Epoch 19/800\n",
      "1000/1000 [==============================] - 1s 633us/sample - loss: 1.2657 - mse: 1.8527\n",
      "Epoch 20/800\n",
      "1000/1000 [==============================] - 1s 722us/sample - loss: 1.2647 - mse: 1.8490\n",
      "Epoch 21/800\n",
      "1000/1000 [==============================] - 1s 819us/sample - loss: 1.2578 - mse: 1.8296\n",
      "Epoch 22/800\n",
      "1000/1000 [==============================] - 1s 666us/sample - loss: 1.2632 - mse: 1.8428\n",
      "Epoch 23/800\n",
      "1000/1000 [==============================] - 1s 745us/sample - loss: 1.2635 - mse: 1.8454\n",
      "Epoch 24/800\n",
      "1000/1000 [==============================] - 1s 733us/sample - loss: 1.2639 - mse: 1.8474\n",
      "Epoch 25/800\n",
      "1000/1000 [==============================] - 1s 762us/sample - loss: 1.2585 - mse: 1.8322\n",
      "Epoch 26/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2627 - mse: 1.8430\n",
      "Epoch 27/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2652 - mse: 1.8507\n",
      "Epoch 28/800\n",
      "1000/1000 [==============================] - 1s 609us/sample - loss: 1.2632 - mse: 1.8472\n",
      "Epoch 29/800\n",
      "1000/1000 [==============================] - 1s 648us/sample - loss: 1.2588 - mse: 1.8316\n",
      "Epoch 30/800\n",
      "1000/1000 [==============================] - 1s 607us/sample - loss: 1.2640 - mse: 1.8470\n",
      "Epoch 31/800\n",
      "1000/1000 [==============================] - 1s 691us/sample - loss: 1.2647 - mse: 1.8473\n",
      "Epoch 32/800\n",
      "1000/1000 [==============================] - 1s 687us/sample - loss: 1.2648 - mse: 1.8490\n",
      "Epoch 33/800\n",
      "1000/1000 [==============================] - 1s 959us/sample - loss: 1.2636 - mse: 1.8458\n",
      "Epoch 34/800\n",
      "1000/1000 [==============================] - 1s 612us/sample - loss: 1.2667 - mse: 1.8581\n",
      "Epoch 35/800\n",
      "1000/1000 [==============================] - 1s 575us/sample - loss: 1.2667 - mse: 1.8564\n",
      "Epoch 36/800\n",
      "1000/1000 [==============================] - 1s 556us/sample - loss: 1.2639 - mse: 1.8465\n",
      "Epoch 37/800\n",
      "1000/1000 [==============================] - 1s 597us/sample - loss: 1.2635 - mse: 1.8453\n",
      "Epoch 38/800\n",
      "1000/1000 [==============================] - 1s 565us/sample - loss: 1.2624 - mse: 1.8434\n",
      "Epoch 39/800\n",
      "1000/1000 [==============================] - 1s 715us/sample - loss: 1.2600 - mse: 1.8344\n",
      "Epoch 40/800\n",
      "1000/1000 [==============================] - 1s 737us/sample - loss: 1.2624 - mse: 1.8429\n",
      "Epoch 41/800\n",
      "1000/1000 [==============================] - 1s 944us/sample - loss: 1.2627 - mse: 1.8428\n",
      "Epoch 42/800\n",
      "1000/1000 [==============================] - 1s 634us/sample - loss: 1.2600 - mse: 1.8357\n",
      "Epoch 43/800\n",
      "1000/1000 [==============================] - 1s 661us/sample - loss: 1.2637 - mse: 1.8478\n",
      "Epoch 44/800\n",
      "1000/1000 [==============================] - 1s 556us/sample - loss: 1.2594 - mse: 1.8352\n",
      "Epoch 45/800\n",
      "1000/1000 [==============================] - 1s 606us/sample - loss: 1.2631 - mse: 1.8430\n",
      "Epoch 46/800\n",
      "1000/1000 [==============================] - 1s 554us/sample - loss: 1.2683 - mse: 1.8605\n",
      "Epoch 47/800\n",
      "1000/1000 [==============================] - 1s 731us/sample - loss: 1.2633 - mse: 1.8463\n",
      "Epoch 48/800\n",
      "1000/1000 [==============================] - 1s 565us/sample - loss: 1.2567 - mse: 1.8281\n",
      "Epoch 49/800\n",
      "1000/1000 [==============================] - 1s 554us/sample - loss: 1.2609 - mse: 1.8390\n",
      "Epoch 50/800\n",
      "1000/1000 [==============================] - 1s 588us/sample - loss: 1.2592 - mse: 1.8357\n",
      "Epoch 51/800\n",
      "1000/1000 [==============================] - 1s 589us/sample - loss: 1.2609 - mse: 1.8380\n",
      "Epoch 52/800\n",
      "1000/1000 [==============================] - 1s 634us/sample - loss: 1.2637 - mse: 1.8482\n",
      "Epoch 53/800\n",
      "1000/1000 [==============================] - 1s 570us/sample - loss: 1.2635 - mse: 1.8464\n",
      "Epoch 54/800\n",
      "1000/1000 [==============================] - 1s 571us/sample - loss: 1.2639 - mse: 1.8469\n",
      "Epoch 55/800\n",
      "1000/1000 [==============================] - 1s 582us/sample - loss: 1.2600 - mse: 1.8368\n",
      "Epoch 56/800\n",
      "1000/1000 [==============================] - 1s 613us/sample - loss: 1.2607 - mse: 1.8362\n",
      "Epoch 57/800\n",
      "1000/1000 [==============================] - 1s 579us/sample - loss: 1.2637 - mse: 1.8471\n",
      "Epoch 58/800\n",
      "1000/1000 [==============================] - 1s 631us/sample - loss: 1.2581 - mse: 1.8308\n",
      "Epoch 59/800\n",
      "1000/1000 [==============================] - 1s 592us/sample - loss: 1.2634 - mse: 1.8446\n",
      "Epoch 60/800\n",
      "1000/1000 [==============================] - 1s 537us/sample - loss: 1.2562 - mse: 1.8246\n",
      "Epoch 61/800\n",
      "1000/1000 [==============================] - 1s 762us/sample - loss: 1.2646 - mse: 1.8492\n",
      "Epoch 62/800\n",
      "1000/1000 [==============================] - 1s 665us/sample - loss: 1.2622 - mse: 1.8415\n",
      "Epoch 63/800\n",
      "1000/1000 [==============================] - 1s 569us/sample - loss: 1.2604 - mse: 1.8357\n",
      "Epoch 64/800\n",
      "1000/1000 [==============================] - 1s 695us/sample - loss: 1.2634 - mse: 1.8422\n",
      "Epoch 65/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2634 - mse: 1.8445\n",
      "Epoch 66/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2603 - mse: 1.8363\n",
      "Epoch 67/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2624 - mse: 1.8408\n",
      "Epoch 68/800\n",
      "1000/1000 [==============================] - 1s 834us/sample - loss: 1.2611 - mse: 1.8376\n",
      "Epoch 69/800\n",
      "1000/1000 [==============================] - 1s 722us/sample - loss: 1.2637 - mse: 1.8487\n",
      "Epoch 70/800\n",
      "1000/1000 [==============================] - 1s 626us/sample - loss: 1.2627 - mse: 1.8448\n",
      "Epoch 71/800\n",
      "1000/1000 [==============================] - 1s 651us/sample - loss: 1.2634 - mse: 1.8446\n",
      "Epoch 72/800\n",
      "1000/1000 [==============================] - 1s 823us/sample - loss: 1.2617 - mse: 1.8382\n",
      "Epoch 73/800\n",
      "1000/1000 [==============================] - 1s 736us/sample - loss: 1.2643 - mse: 1.8460\n",
      "Epoch 74/800\n",
      "1000/1000 [==============================] - 1s 857us/sample - loss: 1.2653 - mse: 1.8506\n",
      "Epoch 75/800\n",
      "1000/1000 [==============================] - 1s 694us/sample - loss: 1.2642 - mse: 1.8473\n",
      "Epoch 76/800\n",
      "1000/1000 [==============================] - 1s 558us/sample - loss: 1.2648 - mse: 1.8489\n",
      "Epoch 77/800\n",
      "1000/1000 [==============================] - 1s 556us/sample - loss: 1.2649 - mse: 1.8493\n",
      "Epoch 78/800\n",
      "1000/1000 [==============================] - 1s 584us/sample - loss: 1.2629 - mse: 1.8439\n",
      "Epoch 79/800\n",
      "1000/1000 [==============================] - 1s 648us/sample - loss: 1.2653 - mse: 1.8507\n",
      "Epoch 80/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2584 - mse: 1.8319\n",
      "Epoch 81/800\n",
      "1000/1000 [==============================] - 1s 888us/sample - loss: 1.2555 - mse: 1.8248\n",
      "Epoch 82/800\n",
      "1000/1000 [==============================] - 1s 645us/sample - loss: 1.2644 - mse: 1.8465\n",
      "Epoch 83/800\n",
      "1000/1000 [==============================] - 1s 611us/sample - loss: 1.2637 - mse: 1.8446\n",
      "Epoch 84/800\n",
      "1000/1000 [==============================] - 1s 568us/sample - loss: 1.2681 - mse: 1.8601\n",
      "Epoch 85/800\n",
      "1000/1000 [==============================] - 1s 633us/sample - loss: 1.2586 - mse: 1.8324\n",
      "Epoch 86/800\n",
      "1000/1000 [==============================] - 1s 995us/sample - loss: 1.2646 - mse: 1.8504\n",
      "Epoch 87/800\n",
      "1000/1000 [==============================] - 1s 669us/sample - loss: 1.2646 - mse: 1.8473\n",
      "Epoch 88/800\n",
      "1000/1000 [==============================] - 1s 644us/sample - loss: 1.2611 - mse: 1.8381\n",
      "Epoch 89/800\n",
      "1000/1000 [==============================] - 1s 632us/sample - loss: 1.2627 - mse: 1.8421\n",
      "Epoch 90/800\n",
      "1000/1000 [==============================] - 1s 571us/sample - loss: 1.2621 - mse: 1.8419\n",
      "Epoch 91/800\n",
      "1000/1000 [==============================] - 1s 646us/sample - loss: 1.2602 - mse: 1.8342\n",
      "Epoch 92/800\n",
      "1000/1000 [==============================] - 1s 604us/sample - loss: 1.2628 - mse: 1.8441\n",
      "Epoch 93/800\n",
      "1000/1000 [==============================] - 1s 698us/sample - loss: 1.2624 - mse: 1.8417\n",
      "Epoch 94/800\n",
      "1000/1000 [==============================] - 1s 652us/sample - loss: 1.2649 - mse: 1.8496\n",
      "Epoch 95/800\n",
      "1000/1000 [==============================] - 1s 935us/sample - loss: 1.2641 - mse: 1.8487\n",
      "Epoch 96/800\n",
      "1000/1000 [==============================] - 1s 922us/sample - loss: 1.2644 - mse: 1.8501\n",
      "Epoch 97/800\n",
      "1000/1000 [==============================] - 1s 909us/sample - loss: 1.2626 - mse: 1.8445\n",
      "Epoch 98/800\n",
      "1000/1000 [==============================] - 1s 827us/sample - loss: 1.2605 - mse: 1.8381\n",
      "Epoch 99/800\n",
      "1000/1000 [==============================] - 1s 791us/sample - loss: 1.2632 - mse: 1.8432\n",
      "Epoch 100/800\n",
      "1000/1000 [==============================] - 1s 810us/sample - loss: 1.2625 - mse: 1.8425\n",
      "Epoch 101/800\n",
      "1000/1000 [==============================] - 1s 855us/sample - loss: 1.2619 - mse: 1.8376\n",
      "Epoch 102/800\n",
      "1000/1000 [==============================] - 1s 757us/sample - loss: 1.2618 - mse: 1.8385\n",
      "Epoch 103/800\n",
      "1000/1000 [==============================] - 1s 622us/sample - loss: 1.2661 - mse: 1.8573\n",
      "Epoch 104/800\n",
      "1000/1000 [==============================] - 1s 620us/sample - loss: 1.2647 - mse: 1.8496\n",
      "Epoch 105/800\n",
      "1000/1000 [==============================] - 1s 556us/sample - loss: 1.2622 - mse: 1.8428\n",
      "Epoch 106/800\n",
      "1000/1000 [==============================] - 1s 543us/sample - loss: 1.2644 - mse: 1.8481\n",
      "Epoch 107/800\n",
      "1000/1000 [==============================] - 1s 677us/sample - loss: 1.2655 - mse: 1.8513\n",
      "Epoch 108/800\n",
      "1000/1000 [==============================] - 1s 588us/sample - loss: 1.2649 - mse: 1.8528\n",
      "Epoch 109/800\n",
      "1000/1000 [==============================] - 1s 593us/sample - loss: 1.2599 - mse: 1.8348\n",
      "Epoch 110/800\n",
      "1000/1000 [==============================] - 1s 582us/sample - loss: 1.2650 - mse: 1.8495\n",
      "Epoch 111/800\n",
      "1000/1000 [==============================] - 1s 597us/sample - loss: 1.2619 - mse: 1.8416\n",
      "Epoch 112/800\n",
      "1000/1000 [==============================] - 1s 551us/sample - loss: 1.2629 - mse: 1.8448\n",
      "Epoch 113/800\n",
      "1000/1000 [==============================] - 1s 585us/sample - loss: 1.2628 - mse: 1.8428\n",
      "Epoch 114/800\n",
      "1000/1000 [==============================] - 1s 552us/sample - loss: 1.2622 - mse: 1.8417\n",
      "Epoch 115/800\n",
      "1000/1000 [==============================] - 1s 623us/sample - loss: 1.2646 - mse: 1.8473\n",
      "Epoch 116/800\n",
      "1000/1000 [==============================] - 1s 747us/sample - loss: 1.2638 - mse: 1.8473\n",
      "Epoch 117/800\n",
      "1000/1000 [==============================] - 1s 595us/sample - loss: 1.2637 - mse: 1.8459\n",
      "Epoch 118/800\n",
      "1000/1000 [==============================] - 1s 575us/sample - loss: 1.2635 - mse: 1.8460\n",
      "Epoch 119/800\n",
      "1000/1000 [==============================] - 1s 556us/sample - loss: 1.2617 - mse: 1.8395\n",
      "Epoch 120/800\n",
      "1000/1000 [==============================] - 1s 583us/sample - loss: 1.2574 - mse: 1.8285\n",
      "Epoch 121/800\n",
      "1000/1000 [==============================] - 1s 608us/sample - loss: 1.2643 - mse: 1.8485\n",
      "Epoch 122/800\n",
      "1000/1000 [==============================] - 1s 586us/sample - loss: 1.2580 - mse: 1.8308\n",
      "Epoch 123/800\n",
      "1000/1000 [==============================] - 1s 570us/sample - loss: 1.2642 - mse: 1.8485\n",
      "Epoch 124/800\n",
      "1000/1000 [==============================] - 1s 564us/sample - loss: 1.2653 - mse: 1.8514\n",
      "Epoch 125/800\n",
      "1000/1000 [==============================] - 1s 578us/sample - loss: 1.2670 - mse: 1.8560\n",
      "Epoch 126/800\n",
      "1000/1000 [==============================] - 1s 624us/sample - loss: 1.2646 - mse: 1.8511\n",
      "Epoch 127/800\n",
      "1000/1000 [==============================] - 1s 668us/sample - loss: 1.2630 - mse: 1.8430\n",
      "Epoch 128/800\n",
      "1000/1000 [==============================] - 1s 535us/sample - loss: 1.2650 - mse: 1.8501\n",
      "Epoch 129/800\n",
      "1000/1000 [==============================] - 1s 560us/sample - loss: 1.2624 - mse: 1.8421\n",
      "Epoch 130/800\n",
      "1000/1000 [==============================] - 1s 573us/sample - loss: 1.2633 - mse: 1.8431\n",
      "Epoch 131/800\n",
      "1000/1000 [==============================] - 1s 742us/sample - loss: 1.2609 - mse: 1.8388\n",
      "Epoch 132/800\n",
      "1000/1000 [==============================] - 1s 549us/sample - loss: 1.2582 - mse: 1.8299\n",
      "Epoch 133/800\n",
      "1000/1000 [==============================] - 1s 647us/sample - loss: 1.2633 - mse: 1.8431\n",
      "Epoch 134/800\n",
      "1000/1000 [==============================] - 1s 771us/sample - loss: 1.2677 - mse: 1.8587\n",
      "Epoch 135/800\n",
      "1000/1000 [==============================] - 1s 689us/sample - loss: 1.2636 - mse: 1.8452\n",
      "Epoch 136/800\n",
      "1000/1000 [==============================] - 1s 632us/sample - loss: 1.2668 - mse: 1.8556\n",
      "Epoch 137/800\n",
      "1000/1000 [==============================] - 1s 550us/sample - loss: 1.2603 - mse: 1.8350\n",
      "Epoch 138/800\n",
      "1000/1000 [==============================] - 1s 602us/sample - loss: 1.2618 - mse: 1.8386\n",
      "Epoch 139/800\n",
      "1000/1000 [==============================] - 1s 554us/sample - loss: 1.2617 - mse: 1.8396\n",
      "Epoch 140/800\n",
      "1000/1000 [==============================] - 1s 610us/sample - loss: 1.2627 - mse: 1.8444\n",
      "Epoch 141/800\n",
      "1000/1000 [==============================] - 1s 533us/sample - loss: 1.2596 - mse: 1.8356\n",
      "Epoch 142/800\n",
      "1000/1000 [==============================] - 1s 580us/sample - loss: 1.2590 - mse: 1.8336\n",
      "Epoch 143/800\n",
      "1000/1000 [==============================] - 1s 649us/sample - loss: 1.2625 - mse: 1.8437\n",
      "Epoch 144/800\n",
      "1000/1000 [==============================] - 1s 611us/sample - loss: 1.2586 - mse: 1.8300\n",
      "Epoch 145/800\n",
      "1000/1000 [==============================] - 1s 629us/sample - loss: 1.2652 - mse: 1.8501\n",
      "Epoch 146/800\n",
      "1000/1000 [==============================] - 1s 674us/sample - loss: 1.2616 - mse: 1.8407\n",
      "Epoch 147/800\n",
      "1000/1000 [==============================] - 1s 705us/sample - loss: 1.2634 - mse: 1.8445\n",
      "Epoch 148/800\n",
      "1000/1000 [==============================] - 1s 583us/sample - loss: 1.2608 - mse: 1.8377\n",
      "Epoch 149/800\n",
      "1000/1000 [==============================] - 1s 640us/sample - loss: 1.2597 - mse: 1.8347\n",
      "Epoch 150/800\n",
      "1000/1000 [==============================] - 1s 762us/sample - loss: 1.2619 - mse: 1.8399\n",
      "Epoch 151/800\n",
      "1000/1000 [==============================] - 1s 608us/sample - loss: 1.2605 - mse: 1.8354\n",
      "Epoch 152/800\n",
      "1000/1000 [==============================] - 1s 557us/sample - loss: 1.2649 - mse: 1.8522\n",
      "Epoch 153/800\n",
      "1000/1000 [==============================] - 1s 623us/sample - loss: 1.2635 - mse: 1.8453\n",
      "Epoch 154/800\n",
      "1000/1000 [==============================] - 1s 709us/sample - loss: 1.2633 - mse: 1.8439\n",
      "Epoch 155/800\n",
      "1000/1000 [==============================] - 1s 615us/sample - loss: 1.2653 - mse: 1.8503\n",
      "Epoch 156/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2612 - mse: 1.8377\n",
      "Epoch 157/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2626 - mse: 1.8419\n",
      "Epoch 158/800\n",
      "1000/1000 [==============================] - 1s 926us/sample - loss: 1.2628 - mse: 1.8442\n",
      "Epoch 159/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2637 - mse: 1.8449\n",
      "Epoch 160/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2635 - mse: 1.8453\n",
      "Epoch 161/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2632 - mse: 1.8431\n",
      "Epoch 162/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2654 - mse: 1.8507\n",
      "Epoch 163/800\n",
      "1000/1000 [==============================] - 1s 708us/sample - loss: 1.2619 - mse: 1.8426\n",
      "Epoch 164/800\n",
      "1000/1000 [==============================] - 1s 658us/sample - loss: 1.2642 - mse: 1.8472\n",
      "Epoch 165/800\n",
      "1000/1000 [==============================] - 1s 635us/sample - loss: 1.2659 - mse: 1.8535\n",
      "Epoch 166/800\n",
      "1000/1000 [==============================] - 1s 690us/sample - loss: 1.2645 - mse: 1.8493\n",
      "Epoch 167/800\n",
      "1000/1000 [==============================] - 1s 650us/sample - loss: 1.2637 - mse: 1.8448\n",
      "Epoch 168/800\n",
      "1000/1000 [==============================] - 1s 571us/sample - loss: 1.2647 - mse: 1.8479\n",
      "Epoch 169/800\n",
      "1000/1000 [==============================] - 1s 649us/sample - loss: 1.2660 - mse: 1.8569\n",
      "Epoch 170/800\n",
      "1000/1000 [==============================] - 1s 569us/sample - loss: 1.2623 - mse: 1.8398\n",
      "Epoch 171/800\n",
      "1000/1000 [==============================] - 1s 914us/sample - loss: 1.2642 - mse: 1.8482\n",
      "Epoch 172/800\n",
      "1000/1000 [==============================] - 1s 781us/sample - loss: 1.2628 - mse: 1.8416\n",
      "Epoch 173/800\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 1.2637 - mse: 1.8472\n",
      "Epoch 174/800\n",
      "1000/1000 [==============================] - 1s 774us/sample - loss: 1.2631 - mse: 1.8425\n",
      "Epoch 175/800\n",
      "1000/1000 [==============================] - 1s 562us/sample - loss: 1.2612 - mse: 1.8391\n",
      "Epoch 176/800\n",
      "1000/1000 [==============================] - 1s 689us/sample - loss: 1.2601 - mse: 1.8371\n",
      "Epoch 177/800\n",
      "1000/1000 [==============================] - 1s 891us/sample - loss: 1.2622 - mse: 1.8425\n",
      "Epoch 178/800\n",
      "1000/1000 [==============================] - 1s 917us/sample - loss: 1.2671 - mse: 1.8557\n",
      "Epoch 179/800\n",
      "1000/1000 [==============================] - 1s 739us/sample - loss: 1.2649 - mse: 1.8484\n",
      "Epoch 180/800\n",
      "1000/1000 [==============================] - 1s 886us/sample - loss: 1.2631 - mse: 1.8435\n",
      "Epoch 181/800\n",
      "1000/1000 [==============================] - 1s 890us/sample - loss: 1.2637 - mse: 1.8457\n",
      "Epoch 182/800\n",
      "1000/1000 [==============================] - 1s 809us/sample - loss: 1.2612 - mse: 1.8391\n",
      "Epoch 183/800\n",
      "1000/1000 [==============================] - 1s 876us/sample - loss: 1.2626 - mse: 1.8422\n",
      "Epoch 184/800\n",
      "1000/1000 [==============================] - 1s 787us/sample - loss: 1.2654 - mse: 1.8500\n",
      "Epoch 185/800\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.2644 - mse: 1.841 - 1s 859us/sample - loss: 1.2629 - mse: 1.8447\n",
      "Epoch 186/800\n",
      "1000/1000 [==============================] - 1s 804us/sample - loss: 1.2602 - mse: 1.8330\n",
      "Epoch 187/800\n",
      "1000/1000 [==============================] - 1s 559us/sample - loss: 1.2635 - mse: 1.8455\n",
      "Epoch 188/800\n",
      "1000/1000 [==============================] - 1s 619us/sample - loss: 1.2652 - mse: 1.8518\n",
      "Epoch 189/800\n",
      "1000/1000 [==============================] - 1s 594us/sample - loss: 1.2570 - mse: 1.8331\n",
      "Epoch 190/800\n",
      "1000/1000 [==============================] - 1s 595us/sample - loss: 1.2610 - mse: 1.8396\n",
      "Epoch 191/800\n",
      "1000/1000 [==============================] - 1s 537us/sample - loss: 1.2611 - mse: 1.8348\n",
      "Epoch 192/800\n",
      "1000/1000 [==============================] - 1s 655us/sample - loss: 1.2671 - mse: 1.8568\n",
      "Epoch 193/800\n",
      "1000/1000 [==============================] - 1s 535us/sample - loss: 1.2606 - mse: 1.8378\n",
      "Epoch 194/800\n",
      "1000/1000 [==============================] - 1s 550us/sample - loss: 1.2650 - mse: 1.8505\n",
      "Epoch 195/800\n",
      "1000/1000 [==============================] - 1s 554us/sample - loss: 1.2649 - mse: 1.8495\n",
      "Epoch 196/800\n",
      "1000/1000 [==============================] - 1s 541us/sample - loss: 1.2634 - mse: 1.8441\n",
      "Epoch 197/800\n",
      "1000/1000 [==============================] - 1s 534us/sample - loss: 1.2603 - mse: 1.8336\n",
      "Epoch 198/800\n",
      "1000/1000 [==============================] - 1s 522us/sample - loss: 1.2595 - mse: 1.8347\n",
      "Epoch 199/800\n",
      "1000/1000 [==============================] - 1s 559us/sample - loss: 1.2601 - mse: 1.8369\n",
      "Epoch 200/800\n",
      "1000/1000 [==============================] - 1s 521us/sample - loss: 1.2626 - mse: 1.8411\n",
      "Epoch 201/800\n",
      "1000/1000 [==============================] - 1s 554us/sample - loss: 1.2649 - mse: 1.8497\n",
      "Epoch 202/800\n",
      "1000/1000 [==============================] - 1s 573us/sample - loss: 1.2648 - mse: 1.8477\n",
      "Epoch 203/800\n",
      "1000/1000 [==============================] - 1s 530us/sample - loss: 1.2606 - mse: 1.8377\n",
      "Epoch 204/800\n",
      "1000/1000 [==============================] - 1s 722us/sample - loss: 1.2610 - mse: 1.8385\n",
      "Epoch 205/800\n",
      "1000/1000 [==============================] - 1s 536us/sample - loss: 1.2634 - mse: 1.8441\n",
      "Epoch 206/800\n",
      "1000/1000 [==============================] - 1s 568us/sample - loss: 1.2636 - mse: 1.8456\n",
      "Epoch 207/800\n",
      "1000/1000 [==============================] - 1s 684us/sample - loss: 1.2630 - mse: 1.8429\n",
      "Epoch 208/800\n",
      "1000/1000 [==============================] - 1s 533us/sample - loss: 1.2612 - mse: 1.8393\n",
      "Epoch 209/800\n",
      "1000/1000 [==============================] - 1s 555us/sample - loss: 1.2634 - mse: 1.8462\n",
      "Epoch 210/800\n",
      "1000/1000 [==============================] - 1s 558us/sample - loss: 1.2641 - mse: 1.8472\n",
      "Epoch 211/800\n",
      "1000/1000 [==============================] - 1s 530us/sample - loss: 1.2610 - mse: 1.8370\n",
      "Epoch 212/800\n",
      "1000/1000 [==============================] - 1s 540us/sample - loss: 1.2631 - mse: 1.8458\n",
      "Epoch 213/800\n",
      "1000/1000 [==============================] - 1s 526us/sample - loss: 1.2611 - mse: 1.8393\n",
      "Epoch 214/800\n",
      "1000/1000 [==============================] - 1s 542us/sample - loss: 1.2642 - mse: 1.8469\n",
      "Epoch 215/800\n",
      "1000/1000 [==============================] - 1s 576us/sample - loss: 1.2615 - mse: 1.8416\n",
      "Epoch 216/800\n",
      "1000/1000 [==============================] - 1s 531us/sample - loss: 1.2643 - mse: 1.8505\n",
      "Epoch 217/800\n",
      "1000/1000 [==============================] - 1s 536us/sample - loss: 1.2643 - mse: 1.8466\n",
      "Epoch 218/800\n",
      "1000/1000 [==============================] - 1s 758us/sample - loss: 1.2651 - mse: 1.8507\n",
      "Epoch 219/800\n",
      "1000/1000 [==============================] - 1s 612us/sample - loss: 1.2638 - mse: 1.8450\n",
      "Epoch 220/800\n",
      "1000/1000 [==============================] - 1s 530us/sample - loss: 1.2626 - mse: 1.8432\n",
      "Epoch 221/800\n",
      "1000/1000 [==============================] - 1s 587us/sample - loss: 1.2646 - mse: 1.8464\n",
      "Epoch 222/800\n",
      "1000/1000 [==============================] - 1s 565us/sample - loss: 1.2616 - mse: 1.8408\n",
      "Epoch 223/800\n",
      "1000/1000 [==============================] - 1s 565us/sample - loss: 1.2626 - mse: 1.8434\n",
      "Epoch 224/800\n",
      "1000/1000 [==============================] - 1s 546us/sample - loss: 1.2645 - mse: 1.8483\n",
      "Epoch 225/800\n",
      "1000/1000 [==============================] - 1s 528us/sample - loss: 1.2636 - mse: 1.8461\n",
      "Epoch 226/800\n",
      "1000/1000 [==============================] - 1s 521us/sample - loss: 1.2612 - mse: 1.8401\n",
      "Epoch 227/800\n",
      "1000/1000 [==============================] - 1s 538us/sample - loss: 1.2621 - mse: 1.8394\n",
      "Epoch 228/800\n",
      "1000/1000 [==============================] - 1s 717us/sample - loss: 1.2586 - mse: 1.8317\n",
      "Epoch 229/800\n",
      "1000/1000 [==============================] - 1s 734us/sample - loss: 1.2629 - mse: 1.8446\n",
      "Epoch 230/800\n",
      "1000/1000 [==============================] - 1s 542us/sample - loss: 1.2634 - mse: 1.8474\n",
      "Epoch 231/800\n",
      "1000/1000 [==============================] - 1s 569us/sample - loss: 1.2615 - mse: 1.8399\n",
      "Epoch 232/800\n",
      "1000/1000 [==============================] - 1s 559us/sample - loss: 1.2670 - mse: 1.8575\n",
      "Epoch 233/800\n",
      "1000/1000 [==============================] - 1s 532us/sample - loss: 1.2611 - mse: 1.8396\n",
      "Epoch 234/800\n",
      "1000/1000 [==============================] - 1s 608us/sample - loss: 1.2636 - mse: 1.8457\n",
      "Epoch 235/800\n",
      "1000/1000 [==============================] - 1s 736us/sample - loss: 1.2636 - mse: 1.8455\n",
      "Epoch 236/800\n",
      "1000/1000 [==============================] - 1s 675us/sample - loss: 1.2625 - mse: 1.8418\n",
      "Epoch 237/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 646us/sample - loss: 1.2638 - mse: 1.8473\n",
      "Epoch 238/800\n",
      "1000/1000 [==============================] - 1s 653us/sample - loss: 1.2631 - mse: 1.8441\n",
      "Epoch 239/800\n",
      "1000/1000 [==============================] - 1s 580us/sample - loss: 1.2626 - mse: 1.8432\n",
      "Epoch 240/800\n",
      "1000/1000 [==============================] - 1s 735us/sample - loss: 1.2634 - mse: 1.8455\n",
      "Epoch 241/800\n",
      "1000/1000 [==============================] - 1s 575us/sample - loss: 1.2626 - mse: 1.8415\n",
      "Epoch 242/800\n",
      "1000/1000 [==============================] - 1s 686us/sample - loss: 1.2585 - mse: 1.8339\n",
      "Epoch 243/800\n",
      "1000/1000 [==============================] - 1s 636us/sample - loss: 1.2656 - mse: 1.8525\n",
      "Epoch 244/800\n",
      "1000/1000 [==============================] - 1s 690us/sample - loss: 1.2630 - mse: 1.8449\n",
      "Epoch 245/800\n",
      "1000/1000 [==============================] - 1s 626us/sample - loss: 1.2619 - mse: 1.8416\n",
      "Epoch 246/800\n",
      "1000/1000 [==============================] - 1s 599us/sample - loss: 1.2651 - mse: 1.8493\n",
      "Epoch 247/800\n",
      "1000/1000 [==============================] - 1s 616us/sample - loss: 1.2605 - mse: 1.8367\n",
      "Epoch 248/800\n",
      "1000/1000 [==============================] - 1s 666us/sample - loss: 1.2646 - mse: 1.8502\n",
      "Epoch 249/800\n",
      "1000/1000 [==============================] - 1s 667us/sample - loss: 1.2599 - mse: 1.8361\n",
      "Epoch 250/800\n",
      "1000/1000 [==============================] - 1s 641us/sample - loss: 1.2636 - mse: 1.8445\n",
      "Epoch 251/800\n",
      "1000/1000 [==============================] - 1s 588us/sample - loss: 1.2620 - mse: 1.8391\n",
      "Epoch 252/800\n",
      "1000/1000 [==============================] - 1s 558us/sample - loss: 1.2537 - mse: 1.8248\n",
      "Epoch 253/800\n",
      "1000/1000 [==============================] - 1s 540us/sample - loss: 1.2566 - mse: 1.8268\n",
      "Epoch 254/800\n",
      "1000/1000 [==============================] - 1s 524us/sample - loss: 1.2624 - mse: 1.8429\n",
      "Epoch 255/800\n",
      "1000/1000 [==============================] - 1s 547us/sample - loss: 1.2640 - mse: 1.8436\n",
      "Epoch 256/800\n",
      "1000/1000 [==============================] - 1s 522us/sample - loss: 1.2639 - mse: 1.8470\n",
      "Epoch 257/800\n",
      "1000/1000 [==============================] - 1s 526us/sample - loss: 1.2626 - mse: 1.8416\n",
      "Epoch 258/800\n",
      "1000/1000 [==============================] - 1s 524us/sample - loss: 1.2642 - mse: 1.8465\n",
      "Epoch 259/800\n",
      "1000/1000 [==============================] - 1s 521us/sample - loss: 1.2639 - mse: 1.8463\n",
      "Epoch 260/800\n",
      "1000/1000 [==============================] - 1s 547us/sample - loss: 1.2630 - mse: 1.8432\n",
      "Epoch 261/800\n",
      "1000/1000 [==============================] - 1s 541us/sample - loss: 1.2654 - mse: 1.8519\n",
      "Epoch 262/800\n",
      "1000/1000 [==============================] - 1s 531us/sample - loss: 1.2629 - mse: 1.8433\n",
      "Epoch 263/800\n",
      "1000/1000 [==============================] - 1s 517us/sample - loss: 1.2648 - mse: 1.8470\n",
      "Epoch 264/800\n",
      "1000/1000 [==============================] - 1s 532us/sample - loss: 1.2643 - mse: 1.8486\n",
      "Epoch 265/800\n",
      "1000/1000 [==============================] - 1s 617us/sample - loss: 1.2650 - mse: 1.8501\n",
      "Epoch 266/800\n",
      "1000/1000 [==============================] - 1s 579us/sample - loss: 1.2588 - mse: 1.8356\n",
      "Epoch 267/800\n",
      "1000/1000 [==============================] - 1s 570us/sample - loss: 1.2609 - mse: 1.8378\n",
      "Epoch 268/800\n",
      "1000/1000 [==============================] - 1s 558us/sample - loss: 1.2600 - mse: 1.8354\n",
      "Epoch 269/800\n",
      "1000/1000 [==============================] - 1s 521us/sample - loss: 1.2628 - mse: 1.8442\n",
      "Epoch 270/800\n",
      "1000/1000 [==============================] - 1s 558us/sample - loss: 1.2646 - mse: 1.8490\n",
      "Epoch 271/800\n",
      "1000/1000 [==============================] - 1s 522us/sample - loss: 1.2633 - mse: 1.8451\n",
      "Epoch 272/800\n",
      "1000/1000 [==============================] - 1s 521us/sample - loss: 1.2618 - mse: 1.8410\n",
      "Epoch 273/800\n",
      "1000/1000 [==============================] - 1s 527us/sample - loss: 1.2625 - mse: 1.8427\n",
      "Epoch 274/800\n",
      "1000/1000 [==============================] - 1s 528us/sample - loss: 1.2576 - mse: 1.8283\n",
      "Epoch 275/800\n",
      "1000/1000 [==============================] - 1s 521us/sample - loss: 1.2637 - mse: 1.8477\n",
      "Epoch 276/800\n",
      "1000/1000 [==============================] - 1s 534us/sample - loss: 1.2646 - mse: 1.8512\n",
      "Epoch 277/800\n",
      "1000/1000 [==============================] - 1s 527us/sample - loss: 1.2673 - mse: 1.8558\n",
      "Epoch 278/800\n",
      "1000/1000 [==============================] - 1s 530us/sample - loss: 1.2614 - mse: 1.8407\n",
      "Epoch 279/800\n",
      "1000/1000 [==============================] - 1s 527us/sample - loss: 1.2627 - mse: 1.8409\n",
      "Epoch 280/800\n",
      "1000/1000 [==============================] - 1s 551us/sample - loss: 1.2622 - mse: 1.8437\n",
      "Epoch 281/800\n",
      "1000/1000 [==============================] - 1s 528us/sample - loss: 1.2630 - mse: 1.8412\n",
      "Epoch 282/800\n",
      "1000/1000 [==============================] - 1s 612us/sample - loss: 1.2638 - mse: 1.8460\n",
      "Epoch 283/800\n",
      "1000/1000 [==============================] - 1s 710us/sample - loss: 1.2642 - mse: 1.8472\n",
      "Epoch 284/800\n",
      "1000/1000 [==============================] - 1s 592us/sample - loss: 1.2622 - mse: 1.8427\n",
      "Epoch 285/800\n",
      "1000/1000 [==============================] - 1s 588us/sample - loss: 1.2629 - mse: 1.8433\n",
      "Epoch 286/800\n",
      "1000/1000 [==============================] - 1s 654us/sample - loss: 1.2631 - mse: 1.8444\n",
      "Epoch 287/800\n",
      "1000/1000 [==============================] - 1s 637us/sample - loss: 1.2619 - mse: 1.8412\n",
      "Epoch 288/800\n",
      "1000/1000 [==============================] - 1s 612us/sample - loss: 1.2623 - mse: 1.8426\n",
      "Epoch 289/800\n",
      "1000/1000 [==============================] - 1s 617us/sample - loss: 1.2614 - mse: 1.8401\n",
      "Epoch 290/800\n",
      "1000/1000 [==============================] - 1s 628us/sample - loss: 1.2642 - mse: 1.8484\n",
      "Epoch 291/800\n",
      "1000/1000 [==============================] - 1s 602us/sample - loss: 1.2537 - mse: 1.8237\n",
      "Epoch 292/800\n",
      "1000/1000 [==============================] - 1s 602us/sample - loss: 1.2615 - mse: 1.8381\n",
      "Epoch 293/800\n",
      "1000/1000 [==============================] - 1s 598us/sample - loss: 1.2602 - mse: 1.8368\n",
      "Epoch 294/800\n",
      "1000/1000 [==============================] - 1s 624us/sample - loss: 1.2592 - mse: 1.8340\n",
      "Epoch 295/800\n",
      "1000/1000 [==============================] - 1s 642us/sample - loss: 1.2641 - mse: 1.8464\n",
      "Epoch 296/800\n",
      "1000/1000 [==============================] - 1s 647us/sample - loss: 1.2607 - mse: 1.8367\n",
      "Epoch 297/800\n",
      "1000/1000 [==============================] - 1s 626us/sample - loss: 1.2593 - mse: 1.8314\n",
      "Epoch 298/800\n",
      "1000/1000 [==============================] - 1s 646us/sample - loss: 1.2631 - mse: 1.8432\n",
      "Epoch 299/800\n",
      "1000/1000 [==============================] - 1s 705us/sample - loss: 1.2638 - mse: 1.8470\n",
      "Epoch 300/800\n",
      "1000/1000 [==============================] - 1s 632us/sample - loss: 1.2624 - mse: 1.8441\n",
      "Epoch 301/800\n",
      "1000/1000 [==============================] - 1s 633us/sample - loss: 1.2655 - mse: 1.8514\n",
      "Epoch 302/800\n",
      "1000/1000 [==============================] - 1s 616us/sample - loss: 1.2588 - mse: 1.8377\n",
      "Epoch 303/800\n",
      "1000/1000 [==============================] - 1s 623us/sample - loss: 1.2545 - mse: 1.8248\n",
      "Epoch 304/800\n",
      "1000/1000 [==============================] - 1s 645us/sample - loss: 1.2599 - mse: 1.8359\n",
      "Epoch 305/800\n",
      "1000/1000 [==============================] - 1s 601us/sample - loss: 1.2607 - mse: 1.8381\n",
      "Epoch 306/800\n",
      "1000/1000 [==============================] - 1s 610us/sample - loss: 1.2613 - mse: 1.8384\n",
      "Epoch 307/800\n",
      "1000/1000 [==============================] - 1s 659us/sample - loss: 1.2644 - mse: 1.8474s - loss: 1.2819 - mse:\n",
      "Epoch 308/800\n",
      "1000/1000 [==============================] - 1s 621us/sample - loss: 1.2623 - mse: 1.8438\n",
      "Epoch 309/800\n",
      "1000/1000 [==============================] - 1s 609us/sample - loss: 1.2607 - mse: 1.8360\n",
      "Epoch 310/800\n",
      "1000/1000 [==============================] - 1s 658us/sample - loss: 1.2635 - mse: 1.8447\n",
      "Epoch 311/800\n",
      "1000/1000 [==============================] - 1s 689us/sample - loss: 1.2638 - mse: 1.8471\n",
      "Epoch 312/800\n",
      "1000/1000 [==============================] - 1s 654us/sample - loss: 1.2627 - mse: 1.8427\n",
      "Epoch 313/800\n",
      "1000/1000 [==============================] - 1s 678us/sample - loss: 1.2637 - mse: 1.8455\n",
      "Epoch 314/800\n",
      "1000/1000 [==============================] - 1s 657us/sample - loss: 1.2646 - mse: 1.8507\n",
      "Epoch 315/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 826us/sample - loss: 1.2673 - mse: 1.8593\n",
      "Epoch 316/800\n",
      "1000/1000 [==============================] - 1s 626us/sample - loss: 1.2603 - mse: 1.8364\n",
      "Epoch 317/800\n",
      "1000/1000 [==============================] - 1s 635us/sample - loss: 1.2682 - mse: 1.8598\n",
      "Epoch 318/800\n",
      "1000/1000 [==============================] - 1s 713us/sample - loss: 1.2609 - mse: 1.8368\n",
      "Epoch 319/800\n",
      "  32/1000 [..............................] - ETA: 0s - loss: 1.2096 - mse: 1.7548"
     ]
    }
   ],
   "source": [
    "NEU_OLS = NEU_OLS_Greedy_init\n",
    "NEU_OLS.fit(data_NEU,NEU_targets,epochs = Full_Epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predictions (for comparison: TEMP)\n",
    "NEU_OLS_prediction = NEU_OLS.predict(data_NEU)\n",
    "NEU_OLS_single_unit_prediction = model.predict(data_NEU)\n",
    "NEU_OLS_greedy_initializations = NEU_OLS_Greedy_init.predict(data_NEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Figure Details\n",
    "plt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Data Plot\n",
    "plt.plot(data_x,true_y,color='k',label='true',linestyle='--')\n",
    "\n",
    "# Plot Models\n",
    "plt.plot(data_x,model_pred_y,color='r',label='OLS')\n",
    "plt.plot(data_x,NEU_OLS_single_unit_prediction,color='b',label='NEU_Unit')\n",
    "plt.plot(data_x,NEU_OLS_greedy_initializations,color='g',label='NEU_Greedy_Init')\n",
    "plt.plot(data_x,NEU_OLS_prediction,color='Aqua',label='NEU-OLS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
