{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Builder\n",
    "This little script contains all the architecutre builders used in benchmarking the NEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconfiguration Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reconfiguration_Network_Readout(learning_rate, input_dim, output_dim, readout_map_depth,readout_map_height,robustness_parameter,homotopy_parameter):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=((input_dim),))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(input_dim), homotopy_parameter = homotopy_parameter)(input_layer)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        \n",
    "    # Projection Layer\n",
    "#     output_layer = projection_layer(deep_readout_map)\n",
    "    # Trainable Output Layer\n",
    "    output_layer = fullyConnected_Dense(output_dim)(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-Structure Building Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NEU_Structure(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_Structure_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_Reconfiguration_Network_Readout, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_Structure_CV = RandomizedSearchCV(estimator=NEU_Structure_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Pipe Standard Scaler \n",
    "    NEU_Structure_CV_mmxscaler_piped = NEU_Structure_CV\n",
    "    #Pipeline([('scaler', MinMaxScaler()), ('model', NEU_Structure_CV)])\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_Structure_CV_mmxscaler_piped.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_Structure_CV_mmxscaler_piped.predict(X_train)\n",
    "    y_hat_test = NEU_Structure_CV_mmxscaler_piped.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_Structure_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-Structure Map: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-Structure Map: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "#     NEU_Structure_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "#     pd.DataFrame.from_dict(NEU_Structure_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-Structure: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-Structure Building Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Just Readout Version:\n",
    "In the case where $D>1$ and $\\mathcal{F}$ is a universal approximator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reconfiguration_Network_Readout_no_project(learning_rate, input_dim, output_dim, readout_map_depth,readout_map_height,robustness_parameter,homotopy_parameter):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=((input_dim),))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(input_dim), homotopy_parameter = homotopy_parameter)(input_layer)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, deep_readout_map)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-Structure Building Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NEU_Readout(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_Structure_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_Reconfiguration_Network_Readout_no_project, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_Structure_CV = RandomizedSearchCV(estimator=NEU_Structure_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Pipe Standard Scaler \n",
    "    NEU_Structure_CV_mmxscaler_piped = NEU_Structure_CV\n",
    "    #Pipeline([('scaler', MinMaxScaler()), ('model', NEU_Structure_CV)])\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_Structure_CV_mmxscaler_piped.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_Structure_CV_mmxscaler_piped.predict(X_train)\n",
    "    y_hat_test = NEU_Structure_CV_mmxscaler_piped.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_Structure_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-Structure Map: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-Structure Map: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "#     NEU_Structure_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "#     pd.DataFrame.from_dict(NEU_Structure_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-Structure: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-Structure Building Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_OLS(learning_rate, input_dim, output_dim, feature_map_depth, feature_map_height,robustness_parameter, homotopy_parameter,implicit_dimension):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map_prep = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    deep_feature_map = tf.concat([input_layer, deep_feature_map_prep], axis=1)\n",
    "    ## Homeomorphic Part\n",
    "    dimension_lifted = (input_dim + embedding_dimension)\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "        # First Layer\n",
    "        ## Spacial-Dependent part of reconfiguration unit\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=dimension_lifted, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        ## Constant part of reconfiguration unit\n",
    "#         deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "        ## Non-linear part of reconfiguration unit\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    # Linear Readout (Really this is the OLS model)\n",
    "    OLS_Layer_output = fullyConnected_Dense(output_dim)(deep_feature_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, OLS_Layer_output)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-ffNN Training Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NEU_OLS(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_OLS_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_OLS, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_OLS_CV = RandomizedSearchCV(estimator=NEU_OLS_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_OLS_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_OLS_CV.predict(X_train)\n",
    "    y_hat_test = NEU_OLS_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_OLS_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-OLS: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-OLS: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_OLS_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    Path('./outputs/models/NEU/NEU_OLS/').mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(NEU_OLS_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/NEU_OLS/Best_Parameters.tex\")\n",
    "    print('NEU-OLS: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test, best_model, NEU_OLS_CV.best_params_\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Models: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Vanilla) Feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "   \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "def build_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "    # Deep Feature Network\n",
    "    ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN, \n",
    "                                                            verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = ffNN_CVer.predict(X_train)\n",
    "    y_hat_test = ffNN_CVer.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = ffNN_CVer.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    \n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('Benchmark-Model: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    ffNN_CVer.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(ffNN_CVer.best_params_,orient='index').to_latex(\"./outputs/models/Benchmarks/Best_Parameters.tex\")\n",
    "    print('Benchmark-Model: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Deep Feature Builder - Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEU-Feed-forward Neural Network\n",
    "This next snippet builds the NEU for the feed-forward network; i.e.:\n",
    "$$\n",
    "f_{NEU} \\triangleq \\rho \\circ f_{ffNN}\\circ \\phi\n",
    ",\n",
    "$$\n",
    "where $\\rho=p\\circ \\xi$, $\\xi,\\phi$ are reconfiguration networks, and $f_{ffNN}$ is a feed-forward network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train NEU-ffNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_ffNN(height, depth, learning_rate, input_dim, output_dim, feature_map_depth, readout_map_depth, feature_map_height,readout_map_height,robustness_parameter,homotopy_parameter,implicit_dimension):\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map_prep = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    deep_feature_map = tf.concat([input_layer, deep_feature_map_prep], axis=1)\n",
    "    ## Homeomorphic Part\n",
    "    dimension_lifted = (input_dim + embedding_dimension)    \n",
    "    ### Execute Random Embedding\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "#        # First Layer\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=dimension_lifted, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        deep_feature_map = fullyConnected_Dense_Invertible(embedding_dimension)(input_layer)\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    core_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=output_dim, homotopy_parameter = homotopy_parameter)(core_layers)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=output_dim, homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, deep_readout_map)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-ffNN Training Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NEU_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_ffNN, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_ffNN_CV = RandomizedSearchCV(estimator=NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-ffNN: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-ffNN: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative NEU-ffNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_ffNN_w_proj(height, depth, learning_rate, input_dim, output_dim, feature_map_depth, readout_map_depth, feature_map_height,readout_map_height,robustness_parameter,homotopy_parameter,implicit_dimension):\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "        #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map_prep = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    deep_feature_map = tf.concat([input_layer, deep_feature_map_prep], axis=1)\n",
    "    ## Homeomorphic Part\n",
    "    dimension_lifted = (input_dim + embedding_dimension)    \n",
    "    ### Execute Random Embedding\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "#        # First Layer\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=dimension_lifted, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        deep_feature_map = fullyConnected_Dense_Invertible(embedding_dimension)(input_layer)\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    core_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    \n",
    "    deep_readout_map = tf.concat([input_layer, core_layers], axis=1)\n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(output_dim+input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(output_dim+input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    \n",
    "    # Projection Layer\n",
    "    output_layer = projection_layer(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-ffNN Training Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NEU_ffNN_w_proj(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_ffNN_w_proj, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_ffNN_CV = RandomizedSearchCV(estimator=NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-ffNN: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-ffNN: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get NEU-ffNN (Only Feature Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_ffNN_w_feature_only(height, depth, learning_rate, input_dim, output_dim, feature_map_depth, readout_map_depth, feature_map_height,readout_map_height,robustness_parameter,homotopy_parameter,implicit_dimension):\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map_prep = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    deep_feature_map = tf.concat([input_layer, deep_feature_map_prep], axis=1)\n",
    "    ## Homeomorphic Part\n",
    "    dimension_lifted = (input_dim + embedding_dimension)    \n",
    "    ### Execute Random Embedding\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "#        # First Layer\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=dimension_lifted, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        deep_feature_map = fullyConnected_Dense_Invertible(embedding_dimension)(input_layer)\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    output_layer = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-ffNN (Fully Coupled) Training Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NEU_ffNN_w_feature_only(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update User\n",
    "    print(\"Training NEU-ffNN (Fully Coupled)\")\n",
    "    \n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_ffNN_w_feature_only, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_ffNN_CV = RandomizedSearchCV(estimator=NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN (Fully Coupled): Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-ffNN (Fully Coupled): Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-ffNN (Fully Coupled): Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN (Fully Coupled) Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive NEU-ffNN\n",
    "Next we implement the NEU but without using reconfiguration networks for the feature and readout maps... Instead we use the (homeomorphic) feed-forward architecture with *sub-minimal width* feed-forward architecture introduced in: [Bilokopytov and Kratsios](https://arxiv.org/pdf/2006.02341.pdf).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NAIVE_NEU_ffNN(feature_map_depth, feature_map_height, ## NEU-Feature Map Hyper-Parameter(s)\n",
    "                       height, depth, ## ffNN Parameter(s)\n",
    "                       readout_map_depth, readout_map_height,\n",
    "                       learning_rate, input_dim, output_dim): ## Training Parameters\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(d,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    \n",
    "    # Initial Features\n",
    "    inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN)\n",
    "    # Higher-Order Feature Depth\n",
    "    if feature_map_depth > 0:\n",
    "        inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN)\n",
    "\n",
    "    \n",
    "    \n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    # First ffNN Layer: Reconfigured inputs -> Hidden Neurons\n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN_feature)\n",
    "    # Higher-Order Deep Layers: Hidden Neurons -> Hidden Neurons\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "    # Last ffNN Layer: Hidden Neurons -> Output Space\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)     \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###########-#\n",
    "    # Readout Map #\n",
    "    #-###########-#\n",
    "    # Input -> Input x ffNN output\n",
    "    output_layer_new = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    # Add Depth to Readout Map\n",
    "    if readout_map_depth > 0:\n",
    "        output_layer_new = Deep_GLd_Layer(d+D)(output_layer_new)\n",
    "\n",
    "    # Project down from graph space to output space (from: Input x Outputs -> Outputs)\n",
    "    output_layer = projection_layer(output_layer_new)\n",
    "    \n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    ffNN.compile(optimizer=opt, loss=Robust_MSE, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return ffNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-ffNN Training Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NAIVE_NEU_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    NAIVE_NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NAIVE_NEU_ffNN, \n",
    "                                                            verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NAIVE_NEU_ffNN_CV = RandomizedSearchCV(estimator=NAIVE_NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NAIVE_NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NAIVE_NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NAIVE_NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NAIVE_NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NAIVE_NEU-ffNN: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NAIVE_NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NAIVE_NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/Naive_NEU/Best_Parameters.tex\")\n",
    "    print('NAIVE_NEU-ffNN: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Coupled NEU-Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_OLS_FullyCoupled(learning_rate, input_dim, output_dim, feature_map_depth, readout_map_depth, feature_map_height,readout_map_height,robustness_parameter,homotopy_parameter,implicit_dimension):\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map_prep = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    deep_feature_map = tf.concat([input_layer, deep_feature_map_prep], axis=1)\n",
    "    ## Homeomorphic Part\n",
    "    dimension_lifted = (input_dim + embedding_dimension)    \n",
    "    ### Execute Random Embedding\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "#        # First Layer\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=dimension_lifted, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        deep_feature_map = fullyConnected_Dense_Invertible(embedding_dimension)(input_layer)\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    core_layers = fullyConnected_Dense(output_dim)(deep_feature_map)  \n",
    "    \n",
    "    deep_readout_map = tf.concat([input_layer, core_layers], axis=1)\n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(output_dim+input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(output_dim+input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    \n",
    "    # Projection Layer\n",
    "    output_layer = projection_layer(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete NEU-ffNN Training Procedure!!!\n"
     ]
    }
   ],
   "source": [
    "def build_NEU_OLS_FullyCoupled(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_OLS_FullyCoupled, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_ffNN_CV = RandomizedSearchCV(estimator=NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-ffNN: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-ffNN: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Specialized Architecture(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_PCA(input_dim,\n",
    "                learning_rate,\n",
    "                PCA_Rank,\n",
    "                implicit_dimension,\n",
    "                feature_map_depth,\n",
    "                feature_map_height,\n",
    "                homotopy_parameter):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(PCA_Rank,))\n",
    "\n",
    "    #----------------------#\n",
    "    # Core Layers: PCA #\n",
    "    #----------------------#\n",
    "#     # PCA\n",
    "#     encoder = fullyConnected_Dense(PCA_Rank)(input_layer)\n",
    "    # Reconstructor\n",
    "\n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(PCA_Rank,implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map_prep = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    deep_feature_map = tf.concat([input_layer, deep_feature_map_prep], axis=1)\n",
    "    ## Homeomorphic Part\n",
    "    dimension_lifted = (PCA_Rank + embedding_dimension)\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "        # First Layer\n",
    "        ## Spacial-Dependent part of reconfiguration unit\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=dimension_lifted, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        ## Constant part of reconfiguration unit\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "\n",
    "    # PCA Readout (Really this is the OLS model)\n",
    "    decoder = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "\n",
    "\n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    NEU_PCA = tf.keras.Model(input_layer, decoder)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    NEU_PCA.compile(metrics=['accuracy'],loss='mean_squared_error',optimizer='Adam')\n",
    "\n",
    "    # Return NEU PCA\n",
    "    return NEU_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NEU_PCA(n_folds, \n",
    "                  n_jobs, \n",
    "                  n_iter, \n",
    "                  param_grid_in, \n",
    "                  X_train_scaled,\n",
    "                  X_test_scaled, \n",
    "                  X_train,\n",
    "                  PCA_Rank):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    param_grid_in_internal['PCA_Rank'] = [PCA_Rank]\n",
    "\n",
    "    print('Performing PCA')\n",
    "    #----------------------#\n",
    "    # Core Layers: PCA     #\n",
    "    #----------------------#\n",
    "    # Get Low-Dimensional Representation\n",
    "    X_train_internal,X_test_internal, Rpca, Rpca_test = get_PCAs(X_train_scaled=X_train_scaled,\n",
    "                                                                 X_test_scaled=X_test_scaled,\n",
    "                                                                 PCA_Rank=PCA_Rank)\n",
    "    \n",
    "    X_train_internal = X_train_internal[:,:PCA_Rank]\n",
    "    X_test_internal = X_test_internal[:,:PCA_Rank]\n",
    "    #-------------------------------------------------#\n",
    "    print('Performing PCAs Computed')\n",
    "    \n",
    "    # Deep Feature Network\n",
    "    NEU_PCA_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_PCA, verbose=True)\n",
    "\n",
    "    # Randomized CV\n",
    "    NEU_PCA_CV = RandomizedSearchCV(estimator=NEU_PCA_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020,shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "\n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    print('Training NEU-Feature Map!')\n",
    "    NEU_PCA_CV.fit(X_train_internal,X_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    NEU_PCA_Reconstruction_train = NEU_PCA_CV.predict(X_train_internal)\n",
    "    NEU_PCA_Reconstruction_test = NEU_PCA_CV.predict(X_test_internal)\n",
    "\n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_PCA_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_NEU_PCA = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-PCA: Trained!')\n",
    "\n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-PCA: Saving')\n",
    "    #     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_PCA_CV.best_params_['N_Trainable_Parameters'] = N_params_best_NEU_PCA\n",
    "    Path('./outputs/models/NEU/NEU_PCA/').mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(NEU_PCA_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/NEU_PCA/Best_Parameters.tex\")\n",
    "    print('NEU-PCA: Saved')\n",
    "\n",
    "    # Get Factor(s) #\n",
    "    # --------------#\n",
    "#     # Extract Auto-Encoder Layer\n",
    "#     encoder_layer = tf.keras.Model(inputs=best_model.model.inputs, outputs=best_model.model.layers[1].output)\n",
    "#     # Get Feature(s)\n",
    "#     # ## Train\n",
    "#     NEU_PCA_Factors_train = np.array(encoder_layer.predict(X_train_scaled))\n",
    "#     NEU_PCA_Factors_test = np.array(encoder_layer.predict(X_test_scaled))\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print('Complete NEU-PCA Training Procedure!!!')\n",
    "\n",
    "    # Return Values #\n",
    "    # ---------------#\n",
    "    return NEU_PCA_Reconstruction_train, NEU_PCA_Reconstruction_test, X_train_internal, X_test_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoencoder(input_dim,\n",
    "                    learning_rate,\n",
    "                    PCA_Rank):\n",
    "    \n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    # Get encoder depth\n",
    "    Encoder_depth = 5\n",
    "    print('We use a DNN of depth: '+str(3+2*Encoder_depth))\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "\n",
    "\n",
    "    #----------------------#\n",
    "    # Core Layers: Encoder #\n",
    "    #----------------------#\n",
    "    encoder = fullyConnected_Dense(512)(input_layer)\n",
    "    encoder = tf.nn.relu(encoder)\n",
    "    encoder = fullyConnected_Dense(128)(encoder)\n",
    "    encoder = tf.nn.relu(encoder)\n",
    "    encoder = fullyConnected_Dense(PCA_Rank)(encoder)\n",
    "\n",
    "    #----------------------#\n",
    "    # Core Layers: Decoder #\n",
    "    #----------------------#\n",
    "    # PCA Readout (Really this is the OLS model)\n",
    "    decoder = fullyConnected_Dense(PCA_Rank)(encoder)\n",
    "    decoder = tf.nn.relu(decoder)\n",
    "    decoder = fullyConnected_Dense(128)(decoder)\n",
    "    decoder = tf.nn.relu(decoder)\n",
    "    decoder = fullyConnected_Dense(512)(decoder)\n",
    "    decoder = tf.nn.relu(decoder)\n",
    "    # Readout Layer #\n",
    "    #---------------#\n",
    "    decoder = fullyConnected_Dense(input_dim)(decoder)\n",
    "#     decoder = tf.nn.sigmoid(decoder)\n",
    "\n",
    "\n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    autoencoder = tf.keras.Model(input_layer, decoder)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    autoencoder.compile(metrics=['accuracy'],loss='mean_squared_error',optimizer='Adam')\n",
    "    \n",
    "    # Give Auto-Encoder\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(n_folds, \n",
    "                      n_jobs, \n",
    "                      n_iter, \n",
    "                      X_train_scaled, \n",
    "                      X_train, \n",
    "                      X_test_scaled,\n",
    "                      PCA_Rank):\n",
    "    \n",
    "    print('Begin autoencoder Training')\n",
    "    # Update Dictionary\n",
    "    Encoder_depth = 5\n",
    "    param_grid_in_internal = Autoencoder_dictionary\n",
    "    param_grid_in_internal['input_dim'] = [(X_train_scaled.shape[1])]\n",
    "    param_grid_in_internal['PCA_Rank'] = [int(PCA_Rank)]\n",
    "\n",
    "    # # Deep Feature Network\n",
    "    AE_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_autoencoder, verbose=True)\n",
    "\n",
    "    # Randomized CV\n",
    "    AE_CV = RandomizedSearchCV(estimator=AE_CV, \n",
    "                               n_jobs=n_jobs,\n",
    "                               cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                               param_distributions=param_grid_in_internal,\n",
    "                               n_iter=n_iter,\n",
    "                               return_train_score=True,\n",
    "                               random_state=2020,\n",
    "                               verbose=10)\n",
    "\n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    AE_CV.fit(X_train,X_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    # -------------------#\n",
    "    AE_Reconstructed_train = AE_CV.predict(X_train_scaled)\n",
    "    AE_Reconstructed_test = AE_CV.predict(X_test_scaled)\n",
    "\n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = AE_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_AE = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('Autoencoder: Trained!')\n",
    "\n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('Autoencoder: Saving')\n",
    "    #     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    AE_CV.best_params_['N_Trainable_Parameters'] = N_params_best_AE\n",
    "    Path('./outputs/models/Autoencoder/').mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(AE_CV.best_params_,orient='index').to_latex(\"./outputs/models/Autoencoder/Best_Parameters.tex\")\n",
    "    print('Autoencoder: Saved')\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print('Complete Autoencoder Training Procedure!!!')\n",
    "\n",
    "    # Get Factor(s) #\n",
    "    # --------------#\n",
    "    # Extract Auto-Encoder Layer\n",
    "    encoder_layer = tf.keras.Model(inputs=best_model.model.inputs, outputs=best_model.model.layers[Encoder_depth].output)\n",
    "    # Get Feature(s)\n",
    "    # ## Train\n",
    "    AE_Factors_train = np.array(encoder_layer.predict(X_train_scaled))\n",
    "    AE_Factors_test = np.array(encoder_layer.predict(X_test_scaled))\n",
    "\n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return AE_Reconstructed_train, AE_Reconstructed_test, AE_Factors_train, AE_Factors_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEU Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_Autoencoder(input_dim,\n",
    "                        learning_rate,\n",
    "                        PCA_Rank,\n",
    "                        implicit_dimension,\n",
    "                        feature_map_depth,\n",
    "                        feature_map_height,\n",
    "                        homotopy_parameter):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "\n",
    "    #----------------------#\n",
    "    # Core Layers: PCA #\n",
    "    #----------------------#\n",
    "    # PCA\n",
    "    encoder = fullyConnected_Dense(PCA_Rank)(input_layer)\n",
    "    for i in range(feature_map_depth):\n",
    "        # Reconstructor\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,\n",
    "                                                 home_space_dim=PCA_Rank, \n",
    "                                                 homotopy_parameter = homotopy_parameter)(encoder)\n",
    "        ## Constant part of reconfiguration unit\n",
    "        deep_feature_map_out = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(PCA_Rank,implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map_prep = fullyConnected_Dense(embedding_dimension)(deep_feature_map_out)\n",
    "    deep_feature_map = tf.concat([deep_feature_map_out, deep_feature_map_prep], axis=1)\n",
    "    ## Homeomorphic Part\n",
    "    dimension_lifted = (PCA_Rank + embedding_dimension)\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "        # First Layer\n",
    "        ## Spacial-Dependent part of reconfiguration unit\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,\n",
    "                                                 home_space_dim=dimension_lifted, \n",
    "                                                 homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        ## Constant part of reconfiguration unit\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "\n",
    "    # PCA Readout (Really this is the OLS model)\n",
    "    decoder = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "\n",
    "\n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    NEU_PCA = tf.keras.Model(input_layer, decoder)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    NEU_PCA.compile(metrics=['accuracy'],loss='mean_squared_error',optimizer='Adam')\n",
    "\n",
    "    # Return NEU PCA\n",
    "    return NEU_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NEU_Autoencoder(n_folds, \n",
    "                          n_jobs, \n",
    "                          n_iter, \n",
    "                          param_grid_in, \n",
    "                          X_train_scaled,\n",
    "                          X_test_scaled, \n",
    "                          X_train,\n",
    "                          PCA_Rank):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    param_grid_in_internal['PCA_Rank'] = [PCA_Rank]\n",
    "\n",
    "    print('Performing AE')\n",
    "    # Deep Feature Network\n",
    "    NEU_PCA_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_Autoencoder, verbose=True)\n",
    "\n",
    "    # Randomized CV\n",
    "    NEU_PCA_CV = RandomizedSearchCV(estimator=NEU_PCA_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020,shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "\n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    print('Training NEU-Feature Map!')\n",
    "    NEU_PCA_CV.fit(X_train_scaled,X_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    NEU_PCA_Reconstruction_train = NEU_PCA_CV.predict(X_train_scaled)\n",
    "    NEU_PCA_Reconstruction_test = NEU_PCA_CV.predict(X_test_scaled)\n",
    "\n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_PCA_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_NEU_PCA = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-AE: Trained!')\n",
    "\n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-AE: Saving')\n",
    "    #     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_PCA_CV.best_params_['N_Trainable_Parameters'] = N_params_best_NEU_PCA\n",
    "    Path('./outputs/models/NEU/NEU_PCA/').mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(NEU_PCA_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/NEU_PCA/Best_Parameters.tex\")\n",
    "    print('NEU-AE: Saved')\n",
    "\n",
    "    # Get Factor(s) #\n",
    "    # --------------#\n",
    "    # Extract Auto-Encoder Layer\n",
    "    Encoder_Depth = 1 + 2*NEU_PCA_CV.best_params_['feature_map_depth']\n",
    "    encoder_layer = tf.keras.Model(inputs=best_model.model.inputs, outputs=best_model.model.layers[Encoder_Depth].output)\n",
    "    # Get Feature(s)\n",
    "    # ## Train\n",
    "    NEU_PCA_Factors_train = np.array(encoder_layer.predict(X_train_scaled))\n",
    "    NEU_PCA_Factors_test = np.array(encoder_layer.predict(X_test_scaled))\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print('Complete NEU-AE Training Procedure!!!')\n",
    "\n",
    "    # Return Values #\n",
    "    # ---------------#\n",
    "    return NEU_PCA_Reconstruction_train, NEU_PCA_Reconstruction_test, NEU_PCA_Factors_train, NEU_PCA_Factors_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
