{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions Depot\n",
    "This little script contains all the custom helper functions required to run any segment of the NEU and its benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Dependances and makes path(s)\n",
    "# exec(open('Initializations_Dump.py').read())\n",
    "# # Load Hyper( and meta) parameter(s)\n",
    "# exec(open('HyperParameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE, between 0 and 100\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    y_true.shape = (y_true.shape[0], 1)\n",
    "    y_pred.shape = (y_pred.shape[0], 1)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Shortfall Loss\n",
    "*These loss functions also have a robust representation but are only used for debugging/sanitychecking and are not included in the final version of the code or the paper itself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_percentile(x, p): #assuming the input is flattened: (n,)\n",
    "\n",
    "    samples = Kb.cast(Kb.shape(x)[0], Kb.floatx()) #batch size\n",
    "    p =  (100. - p)/100.  #100% will return 0 elements, 0% will return all elements\n",
    "\n",
    "    #selected samples\n",
    "    values, indices = tf.math.top_k(x, samples)\n",
    "\n",
    "    return values\n",
    "\n",
    "def Robust_MSE_ES(p):\n",
    "    def ES_p_loss(y_true, y_predicted):\n",
    "        ses = Kb.pow(y_true-y_predicted,2)\n",
    "        above = above_percentile(Kb.flatten(ses), p)\n",
    "        return Kb.mean(above)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust MSE\n",
    "These next loss-functions are the ones used in the paper; they solve the approximate robust MSE problem:\n",
    "$$\n",
    "MSE_{robust}(x,y)\\triangleq \\operatorname{argmax}_{w\\in \\Delta_N} \\sum_{n=1}^N w_n\\|x_n-y_n\\|^2 - \\lambda \\sum_{n=1}^N w_n \\log\\left(\\frac1{N}\\right)\n",
    ";\n",
    "$$\n",
    "where $\\Delta_N\\triangleq \\left\\{w \\in \\mathbb{R}^N:\\, 0\\leq w_n\\leq 1\\mbox{ and } \\sum_{n=1}^N w_n =1\\right\\}$ is the probability simplex!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Version (Formulation with same arg-minima)\n",
    "# @tf.function\n",
    "def Entropic_Risk(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = tf.math.abs((y_true - y_pred))\n",
    "    loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "    loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "    # Return Value\n",
    "    return loss_out\n",
    "\n",
    "# def Robust_MSE(y_true, y_pred):\n",
    "#     # Compute Exponential Utility\n",
    "#     loss_out = tf.math.abs((y_true - y_pred))\n",
    "#     loss_out = tf.math.exp(robustness_parameter*loss_out)\n",
    "#     loss_out = tf.math.reduce_sum(loss_out)\n",
    "\n",
    "#     # Return Value\n",
    "#     return loss_out\n",
    "def Robust_MSE(robustness_parameter=0.05):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Initialize Loss\n",
    "        absolute_errors_eval = tf.math.abs((y_true - y_pred))\n",
    "\n",
    "        # Compute Exponential        \n",
    "        loss_out_expweights = tf.math.exp(robustness_parameter*absolute_errors_eval)\n",
    "        loss_out_expweights_totals = tf.math.reduce_sum(loss_out_expweights)\n",
    "        loss_out_weighted = loss_out_expweights/tf.math.reduce_sum(loss_out_expweights)\n",
    "        loss_out_weighted = loss_out_weighted*absolute_errors_eval\n",
    "        loss_out_weighted = tf.math.reduce_sum(loss_out_weighted)\n",
    "\n",
    "        # Compute Average Loss\n",
    "        #loss_average = tf.math.reduce_mean(absolute_errors_eval)\n",
    "\n",
    "        # Return Value\n",
    "        loss_out = loss_out_weighted# - loss_average\n",
    "\n",
    "        return loss_out\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homotopic Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Robust_MSE_homotopic(robustness_parameter=0.05, homotopy_parameter=1, input_tensor):\n",
    "#     def loss(y_true, y_pred):\n",
    "#         # Initialize Loss\n",
    "#         absolute_errors_eval = tf.math.abs((y_true - y_pred))\n",
    "\n",
    "#         # Compute Exponential        \n",
    "#         loss_out_expweights = tf.math.exp(robustness_parameter*absolute_errors_eval)\n",
    "#         loss_out_expweights_totals = tf.math.reduce_sum(loss_out_expweights)\n",
    "#         loss_out_weighted = loss_out_expweights/tf.math.reduce_sum(loss_out_expweights)\n",
    "#         loss_out_weighted = loss_out_weighted*absolute_errors_eval\n",
    "#         loss_out_weighted = tf.math.reduce_sum(loss_out_weighted)\n",
    "\n",
    "#         # Compute Homotopy Penalty\n",
    "#         absolute_errors_eval = tf.math.abs((y_true - y_pred))\n",
    "\n",
    "#         # Return Value\n",
    "#         loss_out = loss_out_weighted# - loss_average\n",
    "\n",
    "#         return loss_out\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Numpy Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Version (Full dual Version)\n",
    "def Robust_MSE_numpy(y_true, y_pred):\n",
    "    # Compute Exponential Utility\n",
    "    loss_out = np.abs((y_true - y_pred))\n",
    "    loss_out = np.exp(robustness_parameter*loss_out)\n",
    "    loss_out = np.mean(loss_out)\n",
    "    loss_out = np.log(loss_out)/robustness_parameter\n",
    "    # Return Value\n",
    "    return loss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Custom Layers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Feed-Forward Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homeomorphism Layers:\n",
    "- Shift\n",
    "- Euclidean Group\n",
    "- Special Affine Group\n",
    "- Affine Group\n",
    "\n",
    "- *Reconfiguration Unit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift $\\mathbb{R}^d$  Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x \\mapsto x +b$ for some trainable $b\\in \\mathbb{R}^{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shift_Layers(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Shift_Layers, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{E}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{O}_{d}(\\mathbb{R})$  Layers\n",
    "This is the group of all isometries of $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Euclidean_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Euclidean_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        On = tf.linalg.matmul((self.Id + self.glw),tf.linalg.inv(self.Id - self.glw))\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(On,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{SAff}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{SL}_{d}(\\mathbb{R})$  Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $A \\in \\operatorname{SL}_d(\\mathbb{R})$ if and only if $A=\\frac1{\\sqrt[d]{\\det(\\exp(X))}} \\exp(X)$ for some $d\\times d$ matrix $X$.  \n",
    "\n",
    "*Why?*... We use the fact that $\\det(k A) = k^d \\det(A)$ for any $k \\in \\mathbb{R}$ and any $d\\times d$ matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Special_Affine_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Special_Affine_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN_det = tf.linalg.det(GLN)\n",
    "        GLN_det = tf.pow(tf.abs(GLN_det),(1/(d+D)))\n",
    "        SLN = tf.math.divide(GLN,GLN_det)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(SLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep GLd Layer:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Deep-GL}_d(x) \\triangleq& f^{Depth}\\circ \\dots \\circ f^1(x)\\\\\n",
    "f^i(x)\\triangleq &\\exp(A_2) \\operatorname{Leaky-ReLU}\\left(\n",
    "\\exp(A_1)x + b_1\n",
    "\\right)+ b_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $A_i$ are $d\\times d$ matrices and $b_i \\in \\mathbb{R}^d$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_GLd_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Deep_GLd_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "#         self.Id = self.add_weight(name='Identity_Matrix',\n",
    "#                                    shape=(input_shape[-1],input_shape[-1]),\n",
    "#                                    initializer='identity',\n",
    "#                                    trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "#         Element of gl_d\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.glw2 = self.add_weight(name='Tangential_Weights2',\n",
    "                                       shape=(input_shape[-1],input_shape[-1]),\n",
    "                                       initializer='GlorotUniform',\n",
    "                                       trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        self.b2 = self.add_weight(name='location_parameter2',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN2 = tf.linalg.expm(self.glw2)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "\n",
    "        x_out = tf.linalg.matvec(GLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        x_out = tf.nn.leaky_relu(x_out)\n",
    "        x_out = tf.linalg.matvec(GLN2,x_out)\n",
    "        x_out = x_out + self.b2\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple version is the following:\n",
    "This is the code use in the [background article](https://arxiv.org/abs/2006.02341)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense_Invertible(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense_Invertible, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        # Numerical Stability Parameter(s)\n",
    "        #----------------------------------#\n",
    "        self.num_stab_param = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ## Initiality Numerical Stability\n",
    "        numerical_stabilizer = tf.eye(D)*(self.num_stab_param*10**(-3))\n",
    "        expw_log = self.w + numerical_stabilizer\n",
    "#         rescaler = tf.norm(expw_log)\n",
    "        # Cayley Transform\n",
    "        expw = tf.linalg.matmul((self.Id + self.w),tf.linalg.inv(self.Id - self.w)) \n",
    "        # Lie Version\n",
    "#         expw_log = expw_log /rescaler\n",
    "        expw = tf.linalg.expm(expw_log)\n",
    "        return tf.matmul(inputs, expw) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## NEU Layers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New NEU\n",
    "- Feature map version (only differs up to dimension constant)\n",
    "- Readout map version\n",
    "- Constrained parametereized swish with $\\beta \\in \\left[-\\frac1{2},\\frac1{2}\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implemented Version:**: *Lie Version:* $$\n",
    "x \\mapsto \\exp\\left(\n",
    "%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) x.\n",
    "$$\n",
    "\n",
    "**Non-Implemented Variant:**  *Cayley version:*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}(A(x)):\\,x \\mapsto & \\left[(I_d + A(x))(I_d- A(x))^{-1}\\right]x\n",
    "\\\\\n",
    "A(x)\\triangleq &%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the inverse of the Cayley transform of $A(x)$ is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}^{-1}(A(x)):\\,x \\mapsto & \\left[(I_d - A(x))(I_d+ A(x))^{-1}\\right]x\n",
    ".\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32, home_space_dim = d, homotopy_parameter = 0):\n",
    "        super(Reconfiguration_unit, self).__init__()\n",
    "        self.units = units\n",
    "        self.home_space_dim = home_space_dim\n",
    "        self.homotopy_parameter = homotopy_parameter\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(self.home_space_dim,),\n",
    "                                    trainable=True,\n",
    "                                    initializer='random_normal',\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #                                  Decay Rates                                       #\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.sigma = self.add_weight(name='bump_threshfold',\n",
    "                                        shape=[1],\n",
    "                                        initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                        trainable=True,\n",
    "                                        constraint=tf.keras.constraints.NonNeg())\n",
    "        self.a = self.add_weight(name='bump_scale',\n",
    "                                        shape=[1],\n",
    "                                        initializer='ones',\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bump_location',\n",
    "                                        shape=[1],\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Exponential Decay\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.exponential_decay = self.add_weight(name='exponential_decay_rate',\n",
    "                                                 shape=[1],\n",
    "                                                 initializer=RandomUniform(minval=.5, maxval=10),\n",
    "                                                 trainable=True,\n",
    "                                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Mixture\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.m_w1 = self.add_weight(name='no_decay',\n",
    "                                    shape=[1],\n",
    "                                    initializer=RandomUniform(minval=.001, maxval=0.01),\n",
    "                                    trainable=True,\n",
    "                                    constraint=tf.keras.constraints.NonNeg(),\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        self.m_w2 = self.add_weight(name='weight_exponential',\n",
    "                                    shape=[1],\n",
    "                                    initializer='zeros',#RandomUniform(minval=.001, maxval=0.01),\n",
    "                                    trainable=True,\n",
    "                                    constraint=tf.keras.constraints.NonNeg(),\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        self.m_w3 = self.add_weight(name='bump',\n",
    "                                    shape=[1],\n",
    "                                    initializer='zeros',#RandomUniform(minval=.001, maxval=0.01),\n",
    "                                    trainable=True,\n",
    "                                    constraint=tf.keras.constraints.NonNeg(),\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # No Decay\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1',\n",
    "                                   shape=(self.units,(self.home_space_dim**2)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Exponential Decay\n",
    "        self.Tw1_b = self.add_weight(name='Tangential_Weights_1_b',\n",
    "                           shape=(self.units,(self.home_space_dim**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_b = self.add_weight(name='Tangential_Weights_2_b',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Bump\n",
    "        self.Tw1_c = self.add_weight(name='Tangential_Weights_1_c',\n",
    "                           shape=(self.units,(self.home_space_dim**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_c = self.add_weight(name='Tangential_Weights_2_c',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "        # Numerical Stability Parameter(s)\n",
    "        #----------------------------------#\n",
    "        self.num_stab_param = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # C^{\\infty} bump function (numerically unstable...) #\n",
    "    #----------------------------------------------------#\n",
    "#     def bump_function(self, x):\n",
    "#         return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "    # C^1 bump function (numerically stable??) #\n",
    "    #----------------------------------------------------#\n",
    "    def bump_function(self, x):\n",
    "#         return tf.math.pow(x-self.sigma,2)*tf.math.pow(x+self.sigma,2)\n",
    "        bump_out = tf.math.pow(x-self.sigma,2)*tf.math.pow(x+self.sigma,2)\n",
    "        bump_out = tf.math.pow(bump_out,(1/8))\n",
    "        return bump_out\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.norm(input) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Decay Rate Functions\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function (Local Behaviour)\n",
    "        bump_input = self.a*norm_inputs + self.b\n",
    "        greater = tf.math.greater(bump_input, -self.sigma)\n",
    "        less = tf.math.less(bump_input, self.sigma)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bump_decay = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(bump_input),\n",
    "            0.0)\n",
    "#         bump_decay = 1\n",
    "        \n",
    "        # Exponential Decay\n",
    "        exp_decay = tf.math.exp(-self.exponential_decay*norm_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = norm_inputs*self.Id\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        \n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw1,tangential_ffNN)         \n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw2,tangential_ffNN)\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb2\n",
    "        \n",
    "        # Exponential Decay\n",
    "        tangential_ffNN_b = norm_inputs*exp_decay*self.Id\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb1_b\n",
    "        \n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw1_b,tangential_ffNN_b)         \n",
    "        tangential_ffNN_b = tf.nn.relu(tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw2_b,tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb2_b\n",
    "        \n",
    "        # Bump (Local Aspect)\n",
    "        tangential_ffNN_c = bump_decay*norm_inputs*self.Id\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb1_c\n",
    "        \n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw1_c,tangential_ffNN_c)         \n",
    "        tangential_ffNN_c = tf.nn.relu(tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw2_c,tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb2_c\n",
    "    \n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "        tangential_ffNN_b = (tf.transpose(tangential_ffNN_b) - tangential_ffNN_b) \n",
    "        tangential_ffNN_c = (tf.transpose(tangential_ffNN_c) - tangential_ffNN_c) \n",
    "        # Decay\n",
    "        tangential_ffNN = (self.m_w1*tangential_ffNN) + (self.m_w2*tangential_ffNN_b) + (self.m_w3*tangential_ffNN_c) \n",
    "            \n",
    "            \n",
    "        # NUMERICAL STABILIZER\n",
    "        #----------------------#\n",
    "#         tangential_ffNN = tangential_ffNN + tf.eye(self.home_space_dim) *(self.num_stab_param*10**(-3))\n",
    "        tangential_ffNN = tf.math.maximum(tf.math.minimum(-tangential_ffNN,10**(15)),-(10**(15)))\n",
    "    \n",
    "        # Lie Parameterization:  \n",
    "        tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        # Cayley Transformation (Stable):\n",
    "#         tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.pinv(self.Id - tangential_ffNN)) \n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,input) + self.location\n",
    "#         x_out = tf.linalg.matvec(tangential_ffNN,input)\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identity Initialized Variant:\n",
    "Only use this for greedy \"chaining procedure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit_identity_initialization(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32, home_space_dim = d, homotopy_parameter = 0):\n",
    "        super(Reconfiguration_unit_identity_initialization, self).__init__()\n",
    "        self.units = units\n",
    "        self.home_space_dim = home_space_dim\n",
    "        self.homotopy_parameter = homotopy_parameter\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(self.home_space_dim,),\n",
    "                                    trainable=True,\n",
    "                                    initializer='zeros',\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #                                  Decay Rates                                       #\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.sigma = self.add_weight(name='bump_threshfold',\n",
    "                                        shape=[1],\n",
    "                                        initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                        trainable=True,\n",
    "                                        constraint=tf.keras.constraints.NonNeg())\n",
    "        self.a = self.add_weight(name='bump_scale',\n",
    "                                        shape=[1],\n",
    "                                        initializer='ones',\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bump_location',\n",
    "                                        shape=[1],\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Exponential Decay\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.exponential_decay = self.add_weight(name='exponential_decay_rate',\n",
    "                                                 shape=[1],\n",
    "                                                 initializer=RandomUniform(minval=.5, maxval=10),\n",
    "                                                 trainable=True,\n",
    "                                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Mixture\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.m_w1 = self.add_weight(name='no_decay',\n",
    "                                    shape=[1],\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True,\n",
    "                                    constraint=tf.keras.constraints.NonNeg(),\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        self.m_w2 = self.add_weight(name='weight_exponential',\n",
    "                                    shape=[1],\n",
    "                                    initializer='zeros',#RandomUniform(minval=.001, maxval=0.01),\n",
    "                                    trainable=True,\n",
    "                                    constraint=tf.keras.constraints.NonNeg(),\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        self.m_w3 = self.add_weight(name='bump',\n",
    "                                    shape=[1],\n",
    "                                    initializer='zeros',#RandomUniform(minval=.001, maxval=0.01),\n",
    "                                    trainable=True,\n",
    "                                    constraint=tf.keras.constraints.NonNeg(),\n",
    "                                    regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # No Decay\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1',\n",
    "                                   shape=(self.units,(self.home_space_dim**2)),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        # Exponential Decay\n",
    "        self.Tw1_b = self.add_weight(name='Tangential_Weights_1_b',\n",
    "                           shape=(self.units,(self.home_space_dim**2)),\n",
    "                           initializer='zeros',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_b = self.add_weight(name='Tangential_Weights_2_b',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        # Bump\n",
    "        self.Tw1_c = self.add_weight(name='Tangential_Weights_1_c',\n",
    "                           shape=(self.units,(self.home_space_dim**2)),\n",
    "                           initializer='zeros',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_c = self.add_weight(name='Tangential_Weights_2_c',\n",
    "                                   shape=((self.home_space_dim**2),self.units),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=((self.home_space_dim**2),1),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(self.home_space_dim,self.home_space_dim),\n",
    "                                   initializer='zeros',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "        # Numerical Stability Parameter(s)\n",
    "        #----------------------------------#\n",
    "        self.num_stab_param = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    # C^{\\infty} bump function (numerically unstable...) #\n",
    "    #----------------------------------------------------#\n",
    "#     def bump_function(self, x):\n",
    "#         return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "    # C^1 bump function (numerically stable??) #\n",
    "    #----------------------------------------------------#\n",
    "    def bump_function(self, x):\n",
    "#         return tf.math.pow(x-self.sigma,2)*tf.math.pow(x+self.sigma,2)\n",
    "        bump_out = tf.math.pow(x-self.sigma,2)*tf.math.pow(x+self.sigma,2)\n",
    "        bump_out = tf.math.pow(bump_out,(1/8))\n",
    "        return bump_out\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.norm(input) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Decay Rate Functions\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function (Local Behaviour)\n",
    "        bump_input = self.a*norm_inputs + self.b\n",
    "        greater = tf.math.greater(bump_input, -self.sigma)\n",
    "        less = tf.math.less(bump_input, self.sigma)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bump_decay = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(bump_input),\n",
    "            0.0)\n",
    "#         bump_decay = 1\n",
    "        \n",
    "        # Exponential Decay\n",
    "        exp_decay = tf.math.exp(-self.exponential_decay*norm_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = norm_inputs*self.Id\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        \n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw1,tangential_ffNN)         \n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw2,tangential_ffNN)\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb2\n",
    "        \n",
    "        # Exponential Decay\n",
    "        tangential_ffNN_b = norm_inputs*exp_decay*self.Id\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb1_b\n",
    "        \n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw1_b,tangential_ffNN_b)         \n",
    "        tangential_ffNN_b = tf.nn.relu(tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw2_b,tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb2_b\n",
    "        \n",
    "        # Bump (Local Aspect)\n",
    "        tangential_ffNN_c = bump_decay*norm_inputs*self.Id\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[(self.home_space_dim**2),1])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb1_c\n",
    "        \n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw1_c,tangential_ffNN_c)         \n",
    "        tangential_ffNN_c = tf.nn.relu(tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw2_c,tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[self.home_space_dim,self.home_space_dim])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb2_c\n",
    "    \n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "        tangential_ffNN_b = (tf.transpose(tangential_ffNN_b) - tangential_ffNN_b) \n",
    "        tangential_ffNN_c = (tf.transpose(tangential_ffNN_c) - tangential_ffNN_c) \n",
    "        # Decay\n",
    "        tangential_ffNN = (self.m_w1*tangential_ffNN) + (self.m_w2*tangential_ffNN_b) + (self.m_w3*tangential_ffNN_c) \n",
    "            \n",
    "            \n",
    "        # NUMERICAL STABILIZER\n",
    "        #----------------------#\n",
    "#         tangential_ffNN = tangential_ffNN + tf.eye(self.home_space_dim) *(self.num_stab_param*10**(-3))\n",
    "        tangential_ffNN = tf.math.maximum(tf.math.minimum(-tangential_ffNN,10**(15)),-(10**(15)))\n",
    "    \n",
    "        # Lie Parameterization:  \n",
    "        tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,input) + self.location\n",
    "#         x_out = tf.linalg.matvec(tangential_ffNN,input)\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully-connected Feed-forward layer with $GL_{d}$-connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense_Invertible(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense_Invertible, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        # Numerical Stability Parameter(s)\n",
    "        #----------------------------------#\n",
    "        self.num_stab_param = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='ones',\n",
    "                                         trainable=False,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        ## Initiality Numerical Stability\n",
    "        numerical_stabilizer = tf.eye(D)*(self.num_stab_param*10**(-3))\n",
    "        expw_log = self.w + numerical_stabilizer\n",
    "#         rescaler = tf.norm(expw_log)\n",
    "        # Cayley Transform\n",
    "#         expw = tf.linalg.matmul((self.Id + self.w),tf.linalg.inv(self.Id - self.w)) \n",
    "        # Lie Version\n",
    "#         expw_log = expw_log /rescaler\n",
    "        expw = tf.linalg.expm(expw_log)\n",
    "        return tf.matmul(inputs, expw) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma_{\\operatorname{rescaled-swish-trainable}}:x\\mapsto 2 \\frac{x}{1+ \\exp(-\\beta x)}=x\\left(1 + \\tanh(\\frac{\\beta}{2} x)\\right)\n",
    ";\\qquad \\beta \\in [0,\\infty)$.\n",
    "\n",
    "Instead, we use the following \"corrected\" parameterizable swish(-like) activation function\n",
    "$$\n",
    "\\sigma_{\\beta}:x\\mapsto + \\tanh(\\beta x);\\qquad \\beta \\in [0,\\infty).\n",
    "$$\n",
    "The benefit is that it is an isotopty to the identity, and asymptotically, it reproduces the step-like function:\n",
    "$$\n",
    "\\lim\\limits_{\\beta \\uparrow \\infty} \\sigma_{\\beta}(x) =\\sigma_{identity + step}(x)\\triangleq \\begin{cases}\n",
    "x +1 & x\\geq 0\\\\\n",
    "x-1 & x<0.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rescaled_swish_trainable(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32, homotopy_parameter = 0):\n",
    "        super(rescaled_swish_trainable, self).__init__()\n",
    "        self.units = units\n",
    "        self.homotopy_parameter = homotopy_parameter\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.relulevel = self.add_weight(name='relu_level',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg(),\n",
    "                                         regularizer=tf.keras.regularizers.L2(self.homotopy_parameter))\n",
    "                                \n",
    "    def call(self,inputs):\n",
    "        # Trainable Swish\n",
    "#         swish_numerator_rescaled = 2*inputs\n",
    "#         parameter = self.relulevel\n",
    "#         swish_denominator_trainable = tf.math.sigmoid(parameter*inputs)\n",
    "#         swish_trainable_out = tf.math.multiply(swish_numerator_rescaled,swish_denominator_trainable)\n",
    "        # Isotopic Activation\n",
    "        parameter = self.relulevel\n",
    "        non_linear_component = tf.math.tanh(parameter*inputs)\n",
    "        swish_trainable_out = inputs + non_linear_component\n",
    "        return swish_trainable_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection Layers\n",
    "This layer maps $(x,y)\\mapsto y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_layer = tf.keras.layers.Lambda(lambda x: x[:, -D:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class projection_layer(tf.keras.layers.Layer):\n",
    "\n",
    "#     def __init__(self, units=16, input_dim=32, product_dim=1, proj_dimension = 1):\n",
    "#         super(projection_layer, self).__init__()\n",
    "#         self.units = units\n",
    "#         self.proj_dimension = proj_dimension\n",
    "#         self.product_dim = product_dim\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         projector = tf.linalg.diag(np.concatenate([np.zeros(self.product_dim),np.ones(self.proj_dimension)]))\n",
    "#         return tf.matmul(inputs, projector) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Architecture Building Utilities:\n",
    "## NEU-Related:\n",
    "### Feature Extractor:\n",
    "This little function pops the final layer of a tensorflow model and replaces it with the identity.  When we apply it the the NEU-OLS we obtain only the trained linearizing feature map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trained_feature_map(model):\n",
    "\n",
    "    # Dissasemble Network\n",
    "    layers = [l for l in model.layers]\n",
    "\n",
    "    # Define new reconfiguration unit to be added\n",
    "    output_layer_new  = tf.identity(layers[len(layers)-2].output)\n",
    "\n",
    "    for i in range(len(layers)-1):\n",
    "        layers[i].trainable = False\n",
    "\n",
    "\n",
    "    # build model\n",
    "    new_model = tf.keras.Model(inputs=[layers[0].input], outputs=output_layer_new)\n",
    "    \n",
    "    # Return Output\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reporters and Summarizers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We empirically estimate the standard error and confidence intervals or the relevant error distributions using the method of this paper: [Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy - by: B. Efron and R. Tibshirani ](https://www.jstor.org/stable/2245500?casa_token=w_8ZaRuo1qwAAAAA%3Ax5kzbYXzxGSWj-EZaC10XyOVADJyKQGXOVA9huJejP9Tt7fgMNhmPhj-C3WdgbB9AEZdqjT5q_azPmBLH6pDq61jzVFxV4XxqBuerQRBLaaOFKcyr0s&seq=1#metadata_info_tab_contents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data, n=1000, func=np.mean):\n",
    "    \"\"\"\n",
    "    Generate `n` bootstrap samples, evaluating `func`\n",
    "    at each resampling. `bootstrap` returns a function,\n",
    "    which can be called to obtain confidence intervals\n",
    "    of interest.\n",
    "    \"\"\"\n",
    "    simulations = list()\n",
    "    sample_size = len(data)\n",
    "    xbar_init = np.mean(data)\n",
    "    for c in range(n):\n",
    "        itersample = np.random.choice(data, size=sample_size, replace=True)\n",
    "        simulations.append(func(itersample))\n",
    "    simulations.sort()\n",
    "    def ci(p):\n",
    "        \"\"\"\n",
    "        Return 2-sided symmetric confidence interval specified\n",
    "        by p.\n",
    "        \"\"\"\n",
    "        u_pval = (1+p)/2.\n",
    "        l_pval = (1-u_pval)\n",
    "        l_indx = int(np.floor(n*l_pval))\n",
    "        u_indx = int(np.floor(n*u_pval))\n",
    "        return(simulations[l_indx],xbar_init,simulations[u_indx])\n",
    "    return(ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical Summarizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Error_distribution_plots(test_set_data,\n",
    "                                 model_test_results,\n",
    "                                 NEU_model_test_results,\n",
    "                                 model_name):\n",
    "    # Initialization\n",
    "    import seaborn as sns\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Initialize NEU-Model Name\n",
    "    NEU_model_name = \"NEU-\"+model_name\n",
    "    plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "    # Initialize Errors\n",
    "    Er = model_test_results - data_x_test_raw.reshape(-1,)\n",
    "    NEU_Er = NEU_model_test_results - data_x_test_raw.reshape(-1,)\n",
    "    \n",
    "    # Internal Computations \n",
    "    xbar_init = np.mean(Er)\n",
    "    NEU_xbar_init = np.mean(NEU_Er)\n",
    "\n",
    "    # Generate Plots #\n",
    "    #----------------#\n",
    "    # generate 5000 resampled sample means  =>\n",
    "    means = [np.mean(np.random.choice(Er,size=len(Er),replace=True)) for i in range(5000)]\n",
    "    NEU_means = [np.mean(np.random.choice(NEU_Er,size=len(NEU_Er),replace=True)) for i in range(5000)]\n",
    "    sns.distplot(means, color='r', kde=True, hist_kws=dict(edgecolor=\"r\", linewidth=.675),label=model_name)\n",
    "    sns.distplot(NEU_means, color='b', kde=True, hist_kws=dict(edgecolor=\"b\", linewidth=.675),label=NEU_model_name)\n",
    "    plt.xlabel(\"Initial Sample Mean: {}\".format(xbar_init))\n",
    "    plt.title(\"Distribution of Sample Mean\")\n",
    "    plt.axvline(x=xbar_init) # vertical line at xbar_init\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(\"Model Predictions\")\n",
    "    # Save Plot\n",
    "    plt.savefig('./outputs/plotsANDfigures/'+model_name+'.pdf', format='pdf')\n",
    "    # Show Plot\n",
    "    if is_visuallty_verbose == True:\n",
    "        plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Summarizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------#\n",
    "#=### Results & Summarizing ###=#\n",
    "#-------------------------------#\n",
    "def reporter(y_train_hat_in,y_test_hat_in,y_train_in,y_test_in, N_bootstraps=(10**4)):\n",
    "    # Initialize Errors\n",
    "    train_set_errors = y_train_in - y_train_hat_in\n",
    "    test_set_errors = y_test_in - y_test_hat_in\n",
    "    \n",
    "    # Initalize and Compute: Bootstraped Confidence Intervals\n",
    "    train_boot = bootstrap(train_set_errors, n=N_bootstraps)\n",
    "    train_boot = np.asarray(train_boot(.95))\n",
    "    test_boot = bootstrap(test_set_errors, n=N_bootstraps)\n",
    "    test_boot = np.asarray(test_boot(.95))\n",
    "    \n",
    "    # Training Performance\n",
    "    Training_performance = np.array([mean_absolute_error(y_train_hat_in,y_train_in),\n",
    "                                     mean_squared_error(y_train_hat_in,y_train_in),\n",
    "                                     mean_absolute_percentage_error(y_train_hat_in,y_train_in)])\n",
    "    Training_performance = np.concatenate([train_boot,Training_performance])\n",
    "    \n",
    "    # Testing Performance\n",
    "    Test_performance = np.array([mean_absolute_error(y_test_hat_in,y_test_in),\n",
    "                                 mean_squared_error(y_test_hat_in,y_test_in),\n",
    "                                 mean_absolute_percentage_error(y_test_hat_in,y_test_in)])\n",
    "    Test_performance = np.concatenate([test_boot,Test_performance])\n",
    "    \n",
    "    # Organize into Dataframe\n",
    "    Performance_dataframe = pd.DataFrame({'Train': Training_performance,'Test': Test_performance})\n",
    "    Performance_dataframe.index = [\"Er. 95L\",\"Er. Mean\",\"Er. 95U\",\"MAE\",\"MSE\",\"MAPE\"]\n",
    "    return Performance_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variant is for multi-dimensional arrays such as in the MNIST example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------#\n",
    "#=### Results & Summarizing ###=#\n",
    "#-------------------------------#\n",
    "def reporter_array(y_train_hat_in,y_test_hat_in,y_train_in,y_test_in, N_bootstraps=(10**4)):\n",
    "    # Initialize Errors\n",
    "    train_set_errors = np.array(np.mean(y_train_in-y_train_hat_in,axis=1))\n",
    "    test_set_errors = np.array(np.mean(y_test_in - y_test_hat_in,axis=1))\n",
    "\n",
    "    # Initalize and Compute: Bootstraped Confidence Intervals\n",
    "    train_boot = bootstrap(train_set_errors, n=N_bootstraps)\n",
    "    train_boot = np.asarray(train_boot(.95))\n",
    "    test_boot = bootstrap(test_set_errors, n=N_bootstraps)\n",
    "    test_boot = np.asarray(test_boot(.95))\n",
    "\n",
    "    # Training Performance\n",
    "    Training_performance = np.array([mean_absolute_error(y_train_hat_in,y_train_in),\n",
    "                                     mean_squared_error(y_train_hat_in,y_train_in)])\n",
    "    Training_performance = np.concatenate([train_boot,Training_performance])\n",
    "\n",
    "    # Testing Performance\n",
    "    Test_performance = np.array([mean_absolute_error(y_test_hat_in,y_test_in),\n",
    "                                 mean_squared_error(y_test_hat_in,y_test_in)])\n",
    "    Test_performance = np.concatenate([test_boot,Test_performance])\n",
    "\n",
    "    # Organize into Dataframe\n",
    "    Performance_dataframe = pd.DataFrame({'Train': Training_performance,'Test': Test_performance})\n",
    "    Performance_dataframe.index = [\"Er. 95L\",\"Er. Mean\",\"Er. 95U\",\"MAE\",\"MSE\"]\n",
    "    return Performance_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe simplifier(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10, floor\n",
    "def round_to_3(x):\n",
    "    return round(x, 3 - int(np.floor(np.log10(abs(x)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Specialized Layers/Funtions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCAs(X_train_scaled,X_test_scaled,PCA_Rank):\n",
    "    mu = X_train_scaled.mean(axis=0)\n",
    "    U,s,V = np.linalg.svd(X_train_scaled - mu, full_matrices=False)\n",
    "    Zpca = np.dot(X_train_scaled - mu, V.transpose())\n",
    "    Zpca_test = np.dot(X_test_scaled - mu, V.transpose())\n",
    "\n",
    "    # Reconstruct Training Data\n",
    "    Rpca = np.dot(Zpca[:,:PCA_Rank], V[:PCA_Rank,:]) + mu    # reconstruction\n",
    "    # Reconstruct Testing Data\n",
    "    Rpca_test = np.dot(Zpca_test[:,:PCA_Rank], V[:PCA_Rank,:]) + mu    # reconstruction\n",
    "    \n",
    "    # Return PCA(s) and Reconstruction(s)\n",
    "    return Zpca,Zpca_test, Rpca, Rpca_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Layer\n",
    "The following is the Special PCA Layer which implements the hyperplane model $f_{V,\\mu}:\\mathbb{R}^r \\rightarrow \\mathbb{R}^d$\n",
    "$$\n",
    "z \\mapsto Vz + \\mu\n",
    ";\n",
    "$$\n",
    "where $V\\in SO(d,r)$ is parameterized by $v \\in \\mathbb{R}^{r\\times d}$ by\n",
    "$$\n",
    "V\\triangleq \\exp\\left(Skw(v)\\right)P_{r,d}\n",
    ".\n",
    "$$\n",
    "Where $(P_{r,d})_{i,j}=1$ iff $i=j\\leq r$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b18f55c27f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPCA_Layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_Layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class PCA_Layer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, input_dim=32,rank=1):\n",
    "        super(PCA_Layer, self).__init__()\n",
    "        self.rank = rank\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Write Internal Parameters\n",
    "        self.skew_symmetric_weight = self.add_weight(name='Weights_ffNN',\n",
    "                                                     shape=(input_shape[-1],input_shape[-1]),\n",
    "                                                     initializer='random_normal',\n",
    "                                                     trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        # Get Projector\n",
    "        input_dimension = input_shape[-1]\n",
    "        kernel_dimension = np.maximum(input_dimension-self.rank,0)\n",
    "        digaonal_entries = tf.concat([tf.ones(self.rank),tf.zeros(kernel_dimension)],-1)\n",
    "        self.projector = tf.linalg.diag(digaonal_entries)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Get Skew-Symmetric Matrix\n",
    "        skew_symmetric_matrix = self.skew_symmetric_weight - tf.linalg.matrix_transpose(self.skew_symmetric_weight)\n",
    "        # (Special) Orthogonalize\n",
    "        SOd_mat = tf.linalg.expm(skew_symmetric_matrix)\n",
    "        # Get Low-Rank Orthogonal of Skew-Symmetric Matrix\n",
    "        SOrd_mat = tf.linalg.matmul(self.projector,SOd_mat)\n",
    "        # Return output\n",
    "        return SOrd_mat#tf.matmul(inputs, SOd_mat) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define Custom Scorer Function\n",
    "def my_scorer(estimator, X, y=None):\n",
    "    X_reduced = estimator.transform(X)\n",
    "    X_preimage = estimator.inverse_transform(X_reduced)\n",
    "    return -1 * mean_squared_error(X, X_preimage)\n",
    "    \n",
    "def get_kPCAs(X_inputs):\n",
    "    # Import Dependancies\n",
    "    from sklearn.decomposition import KernelPCA\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    # Define kPCA Model\n",
    "    kpca=KernelPCA(fit_inverse_transform=True, n_jobs=n_jobs) \n",
    "    # Initialize Grid Searcher\n",
    "    grid_search = RandomizedSearchCV(kpca, \n",
    "                                     kPCA_grid, \n",
    "                                     cv=CV_folds, \n",
    "                                     scoring=my_scorer,\n",
    "                                     random_state=2020)\n",
    "    \n",
    "    # Perform Randomized GridSearch for Hyperparameter Selection\n",
    "    kPCAsModel = grid_search.fit(X_inputs)\n",
    "    \n",
    "    # Get Feature(s) and Reconstruction(s)\n",
    "    ## Define\n",
    "    kPCA = KernelPCA(fit_inverse_transform =True, \n",
    "                     n_components=PCA_Rank,\n",
    "                     coef0=kPCAsModel.best_estimator_.coef0,\n",
    "                     gamma=kPCAsModel.best_estimator_.gamma,\n",
    "                     kernel=kPCAsModel.best_estimator_.kernel)\n",
    "    ## Fit and (inverse) Transform\n",
    "    kPCA_Features = kPCA.fit_transform(X_inputs)\n",
    "    kPCA_Reconstruction = kPCA.inverse_transform(kPCA_Features)\n",
    "    \n",
    "    # Return Prediction\n",
    "    return kPCA_Features, kPCA_Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Reconstruction Error for Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE_reconstruction_score(estimator, X, y=None):\n",
    "    X_reduced = estimator.transform(X)\n",
    "    X_preimage = estimator.inverse_transform(X_reduced)\n",
    "    return -1 * mean_absolute_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainer(s)\n",
    "Chainer iteratively constructs NEU-feature maps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_Feature_Chaining(learning_rate, \n",
    "                             X_train_in,\n",
    "                             X_test_in,\n",
    "                             y_train_in,\n",
    "                             block_depth, \n",
    "                             feature_map_height,\n",
    "                             robustness_parameter, \n",
    "                             homotopy_parameter,\n",
    "                             N_epochs,\n",
    "                             batch_size,\n",
    "                             output_dim):\n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    home_dimension = X_train_in.shape[1]\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(home_dimension,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    for i_feature_depth in range(block_depth):\n",
    "        # First Layer\n",
    "        ## Spacial-Dependent part of reconfiguration unit\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map  = Reconfiguration_unit_identity_initialization(units=feature_map_height,\n",
    "                                                                             home_space_dim=home_dimension, \n",
    "                                                                             homotopy_parameter = homotopy_parameter)(input_layer)\n",
    "        else:\n",
    "            deep_feature_map  = Reconfiguration_unit_identity_initialization(units=feature_map_height,\n",
    "                                                                             home_space_dim=home_dimension, \n",
    "                                                                             homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        ## Constant part of reconfiguration unit\n",
    "        deep_feature_map = fullyConnected_Dense_Invertible(home_dimension)(deep_feature_map)\n",
    "        ## Non-linear part of reconfiguration unit\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    # Linear Readout (Really this is the OLS model)\n",
    "    OLS_Layer_output = fullyConnected_Dense(output_dim)(deep_feature_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, OLS_Layer_output)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\"])\n",
    "        \n",
    "    # Train #\n",
    "    #-------#\n",
    "    trainable_layers_model.fit(X_train_in,y_train_in,epochs = N_epochs,batch_size=batch_size)\n",
    "    \n",
    "    # Prepare Output(s) #\n",
    "    #-------------------#\n",
    "    # Extract Linearizing Feature Map\n",
    "    Linearizing_Feature_Map = extract_trained_feature_map(trainable_layers_model)\n",
    "\n",
    "    # Pre-process Linearized Data #\n",
    "    #========================b=====#\n",
    "    # Get Linearized Predictions #\n",
    "    #----------------------------#\n",
    "    data_x_featured_train = Linearizing_Feature_Map.predict(X_train_in)\n",
    "    data_x_featured_test = Linearizing_Feature_Map.predict(X_test_in)\n",
    "\n",
    "    return data_x_featured_train, data_x_featured_test, Linearizing_Feature_Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
