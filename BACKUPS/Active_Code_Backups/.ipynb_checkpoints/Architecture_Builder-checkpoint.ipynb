{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions Depot\n",
    "This little script contains all the architecutre builders used in benchmarking the NEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconfiguration Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Reconfiguration_Network_Readout(learning_rate, input_dim, output_dim, readout_map_depth,readout_map_height,robustness_parameter,homotopy_parameter):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=((input_dim),))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(input_dim), homotopy_parameter = homotopy_parameter)(input_layer)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        \n",
    "    # Projection Layer\n",
    "#     output_layer = projection_layer(deep_readout_map)\n",
    "    # Trainable Output Layer\n",
    "    output_layer = fullyConnected_Dense(output_dim)(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NEU_Structure(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_Structure_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_Reconfiguration_Network_Readout, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_Structure_CV = RandomizedSearchCV(estimator=NEU_Structure_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Pipe Standard Scaler \n",
    "    NEU_Structure_CV_mmxscaler_piped = NEU_Structure_CV\n",
    "    #Pipeline([('scaler', MinMaxScaler()), ('model', NEU_Structure_CV)])\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_Structure_CV_mmxscaler_piped.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_Structure_CV_mmxscaler_piped.predict(X_train)\n",
    "    y_hat_test = NEU_Structure_CV_mmxscaler_piped.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_Structure_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-Structure Map: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-Structure Map: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "#     NEU_Structure_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "#     pd.DataFrame.from_dict(NEU_Structure_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-Structure: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-Structure Building Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_OLS(learning_rate, input_dim, output_dim, feature_map_depth, feature_map_height,robustness_parameter, homotopy_parameter,implicit_dimension):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map  = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    ## Homeomorphic Part\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "        # First Layer\n",
    "        ## Spacial-Dependent part of reconfiguration unit\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=embedding_dimension, homotopy_parameter = 1)(deep_feature_map)\n",
    "        ## Constant part of reconfiguration unit\n",
    "#         deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "        ## Non-linear part of reconfiguration unit\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    # Linear Readout (Really this is the OLS model)\n",
    "    OLS_Layer_output = fullyConnected_Dense(output_dim)(deep_feature_map)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, OLS_Layer_output)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NEU-OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NEU_OLS(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_OLS_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_OLS, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_OLS_CV = RandomizedSearchCV(estimator=NEU_OLS_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_OLS_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_OLS_CV.predict(X_train)\n",
    "    y_hat_test = NEU_OLS_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_OLS_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-OLS: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-OLS: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_OLS_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    Path('./outputs/models/NEU/NEU_OLS/').mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(NEU_OLS_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/NEU_OLS/Best_Parameters.tex\")\n",
    "    print('NEU-OLS: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test, best_model\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Models: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Vanilla) Feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "   \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "def build_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "    # Deep Feature Network\n",
    "    ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN, \n",
    "                                                            verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = ffNN_CVer.predict(X_train)\n",
    "    y_hat_test = ffNN_CVer.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = ffNN_CVer.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    \n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('Benchmark-Model: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    ffNN_CVer.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(ffNN_CVer.best_params_,orient='index').to_latex(\"./outputs/models/Benchmarks/Best_Parameters.tex\")\n",
    "    print('Benchmark-Model: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Deep Feature Builder - Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEU-Feed-forward Neural Network\n",
    "This next snippet builds the NEU for the feed-forward network; i.e.:\n",
    "$$\n",
    "f_{NEU} \\triangleq \\rho \\circ f_{ffNN}\\circ \\phi\n",
    ",\n",
    "$$\n",
    "where $\\rho=p\\circ \\xi$, $\\xi,\\phi$ are reconfiguration networks, and $f_{ffNN}$ is a feed-forward network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Train NEU-ffNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_ffNN(height, depth, learning_rate, input_dim, output_dim, feature_map_depth, readout_map_depth, feature_map_height,readout_map_height,robustness_parameter,homotopy_parameter,implicit_dimension):\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map  = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    ### Execute Random Embedding\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "#        # First Layer\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=embedding_dimension, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        deep_feature_map = fullyConnected_Dense_Invertible(embedding_dimension)(input_layer)\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    core_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=output_dim, homotopy_parameter = homotopy_parameter)(core_layers)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=output_dim, homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, deep_readout_map)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NEU_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_ffNN, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_ffNN_CV = RandomizedSearchCV(estimator=NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-ffNN: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-ffNN: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative NEU-ffNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NEU_ffNN_w_proj(height, depth, learning_rate, input_dim, output_dim, feature_map_depth, readout_map_depth, feature_map_height,readout_map_height,robustness_parameter,homotopy_parameter,implicit_dimension):\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "        #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    ##Random Embedding\n",
    "    ### Compute Required Dimension\n",
    "    embedding_dimension = 2*np.maximum(np.maximum(input_dim,output_dim),implicit_dimension)\n",
    "    ### Execute Random Embedding\n",
    "    deep_feature_map  = fullyConnected_Dense(embedding_dimension)(input_layer)\n",
    "    for i_feature_depth in range(feature_map_depth):\n",
    "#        # First Layer\n",
    "        deep_feature_map  = Reconfiguration_unit(units=feature_map_height,home_space_dim=embedding_dimension, homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "        deep_feature_map = fullyConnected_Dense_Invertible(embedding_dimension)(input_layer)\n",
    "        deep_feature_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_feature_map)\n",
    "            \n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    core_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    \n",
    "    deep_readout_map = tf.concat([input_layer, core_layers], axis=1)\n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Readout Map #\n",
    "    #-###############-#\n",
    "    deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(output_dim+input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    for i_readout_depth in range(readout_map_depth):\n",
    "        deep_readout_map = rescaled_swish_trainable(homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "        deep_readout_map  = Reconfiguration_unit(units=readout_map_height,home_space_dim=(output_dim+input_dim), homotopy_parameter = homotopy_parameter)(deep_readout_map)\n",
    "    \n",
    "    # Projection Layer\n",
    "    output_layer = projection_layer(deep_readout_map)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    if robustness_parameter == 0:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss='mae', metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "    else:\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=Robust_MSE(robustness_parameter), metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NEU_ffNN_w_proj(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "\n",
    "    # Deep Feature Network\n",
    "    NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NEU_ffNN_w_proj, \n",
    "                                                                verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NEU_ffNN_CV = RandomizedSearchCV(estimator=NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NEU-ffNN: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/NEU/Best_Parameters.tex\")\n",
    "    print('NEU-ffNN: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive NEU-ffNN\n",
    "Next we implement the NEU but without using reconfiguration networks for the feature and readout maps... Instead we use the (homeomorphic) feed-forward architecture with *sub-minimal width* feed-forward architecture introduced in: [Bilokopytov and Kratsios](https://arxiv.org/pdf/2006.02341.pdf).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NAIVE_NEU_ffNN(feature_map_depth, feature_map_height, ## NEU-Feature Map Hyper-Parameter(s)\n",
    "                       height, depth, ## ffNN Parameter(s)\n",
    "                       readout_map_depth, readout_map_height,\n",
    "                       learning_rate, input_dim, output_dim): ## Training Parameters\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(d,))\n",
    "    \n",
    "    \n",
    "    #-###############-#\n",
    "    # NEU Feature Map #\n",
    "    #-###############-#\n",
    "    \n",
    "    # Initial Features\n",
    "    inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN)\n",
    "    # Higher-Order Feature Depth\n",
    "    if feature_map_depth > 0:\n",
    "        inputs_ffNN_feature = Deep_GLd_Layer(d)(inputs_ffNN)\n",
    "\n",
    "    \n",
    "    \n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    # First ffNN Layer: Reconfigured inputs -> Hidden Neurons\n",
    "    x_ffNN = fullyConnected_Dense(height)(inputs_ffNN_feature)\n",
    "    # Higher-Order Deep Layers: Hidden Neurons -> Hidden Neurons\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "    # Last ffNN Layer: Hidden Neurons -> Output Space\n",
    "    x_ffNN = fullyConnected_Dense(D)(x_ffNN)     \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-###########-#\n",
    "    # Readout Map #\n",
    "    #-###########-#\n",
    "    # Input -> Input x ffNN output\n",
    "    output_layer_new = tf.concat([inputs_ffNN, x_ffNN], axis=1)\n",
    "    \n",
    "    # Add Depth to Readout Map\n",
    "    if readout_map_depth > 0:\n",
    "        output_layer_new = Deep_GLd_Layer(d+D)(output_layer_new)\n",
    "\n",
    "    # Project down from graph space to output space (from: Input x Outputs -> Outputs)\n",
    "    output_layer = projection_layer(output_layer_new)\n",
    "    \n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, output_layer)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    ffNN.compile(optimizer=opt, loss=Robust_MSE, metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return ffNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NAIVE_NEU_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train, X_test):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    NAIVE_NEU_ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_NAIVE_NEU_ffNN, \n",
    "                                                            verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    NAIVE_NEU_ffNN_CV = RandomizedSearchCV(estimator=NAIVE_NEU_ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    NAIVE_NEU_ffNN_CV.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = NAIVE_NEU_ffNN_CV.predict(X_train)\n",
    "    y_hat_test = NAIVE_NEU_ffNN_CV.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = NAIVE_NEU_ffNN_CV.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    print('NEU-ffNN: Trained!')\n",
    "    \n",
    "    #-----------------#\n",
    "    # Save Full-Model #\n",
    "    #-----------------#\n",
    "    print('NAIVE_NEU-ffNN: Saving')\n",
    "#     joblib.dump(best_model, './outputs/models/Benchmarks/ffNN_trained_CV.pkl', compress = 1)\n",
    "    NAIVE_NEU_ffNN_CV.best_params_['N_Trainable_Parameters'] = N_params_best_ffNN\n",
    "    pd.DataFrame.from_dict(NAIVE_NEU_ffNN_CV.best_params_,orient='index').to_latex(\"./outputs/models/Naive_NEU/Best_Parameters.tex\")\n",
    "    print('NAIVE_NEU-ffNN: Saved')\n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Complete NEU-ffNN Training Procedure!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
