{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Custom) Special Layers Homeomorphic\n",
    "(In order of Generality):\n",
    "- Shift\n",
    "- Euclidean Group\n",
    "- Special Affine Group\n",
    "- Affine Group\n",
    "\n",
    "- *Reconfiguration Unit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift $\\mathbb{R}^d$  Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cd5266d8e948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mShift_Layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mShift_Layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Shift_Layers(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Shift_Layers, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{E}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{O}_{d}(\\mathbb{R})$  Layers\n",
    "This is the group of all isometries of $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Euclidean_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Euclidean_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        On = tf.linalg.matmul((self.Id + self.glw),tf.linalg.inv(self.Id - self.glw))\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(On,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\operatorname{SAff}_{d}(\\mathbb{R}) \\cong \\mathbb{R}^d \\rtimes \\operatorname{SL}_{d}(\\mathbb{R})$  Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $A \\in \\operatorname{SL}_d(\\mathbb{R})$ if and only if $A=\\frac1{\\sqrt[d]{\\det(\\exp(X))}} \\exp(X)$ for some $d\\times d$ matrix $X$.  \n",
    "\n",
    "*Why?*... We use the fact that $\\det(k A) = k^d \\det(A)$ for any $k \\in \\mathbb{R}$ and any $d\\times d$ matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38a319acd40e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSpecial_Affine_Layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpecial_Affine_Layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Special_Affine_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Special_Affine_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "        # Element of gld\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN_det = tf.linalg.det(GLN)\n",
    "        GLN_det = tf.pow(tf.abs(GLN_det),(1/(d+D)))\n",
    "        SLN = tf.math.divide(GLN,GLN_det)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "        x_out = tf.linalg.matvec(SLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep GLd Layer:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Deep-GL}_d(x) \\triangleq& f^{Depth}\\circ \\dots f^1(x)\\\\\n",
    "f^i(x)\\triangleq &\\exp(A_2) \\operatorname{Leaky-ReLU}\\left(\n",
    "\\exp(A_1)x + b_1\n",
    "\\right)+ b_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $A_i$ are $d\\times d$ matrices and $b_i \\in \\mathbb{R}^d$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep_GLd_Layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Deep_GLd_Layer, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # For Numerical Stability (problems with Tensorflow's Exp rounding)\n",
    "#         self.Id = self.add_weight(name='Identity_Matrix',\n",
    "#                                    shape=(input_shape[-1],input_shape[-1]),\n",
    "#                                    initializer='identity',\n",
    "#                                    trainable=False)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',\n",
    "#                                               shape=[1],\n",
    "#                                               initializer=RandomUniform(minval=0.0, maxval=0.01),\n",
    "#                                               trainable=True,\n",
    "#                                               constraint=tf.keras.constraints.NonNeg())\n",
    "#         Element of gl_d\n",
    "        self.glw = self.add_weight(name='Tangential_Weights',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.glw2 = self.add_weight(name='Tangential_Weights2',\n",
    "                                       shape=(input_shape[-1],input_shape[-1]),\n",
    "                                       initializer='GlorotUniform',\n",
    "                                       trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Euclidean Parameters\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.b = self.add_weight(name='location_parameter',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        self.b2 = self.add_weight(name='location_parameter2',\n",
    "                                 shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=False)\n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        GLN = tf.linalg.expm(self.glw)\n",
    "        GLN2 = tf.linalg.expm(self.glw2)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = input\n",
    "\n",
    "        x_out = tf.linalg.matvec(GLN,x_out)\n",
    "        x_out = x_out + self.b\n",
    "        x_out = tf.nn.leaky_relu(x_out)\n",
    "        x_out = tf.linalg.matvec(GLN2,x_out)\n",
    "        x_out = x_out + self.b2\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconfiguration Unit\n",
    "*Lie Version:* $$\n",
    "x \\mapsto \\exp\\left(\n",
    "%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\n",
    "\\right)\n",
    "\\right) x.\n",
    "$$\n",
    "\n",
    "*Cayley version:*\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}(A(x)):\\,x \\mapsto & \\left[(I_d + A(x))(I_d- A(x))^{-1}\\right]x\n",
    "\\\\\n",
    "A(x)\\triangleq &%\\psi(a\\|x\\|+b)\n",
    "\\operatorname{Skew}_d\\left(\n",
    "    F(\\|x\\|)\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the inverse of the Cayley transform of $A(x)$ is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Cayley}^{-1}(A(x)):\\,x \\mapsto & \\left[(I_d - A(x))(I_d+ A(x))^{-1}\\right]x\n",
    ".\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readout Map Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-68214b43acd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mReconfiguration_unit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReconfiguration_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class Reconfiguration_unit(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Reconfiguration_unit, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(input_shape[-1],),\n",
    "                                    initializer='random_normal',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #                                  Decay Rates                                       #\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.sigma = self.add_weight(name='bump_threshfold',\n",
    "                                        shape=[1],\n",
    "                                        initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                        trainable=True,\n",
    "                                        constraint=tf.keras.constraints.NonNeg())\n",
    "        self.a = self.add_weight(name='bump_scale',\n",
    "                                        shape=[1],\n",
    "                                        initializer='ones',\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bump_location',\n",
    "                                        shape=[1],\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Exponential Decay\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.exponential_decay = self.add_weight(name='exponential_decay_rate',\n",
    "                                                 shape=[1],\n",
    "                                                 initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                                 trainable=True,\n",
    "                                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Mixture\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.m_w1 = self.add_weight(name='no_decay',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w2 = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w3 = self.add_weight(name='bump',\n",
    "                                     shape=[1],\n",
    "                                     initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                     trainable=True,\n",
    "                                     constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # No Decay\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1',\n",
    "                                   shape=(self.units,((d+D)**2)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2',\n",
    "                                   shape=(((d+D)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=((d+D),(d+D)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Exponential Decay\n",
    "        self.Tw1_b = self.add_weight(name='Tangential_Weights_1_b',\n",
    "                           shape=(self.units,((d+D)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_b = self.add_weight(name='Tangential_Weights_2_b',\n",
    "                                   shape=(((d+D)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=((d+D),(d+D)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Bump\n",
    "        self.Tw1_c = self.add_weight(name='Tangential_Weights_1_c',\n",
    "                           shape=(self.units,((d+D)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_c = self.add_weight(name='Tangential_Weights_2_c',\n",
    "                                   shape=(((d+D)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=((d+D),(d+D)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "        # Stability Parameter(s)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',shape=[1],initializer=RandomUniform(minval=0.0, maxval=0.01),trainable=True,constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.norm(input) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Decay Rate Functions\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function (Local Behaviour)\n",
    "        bump_input = self.a*norm_inputs + self.b\n",
    "        greater = tf.math.greater(bump_input, -self.sigma)\n",
    "        less = tf.math.less(bump_input, self.sigma)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bump_decay = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(bump_input),\n",
    "            0.0)\n",
    "        \n",
    "        # Exponential Decay\n",
    "        exp_decay = tf.math.exp(-self.exponential_decay*norm_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = norm_inputs*self.Id\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[((d+D)**2),1])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        \n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw1,tangential_ffNN)         \n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw2,tangential_ffNN)\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[(d+D),(d+D)])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb2\n",
    "        \n",
    "        # Exponential Decay\n",
    "        tangential_ffNN_b = norm_inputs*exp_decay*self.Id\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[((d+D)**2),1])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb1_b\n",
    "        \n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw1_b,tangential_ffNN_b)         \n",
    "        tangential_ffNN_b = tf.nn.relu(tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw2_b,tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[(d+D),(d+D)])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb2_b\n",
    "        \n",
    "        # Bump (Local Aspect)\n",
    "        tangential_ffNN_c = bump_decay*norm_inputs*self.Id\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[((d+D)**2),1])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb1_c\n",
    "        \n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw1_c,tangential_ffNN_c)         \n",
    "        tangential_ffNN_c = tf.nn.relu(tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw2_c,tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[(d+D),(d+D)])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb2_c\n",
    "    \n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "        tangential_ffNN_b = (tf.transpose(tangential_ffNN_b) - tangential_ffNN_b) \n",
    "        tangential_ffNN_c = (tf.transpose(tangential_ffNN_c) - tangential_ffNN_c) \n",
    "        # Decay\n",
    "        tangential_ffNN = (self.m_w1*tangential_ffNN) + (self.m_w2*tangential_ffNN_b) + (self.m_w3*tangential_ffNN_c) \n",
    "            \n",
    "        # Cayley Transformation (Stable):\n",
    "        tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.inv(self.Id - tangential_ffNN)) # Lie Parameterization (Numerically Unstable):  #tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,input) + self.location\n",
    "#         x_out = tf.linalg.matvec(tangential_ffNN,input)\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature_map version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reconfiguration_unit_Feature(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Reconfiguration_unit_Feature, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Center\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.location = self.add_weight(name='location',\n",
    "                                    shape=(input_shape[-1],),\n",
    "                                    initializer='random_normal',\n",
    "                                    trainable=True)\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #                                  Decay Rates                                       #\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        #====================================================================================#\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.sigma = self.add_weight(name='bump_threshfold',\n",
    "                                        shape=[1],\n",
    "                                        initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                        trainable=True,\n",
    "                                        constraint=tf.keras.constraints.NonNeg())\n",
    "        self.a = self.add_weight(name='bump_scale',\n",
    "                                        shape=[1],\n",
    "                                        initializer='ones',\n",
    "                                        trainable=True)\n",
    "        self.b = self.add_weight(name='bump_location',\n",
    "                                        shape=[1],\n",
    "                                        initializer='zeros',\n",
    "                                        trainable=True)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Exponential Decay\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.exponential_decay = self.add_weight(name='exponential_decay_rate',\n",
    "                                                 shape=[1],\n",
    "                                                 initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                                 trainable=True,\n",
    "                                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Mixture\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.m_w1 = self.add_weight(name='no_decay',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w2 = self.add_weight(name='weight_exponential',\n",
    "                                         shape=[1],\n",
    "                                         initializer='zeros',\n",
    "                                         trainable=True,\n",
    "                                         constraint=tf.keras.constraints.NonNeg())\n",
    "        self.m_w3 = self.add_weight(name='bump',\n",
    "                                     shape=[1],\n",
    "                                     initializer=RandomUniform(minval=.5, maxval=1),\n",
    "                                     trainable=True,\n",
    "                                     constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        self.Id = self.add_weight(name='Identity_Matrix',\n",
    "                                   shape=(input_shape[-1],input_shape[-1]),\n",
    "                                   initializer='identity',\n",
    "                                   trainable=False)\n",
    "        # No Decay\n",
    "        self.Tw1 = self.add_weight(name='Tangential_Weights_1',\n",
    "                                   shape=(self.units,((d)**2)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)        \n",
    "        self.Tw2 = self.add_weight(name='Tangential_Weights_2',\n",
    "                                   shape=(((d)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2 = self.add_weight(name='Tangential_basies_1',\n",
    "                                   shape=((d),(d)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Exponential Decay\n",
    "        self.Tw1_b = self.add_weight(name='Tangential_Weights_1_b',\n",
    "                           shape=(self.units,((d)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_b = self.add_weight(name='Tangential_Weights_2_b',\n",
    "                                   shape=(((d)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_b = self.add_weight(name='Tangential_basies_1_b',\n",
    "                                   shape=((d),(d)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        # Bump\n",
    "        self.Tw1_c = self.add_weight(name='Tangential_Weights_1_c',\n",
    "                           shape=(self.units,((d)**2)),\n",
    "                           initializer='GlorotUniform',\n",
    "                           trainable=True)        \n",
    "        self.Tw2_c = self.add_weight(name='Tangential_Weights_2_c',\n",
    "                                   shape=(((d)**2),self.units),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb1_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=(((input_shape[-1])**2),1),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        self.Tb2_c = self.add_weight(name='Tangential_basies_1_c',\n",
    "                                   shape=((d),(d)),\n",
    "                                   initializer='GlorotUniform',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "        # Stability Parameter(s)\n",
    "#         self.num_stab_param = self.add_weight(name='matrix_exponential_stabilizer',shape=[1],initializer=RandomUniform(minval=0.0, maxval=0.01),trainable=True,constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        # Wrap things up!\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def bump_function(self, x):\n",
    "        return tf.math.exp(-self.sigma / (self.sigma - x))\n",
    "\n",
    "        \n",
    "    def call(self, input):\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Initializations\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        norm_inputs = tf.norm(input) #WLOG if norm is squared!\n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Decay Rate Functions\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Bump Function (Local Behaviour)\n",
    "        bump_input = self.a*norm_inputs + self.b\n",
    "        greater = tf.math.greater(bump_input, -self.sigma)\n",
    "        less = tf.math.less(bump_input, self.sigma)\n",
    "        condition = tf.logical_and(greater, less)\n",
    "\n",
    "        bump_decay = tf.where(\n",
    "            condition, \n",
    "            self.bump_function(bump_input),\n",
    "            0.0)\n",
    "        \n",
    "        # Exponential Decay\n",
    "        exp_decay = tf.math.exp(-self.exponential_decay*norm_inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Tangential Map\n",
    "        #------------------------------------------------------------------------------------#\n",
    "        # Build Radial, Tangent-Space Valued Function, i.e.: C(R^d,so_d) st. f(x)=f(y) if |x|=|y|\n",
    "        \n",
    "        \n",
    "        # Build Tangential Feed-Forward Network (Bonus)\n",
    "        #-----------------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = norm_inputs*self.Id\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[((d)**2),1])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb1\n",
    "        \n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw1,tangential_ffNN)         \n",
    "        tangential_ffNN = tf.nn.relu(tangential_ffNN)\n",
    "        tangential_ffNN = tf.linalg.matmul(self.Tw2,tangential_ffNN)\n",
    "        tangential_ffNN = tf.reshape(tangential_ffNN,[(d),(d)])\n",
    "        tangential_ffNN = tangential_ffNN + self.Tb2\n",
    "        \n",
    "        # Exponential Decay\n",
    "        tangential_ffNN_b = norm_inputs*exp_decay*self.Id\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[((d)**2),1])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb1_b\n",
    "        \n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw1_b,tangential_ffNN_b)         \n",
    "        tangential_ffNN_b = tf.nn.relu(tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.linalg.matmul(self.Tw2_b,tangential_ffNN_b)\n",
    "        tangential_ffNN_b = tf.reshape(tangential_ffNN_b,[(d),(d)])\n",
    "        tangential_ffNN_b = tangential_ffNN_b + self.Tb2_b\n",
    "        \n",
    "        # Bump (Local Aspect)\n",
    "        tangential_ffNN_c = bump_decay*norm_inputs*self.Id\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[((d)**2),1])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb1_c\n",
    "        \n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw1_c,tangential_ffNN_c)         \n",
    "        tangential_ffNN_c = tf.nn.relu(tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.linalg.matmul(self.Tw2_c,tangential_ffNN_c)\n",
    "        tangential_ffNN_c = tf.reshape(tangential_ffNN_c,[(d),(d)])\n",
    "        tangential_ffNN_c = tangential_ffNN_c + self.Tb2_c\n",
    "    \n",
    "        # Map to Rotation-Matrix-Valued Function #\n",
    "        #----------------------------------------#\n",
    "        # No Decay\n",
    "        tangential_ffNN = (tf.transpose(tangential_ffNN) - tangential_ffNN) \n",
    "        tangential_ffNN_b = (tf.transpose(tangential_ffNN_b) - tangential_ffNN_b) \n",
    "        tangential_ffNN_c = (tf.transpose(tangential_ffNN_c) - tangential_ffNN_c) \n",
    "        # Decay\n",
    "        tangential_ffNN = (self.m_w1*tangential_ffNN) + (self.m_w2*tangential_ffNN_b) + (self.m_w3*tangential_ffNN_c) \n",
    "            \n",
    "        # Cayley Transformation (Stable):\n",
    "        tangential_ffNN = tf.linalg.matmul((self.Id + tangential_ffNN),tf.linalg.inv(self.Id - tangential_ffNN)) # Lie Parameterization (Numerically Unstable):  #tangential_ffNN = tf.linalg.expm(tangential_ffNN)\n",
    "        \n",
    "        # Exponentiation and Action\n",
    "        #----------------------------#\n",
    "        x_out = tf.linalg.matvec(tangential_ffNN,input) + self.location\n",
    "#         x_out = tf.linalg.matvec(tangential_ffNN,input)\n",
    "        \n",
    "        # Return Output\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Non-Homeomorphic Layers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c8b12ca806c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mfullyConnected_Dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullyConnected_Dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class fullyConnected_Dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
